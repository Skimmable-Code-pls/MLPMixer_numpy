{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ***0. Data Loading***"],"metadata":{"id":"Q1wcnjQODYQ4"}},{"cell_type":"markdown","source":["Import NB15 dataset on your Google Drive then run this cell\n","\n","To-do:\n","- [x] Verify pandas.DataFrame = polars.DataFrame with no floating value difference\n","- [x] Verify polars is faster than pandas in reading csv, around 70-110ms difference"],"metadata":{"id":"Q3VVMd4Fj301"}},{"cell_type":"code","source":["############################################################\n","#put your code for reading the csv data here\n","############################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/\n","\n","import polars as pl\n","def polar_read_csv(csv_train, csv_val, csv_test):\n","    \"\"\"\n","    Args:\n","        csv_train (str): name of train set csv file\n","        csv_val (str): name of val set csv file\n","        csv_test (str): name of test set csv file\n","\n","    Output:\n","        read train, val, test sets in DataFrame format\n","    \"\"\"\n","    df_train = pl.read_csv(csv_train)\n","    df_val = pl.read_csv(csv_val)\n","    df_test = pl.read_csv(csv_test)\n","\n","    return df_train, df_val, df_test\n","\n","polar_read_csv(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")"],"metadata":{"id":"n6MJyx0bDlFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***1. Data Pre-Processing (Task 1)***"],"metadata":{"id":"oYce1Z49mIsz"}},{"cell_type":"markdown","source":["TO-DO:\n","- [x] Convert DataFrame to np.float32\n","- [x] Sanity check if min-max rescaling will squash small number. Observation: np.float32 will pick up the difference in (small_number+1)/big_number, but not when small_number/(big_number+1). Looking at column feature contain big numbers like dur, sload, dload, rate: their value vary greater than 1000 between different samples. This is something np.float32 can pick up. Last but not least, this tells us that we shoudl add noise to dataset after normalisation, not before\n","- [x] Sanity check fuse rescale-normalise == rescale first then normalise"],"metadata":{"id":"PY_MbXp2cEsj"}},{"cell_type":"code","source":["############################################################\n","#put your code for data preprocessing here\n","############################################################\n","\n","import polars as pl\n","import numpy as np\n","\n","class NIDdataset():\n","    def __init__(\n","        self,\n","        train_file,\n","        val_file,\n","        test_file\n","    ):\n","        assert train_file.endswith(\".csv\"), print(\"train dataset should be either .csv in this project\")\n","        assert val_file.endswith(\".csv\"), print(\"val dataset should be either .csv in this project\")\n","        assert test_file.endswith(\".csv\"), print(\"test dataset should be either .csv in this project\")\n","\n","        if train_file.endswith(\".csv\") and val_file.endswith(\".csv\") and test_file.endswith(\".csv\"):\n","            train_df_encoded, val_df_encoded, test_df_encoded = self.encode_category_attribute(train_file, val_file, test_file, [\"proto\", \"service\", \"state\"])\n","            train_arr = train_df_encoded.to_numpy().astype(np.float32)\n","            val_arr = val_df_encoded.to_numpy().astype(np.float32)\n","            test_arr = test_df_encoded.to_numpy().astype(np.float32)\n","\n","            train_features = train_arr[:, 1:-1]\n","            val_features = val_arr[:, 1:-1]\n","            test_features = test_arr[:, 1:]\n","\n","            self.get_mean_std_after_rescaled(train_features)\n","\n","        self.train_set = np.concatenate([self._transform(train_features), train_arr[:, -1].reshape(-1, 1)], axis=1)\n","        self.val_set = np.concatenate([self._transform(val_features), val_arr[:, -1].reshape(-1, 1)], axis=1)\n","        self.test_set = self._transform(test_features)\n","\n","    def _transform(self, input: np.ndarray):\n","        processed_input = self.fused_rescale_normalise(input)\n","        return processed_input\n","\n","    def encode_category_attribute(self, csv_train, csv_val, csv_test, attribute_list):\n","        \"\"\"\n","        Args:\n","            csv_train (str): train set as .csv\n","            csv_val (str): val set as .csv\n","            csv_test (str): test set as .csv\n","\n","        Return:\n","            encoded version of train set, val set, test set as polars.DataFrame using encoding dictionary for train set\n","        \"\"\"\n","        df_train, df_val, df_test = polar_read_csv(csv_train, csv_val, csv_test)\n","\n","        attribute_dict = {}\n","        for attribute in attribute_list:\n","            if attribute in df_train.columns:\n","                unique_vals = (df_train.select(attribute).drop_nulls().unique().to_series().to_list())\n","                attribute_dict[attribute] = {val: idx for idx, val in enumerate(unique_vals)}\n","            else:\n","                print(f\"Warning: '{attribute}' not found in training CSV.\")\n","\n","        def apply_encoding(df, attribute_dict):\n","            for attribute, val_to_idx in attribute_dict.items():\n","                if attribute in df.columns:\n","                    df = df.with_columns(pl.col(attribute).replace(val_to_idx).alias(attribute))\n","            return df\n","        df_train_encoded = apply_encoding(df_train, attribute_dict)\n","        df_val_encoded = apply_encoding(df_val, attribute_dict)\n","        df_val_encoded = df_val_encoded.filter(pl.col(\"state\") != \"RST\")\n","        df_test_encoded = apply_encoding(df_test, attribute_dict)\n","\n","        return df_train_encoded, df_val_encoded, df_test_encoded\n","\n","    def get_mean_std_after_rescaled(self, x: np.ndarray):\n","        scale = np.max(x)\n","        x_rescaled = x/scale\n","        # usually we get mean, std per channel. But nb15 doesn't channel so we get 1 mean, std over anything to not destroy local inductive bias in train_features\n","        self.train_mean = np.mean(x_rescaled, axis=(0,1), keepdims=True)\n","        self.train_std = np.std(x_rescaled, axis=(0,1), keepdims=True)\n","\n","    def fused_rescale_normalise(self, x: np.ndarray):\n","        mean = self.train_mean.copy()\n","        std = self.train_std.copy()\n","        scaling_factor = np.max(x)\n","        return (x - mean*scaling_factor) / (std*scaling_factor)\n","\n","nb15_dataset = NIDdataset(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")\n","batch_size = 64\n","#print(f\"Train set shape: {nb15_dataset.train_set.shape}\")\n","#print(f\"Val set shape: {nb15_dataset.val_set.shape}\")\n","#print(f\"Test set shape: {nb15_dataset.test_set.shape}\")\n","print(nb15_dataset.train_mean)\n","print(nb15_dataset.train_std)\n","np.random.shuffle(nb15_dataset.train_set)\n","batches = [nb15_dataset.train_set[i:i+batch_size] for i in range(0, nb15_dataset.train_set.shape[0], batch_size)]\n","\n","# Check shapes of initial 3 batches.\n","for idx, batch in enumerate(batches[:3]):\n","    labels = batch[:, -1].reshape(-1, 1)\n","    features = np.expand_dims(batch[:, :-1], axis=-1)\n","    print(f\"\\nBatch {idx+1} shape:\", batch.shape)\n","    print(f\"Batch {idx+1} feature shape:\", features.shape)\n","    print(f\"Batch {idx+1} label shape:\", labels.shape)"],"metadata":{"id":"EpksctcIiBjB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745224887035,"user_tz":-60,"elapsed":1567,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"7d244e2b-7420-4f70-ccb7-7baadb5b5b62"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.01006994]]\n","[[0.07261028]]\n","\n","Batch 1 shape: (64, 43)\n","Batch 1 feature shape: (64, 42, 1)\n","Batch 1 label shape: (64, 1)\n","\n","Batch 2 shape: (64, 43)\n","Batch 2 feature shape: (64, 42, 1)\n","Batch 2 label shape: (64, 1)\n","\n","Batch 3 shape: (64, 43)\n","Batch 3 feature shape: (64, 42, 1)\n","Batch 3 label shape: (64, 1)\n"]}]},{"cell_type":"code","source":["dummy_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n","for _, _, c in dummy_tuples:\n","    c = c * 10\n","\n","print(dummy_tuples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtVP4_0pB2mp","executionInfo":{"status":"ok","timestamp":1745181637977,"user_tz":-60,"elapsed":15,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"faea4879-782b-49e0-c8b2-c5408643d2da"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n"]}]},{"cell_type":"markdown","source":["# ***2. Model Implementation and Training (Task 2)***"],"metadata":{"id":"u4uQmeM_Albh"}},{"cell_type":"markdown","source":["TO-DO manually in numpy in order to implement MLP-Mixer:\n","- [x] Linear layer w/ manual backprop and init weight & bias w/ Kaiming uniform distribution\n","- [x] DropOut=0.1 w/ manual backprop\n","- [x] Stochastic Depth={0->0.1}\n","- [x] LayerNorm w/ manual backprop\n","- [x] GeLU or Tanh w/ manual backprop\n","- [x] TokenMixer block\n","- [x] ChannelMixer block\n","- [x] Adam optimiser w/ β1 = 0.9, β2 = 0.999, weight decay = 0.03, lr=0.003\n","- [x] Binary Cross Entropy with logits\n","- [x] Optimiser.zero_grad()\n","- [x] Gradient accumulation for more desired batch size = 4096"],"metadata":{"id":"2X0Rnga6nlrw"}},{"cell_type":"code","source":["input_dim = 1 if nb15_dataset.train_set.ndim == 2 else nb15_dataset.train_set.shape[-1]\n","print(input_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsrV842EgbH2","executionInfo":{"status":"ok","timestamp":1745224909789,"user_tz":-60,"elapsed":16,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"ca58aeca-188d-4ab6-8057-c47bffc20796"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"code","source":["print(nb15_dataset.train_set.shape[-1] - 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IehnlBNsuZjq","executionInfo":{"status":"ok","timestamp":1745181707415,"user_tz":-60,"elapsed":9,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"730e73b4-fdab-4e55-fe65-f1190d10128e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["42\n"]}]},{"cell_type":"code","source":["############################################################\n","#put your code for model implementation and training here\n","############################################################\n","\n","import numpy as np\n","import math\n","from scipy.special import erf\n","\n","\n","# ---------- Init Linear weight and layer with Kaiming uniform distribution ----------\n","def calculate_gain(nonlinearity, param=None):\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L72\n","    if nonlinearity in ['linear', 'conv1d', 'conv2d', 'conv3d',\n","                        'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'sigmoid']:\n","        return 1.0\n","    elif nonlinearity == 'tanh':\n","        return 5.0 / 3\n","    elif nonlinearity == 'relu':\n","        return math.sqrt(2.0)\n","    elif nonlinearity == 'leaky_relu':\n","        negative_slope = 0.01 if param is None else param\n","        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n","    else:\n","        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n","\n","\n","def _calculate_correct_fan(tensor, mode):\n","    # Inspired from _calculate_fan_in_and_fan_out: https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L345\n","    # Note that pytorch uses _calculate_correct_fan as API for _calculate_fan_in_and_fan_out so I merely remove the API\n","    if tensor.ndim < 2:\n","        raise ValueError(\"Fan in and fan out cannot be computed for tensor with fewer than 2 dimensions\")\n","    if mode == \"fan_in\":\n","        num_input_fmaps = tensor.shape[1]\n","        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n","        return num_input_fmaps * receptive_field_size\n","    elif mode == \"fan_out\":\n","        num_output_fmaps = tensor.shape[0]\n","        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n","        return num_output_fmaps * receptive_field_size\n","    else:\n","        raise ValueError(\"mode must be either 'fan_in' or 'fan_out'\")\n","\n","\n","def kaiming_uniform_(tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"):\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L456\n","    fan = _calculate_correct_fan(tensor, mode)\n","    gain = calculate_gain(nonlinearity, a)\n","    std = gain / math.sqrt(fan)\n","    bound = math.sqrt(3.0) * std\n","    tensor[:] = np.random.uniform(-bound, bound, tensor.shape).astype(np.float32)\n","    return tensor\n","\n","\n","class Linear:\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py#L50\n","    def __init__(self, in_features, out_features, bias=True):\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = np.empty((out_features, in_features), dtype=np.float32)\n","        self.bias = np.empty((out_features,), dtype=np.float32) if bias else None\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        kaiming_uniform_(self.weight, a=math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n","        if self.bias is not None:\n","            fan_in = _calculate_correct_fan(self.weight, \"fan_in\")\n","            bound = 1 / math.sqrt(fan_in)\n","            self.bias[:] = np.random.uniform(-bound, bound, self.bias.shape).astype(np.float32)\n","\n","    def forward(self, input):\n","        \"\"\"\n","            input (batch_size, N, in_features)\n","\n","            Returns np.ndarray (batch_size, N, out_features)\n","        \"\"\"\n","\n","        #output = np.einsum('bni,oi->bno', input, self.weight)\n","        #if self.bias is not None:\n","        #    output += self.bias.reshape(1, 1, -1)\n","        #return output\n","\n","        # Original implementation # Observed no difference between einsum and original implementation\n","        return np.matmul(np.ascontiguousarray(input), self.weight.T) + (self.bias if self.bias is not None else 0)\n","\n","    def backward(self, grad_output, input):\n","        \"\"\"\n","            grad_output (np.ndarray): Gradient w.r.t. output with shape (B, N, out_features)\n","            input (np.ndarray): Original input with shape (B, N, in_features)\n","\n","            Returns:\n","                grad_input (np.ndarray): Gradient w.r.t. input with shape (B, N, in_features)\n","                grad_weight (np.ndarray): Gradient w.r.t. weight with shape (out_features, in_features)\n","                grad_bias (np.ndarray or None): Gradient w.r.t. bias with shape (out_features,)\n","        \"\"\"\n","        grad_input = np.matmul(grad_output, self.weight)\n","        if input.ndim == 2:\n","            self.grad_weight = np.einsum('bi,bj->ij', grad_output, input)\n","            self.grad_bias = np.sum(grad_output, axis=0) if self.bias is not None else None\n","        elif input.ndim == 3:\n","            self.grad_weight = np.einsum('bnk,bnl->kl', grad_output, input)\n","            self.grad_bias = np.sum(grad_output, axis=(0, 1))\n","        # store self.grad_weight and self.grad_bias for optimiser.step later on\n","        return grad_input, self.grad_weight, self.grad_bias\n","\n","\n","eps = 1e-5\n","class LayerNorm:\n","    def __init__(self, dim):\n","        self.dim = dim\n","        self.weight = np.ones((dim,), dtype=np.float32)\n","        self.bias = np.zeros((dim,), dtype=np.float32)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: (B, N, embed_dim)\n","        Returns:\n","            out: layer-normalized activations, shape (B, T, embed_dim)\n","            cache: tuple of (x, self.w, mean, rstd) for backward pass.\n","        \"\"\"\n","        B, T, C = x.shape\n","        mean = np.sum(x, axis=-1, keepdims=True) / C           # (B, T, 1)\n","        xshift = x - mean\n","        var = np.sum(xshift ** 2, axis=-1, keepdims=True) / C  # (B, T, 1)\n","        rstd = (var + eps) ** (-0.5)                           # (B, T, 1)\n","        norm = xshift * rstd\n","        out = norm * self.weight + self.bias\n","        cache = (x, self.weight, mean, rstd)\n","        return out, cache\n","\n","    def backward(self, dout, cache):\n","        \"\"\"\n","        dout: upstream gradients, shape (B, N, embed_dim)\n","        cache: tuple of (x, w, mean, rstd) from forward pass.\n","        Returns:\n","            dx: gradient with respect to x, shape (B, T, embed_dim)\n","            dw: gradient with respect to self.w, shape (embed_dim,)\n","            db: gradient with respect to self.b, shape (embed_dim,)\n","        \"\"\"\n","        x, w, mean, rstd = cache\n","        norm = (x - mean) * rstd\n","        db = np.sum(dout, axis=(0, 1))\n","        dw = np.sum(dout * norm, axis=(0, 1))\n","        dnorm = dout * w\n","        dnorm_mean = np.mean(dnorm, axis=-1, keepdims=True)\n","        dx = dnorm - dnorm_mean - norm * np.mean(dnorm * norm, axis=-1, keepdims=True)\n","        dx *= rstd\n","        self.grad_weight = dw\n","        self.grad_bias = db\n","        return dx, dw, db\n","\n","\n","class Dropout:\n","    # Inspired from https://gist.github.com/nbertagnolli/35eb960d08c566523b4da599f6099b41\n","    # and https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L35\n","    def __init__(self, p=0.5):\n","        if p < 0 or p > 1:\n","            raise ValueError(\"p must be a probability in [0, 1].\")\n","        self.p = p\n","        self.training = True\n","        self.mask = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (np.ndarray): Input array of any shape.\n","\n","        Returns:\n","            np.ndarray: Output array with dropout applied.\n","        \"\"\"\n","        if self.training:\n","            # Only generate a new mask if one hasn't been set externally.\n","            if self.mask is None or self.mask.shape != x.shape:\n","                self.mask = (np.random.rand(*x.shape) >= self.p).astype(x.dtype)\n","            return (x * self.mask) / (1 - self.p)\n","        else:\n","            self.mask = None\n","            return x\n","\n","    def backward(self, grad_output):\n","        if self.training:\n","            if self.mask is None:\n","                raise ValueError(\"Must run forward pass before backward pass in training mode.\")\n","            return (grad_output * self.mask) / (1 - self.p)\n","        else:\n","            return grad_output\n","\n","\n","class Reduce_np:\n","    # Practically the same as AveragePooling layer along specified axis\n","    def __init__(self, axis, reduction='mean'):\n","        self.axis = axis\n","        self.reduction = reduction\n","\n","    def forward(self, x):\n","        self.last_input = x.copy()\n","        if self.reduction == 'mean':\n","            return np.mean(x, axis=self.axis)\n","        else:\n","            raise NotImplementedError()\n","\n","    def backward(self, grad_output):\n","        shape = self.last_input.shape                         # (B, N, dim)\n","        scale = shape[self.axis]                              # axis = N in MLP_mixer\n","        grad_input = grad_output / scale\n","        grad_input = grad_input.reshape(grad_input.shape[0], 1, grad_input.shape[1])  # Reshape to (B, 1, dim) to broadcast along N dim\n","        return np.broadcast_to(grad_input, shape)             # broadcasts to (B, N, dim)\n","\n","\n","# ---------- Activation Function ----------\n","class GELU:\n","    # Inspired from https://github.com/oniani/ai/blob/main/activation/gelu.py#L4\n","    def __init__(self):\n","        self.ctx = {}\n","\n","    def forward(self, input):\n","        cdf = 0.5 * (1 + erf(input / np.sqrt(2)))\n","        self.ctx['input'] = input.copy()\n","        self.ctx['cdf'] = cdf.copy()\n","        return input * cdf\n","\n","    def backward(self, grad_output):\n","        input = self.ctx['input']\n","        cdf = self.ctx['cdf']\n","        pdf_val = 1 / np.sqrt(2 * np.pi) * np.exp(-0.5 * np.power(input, 2))\n","        grad_local = cdf + input * pdf_val\n","\n","        return grad_output * grad_local\n","\n","\n","class Tanh:\n","    def __init__(self):\n","        self.ctx = {}\n","\n","    def forward(self, input):\n","        output = np.tanh(input)\n","        self.ctx['output'] = output.copy()\n","        return output\n","\n","    def backward(self, grad_output):\n","        output = self.ctx['output']\n","        grad_input = grad_output * (1 - output**2)\n","        return grad_input\n","\n","\n","# ---------- Blocks ----------\n","class PreNormResidual:\n","    # Inspired from https://github.com/lucidrains/mlp-mixer-pytorch/blob/main/mlp_mixer_pytorch/mlp_mixer_pytorch.py#L7\n","    \"\"\"\n","    PreNorm + residual connection with stochastic depth (DropPath).\n","    Each forward pass, with probability drop_prob the residual branch is skipped.\n","    When kept, its output is scaled by 1/(1 - drop_prob) to preserve expectation.\n","    \"\"\"\n","    def __init__(self, dim, fn, drop_prob=0.0):\n","        self.norm = LayerNorm(dim)\n","        self.fn = fn\n","        self.drop_prob = drop_prob\n","        self.training = True\n","        self.cache = {}\n","\n","    def forward(self, x):\n","        # LayerNorm\n","        norm_x, norm_cache = self.norm.forward(x)\n","        # Apply function\n","        f_out = self.fn.forward(norm_x)\n","        # Decide whether to drop the branch\n","        if self.training and np.random.rand() < self.drop_prob:\n","            # dropped: skip branch\n","            out = x\n","            keep = False\n","        else:\n","            # kept: scale to preserve expected value\n","            out = x + f_out / (1 - self.drop_prob)\n","            keep = True\n","        # store caches for backward, including f_out for skip-case shape\n","        self.cache = {\n","            'x': x,\n","            'norm_cache': norm_cache,\n","            'f_out': f_out,\n","            'fn_cache': getattr(self.fn, 'cache', None),\n","            'keep': keep\n","        }\n","        return out\n","\n","    def backward(self, d_out):\n","        keep = self.cache['keep']\n","        x = self.cache['x']\n","        norm_cache = self.cache['norm_cache']\n","\n","        if not keep:\n","            # branch was dropped: set gradients in norm and fn to zero\n","            # zero gradient flowing into residual branch\n","            f_out = self.cache['f_out']\n","            # backprop zeros through fn to produce zero grads on its parameters\n","            d_zero = np.zeros_like(f_out)\n","            d_fn, f_params_grad = self.fn.backward(d_zero)\n","            # backprop zeros through norm\n","            # norm.backward expects (dout, cache) -> (dx, dw, db)\n","            d_norm, norm_dw, norm_db = self.norm.backward(np.zeros_like(d_fn), norm_cache)\n","            # gradient flows only through the skip connection\n","            d_x = d_out\n","        else:\n","            # branch kept: backprop through scaled residual\n","            # merge grad for fn\n","            d_res = d_out / (1 - self.drop_prob)\n","            # backward through fn\n","            d_f, f_params_grad = self.fn.backward(d_res)\n","            # backward through norm\n","            d_norm, norm_dw, norm_db = self.norm.backward(d_f, norm_cache)\n","            # total gradient to x\n","            d_x = d_norm + d_out\n","        # store norm grads for optimiser\n","        self.norm.grad_weight = norm_dw\n","        self.norm.grad_bias = norm_db\n","        return d_x, norm_dw, norm_db, f_params_grad\n","\n","\n","# TokenMixing operates along N dimension\n","class TokenMixing:\n","    def __init__(self, num_tokens, expansion_factor, dropout_p):\n","        inner_dim = int(num_tokens * expansion_factor)\n","        self.linear1 = Linear(num_tokens, inner_dim)\n","        self.activation_fn = GELU()\n","        # self.gelu = GELU()\n","        self.dropout1 = Dropout(dropout_p)\n","        self.linear2 = Linear(inner_dim, num_tokens)\n","        self.dropout2 = Dropout(dropout_p)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x_t = x transposed\n","        \"\"\"\n","        cache = {}\n","        x_t = np.transpose(x, (0, 2, 1))\n","        cache['x_t'] = x_t.copy()\n","        out_l1 = self.linear1.forward(x_t)\n","        cache['out_l1'] = out_l1.copy()\n","        #out_gelu = self.gelu.forward(out_l1)\n","        out_activation_fn = self.activation_fn.forward(out_l1)\n","        # cache['out_gelu'] = out_gelu.copy()\n","        out_drop1 = self.dropout1.forward(out_activation_fn)\n","        cache['out_drop1'] = out_drop1.copy()\n","        out_l2 = self.linear2.forward(out_drop1)\n","        cache['out_l2'] = out_l2.copy()\n","        out_drop2 = self.dropout2.forward(out_l2)\n","        cache['out_drop2'] = out_drop2.copy()\n","        out = np.transpose(out_drop2, (0, 2, 1))\n","        self.cache = cache\n","        return out\n","\n","    def backward(self, d_out):\n","        cache = self.cache\n","        d_drop2 = np.transpose(d_out, (0, 2, 1))\n","        d_l2 = self.dropout2.backward(d_drop2)\n","        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_l2, cache['out_drop1'])\n","        # d_gelu = self.dropout1.backward(d_drop1)\n","        d_activation_fn = self.dropout1.backward(d_drop1)\n","        # d_l1 = self.gelu.backward(d_gelu)\n","        d_x_t, grad_w1, grad_b1 = self.linear1.backward(self.activation_fn.backward(d_activation_fn), cache['x_t'])\n","        d_x = np.transpose(d_x_t, (0, 2, 1))\n","        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n","\n","        # Reset cache to prevent memory leak\n","        cache = {}\n","        return d_x, self.params_grad\n","\n","\n","# ChannelMixing operates along embedding dimension\n","class ChannelMixing:\n","    def __init__(self, dim, expansion_factor, dropout_p):\n","        inner_dim = int(dim * expansion_factor)\n","        self.linear1 = Linear(dim, inner_dim)\n","        # self.gelu = GELU()\n","        self.activation_fn = GELU()\n","        self.dropout1 = Dropout(dropout_p)\n","        self.linear2 = Linear(inner_dim, dim)\n","        self.dropout2 = Dropout(dropout_p)\n","\n","    def forward(self, x):\n","        cache = {}\n","        cache['x'] = x.copy()\n","        out_l1 = self.linear1.forward(x)\n","        cache['out_l1'] = out_l1.copy()\n","        # out_gelu = self.gelu.forward(out_l1)\n","        out_activation_fn = self.activation_fn.forward(out_l1)\n","        # cache['out_gelu'] = out_gelu.copy()\n","        out_drop1 = self.dropout1.forward(out_activation_fn)\n","        cache['out_drop1'] = out_drop1.copy()\n","        out_l2 = self.linear2.forward(out_drop1)\n","        cache['out_l2'] = out_l2.copy()\n","        out_drop2 = self.dropout2.forward(out_l2)\n","        #cache['out_drop2'] = out_drop2.copy()\n","        self.cache = cache\n","        return out_drop2\n","\n","    def backward(self, d_out):\n","        cache = self.cache\n","        d_drop2 = self.dropout2.backward(d_out)\n","        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_drop2, cache['out_drop1'])\n","        d_activation_fn = self.dropout1.backward(d_drop1)\n","        # d_gelu = self.dropout1.backward(d_drop1)\n","        d_l1, grad_w1, grad_b1 = self.linear1.backward(self.activation_fn.backward(d_activation_fn), cache['x'])\n","        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n","\n","        # Reset cache to prevent memory leak\n","        cache = {}\n","        return d_l1, self.params_grad\n","\n","\n","# ---------- Model ----------\n","class MLPMixer:\n","    def __init__(self, num_tokens, input_dim, dim, depth, num_classes,\n","                 expansion_factor=4, expansion_factor_token=0.5, dropout_p=0., stochastic_depth_p=0.0):\n","        \"\"\"\n","        Args:\n","            num_tokens (int): Number of tokens (N).\n","            input_dim (int): Input token dimension.\n","            dim (int): Mixer embedding dimension.\n","            depth (int): Number of mixer layers.\n","            num_classes (int): Number of classes for classification.\n","            expansion_factor (float): Expansion factor for token mixing.\n","            expansion_factor_token (float): Expansion factor for channel mixing.\n","            dropout_p (float): Dropout probability.\n","        \"\"\"\n","        self.num_tokens = num_tokens\n","        self.input_dim = input_dim\n","        self.dim = dim\n","        self.depth = depth\n","        self.num_classes = num_classes\n","        self.dropout_p = dropout_p\n","        self.proj = Linear(input_dim, dim) if input_dim != dim else None\n","        self.mixer_layers = []\n","        for i in range(depth):\n","            drop_prob = stochastic_depth_p * (i / max(depth - 1, 1))\n","            token_mixing = PreNormResidual(dim, fn=TokenMixing(num_tokens, expansion_factor, dropout_p), drop_prob=drop_prob)\n","            channel_mixing = PreNormResidual(dim, fn=ChannelMixing(dim, expansion_factor_token, dropout_p), drop_prob=drop_prob)\n","            self.mixer_layers.append((token_mixing, channel_mixing))\n","        self.layer_norm = LayerNorm(dim)\n","        self.reduce = Reduce_np(axis=1, reduction='mean')\n","        self.classifier = Linear(dim, num_classes)\n","        self.cache = {}\n","\n","    def forward(self, x):\n","        \"\"\"\n","        if self.proj is not None:\n","            self.cache['proj_in'] = x.copy()\n","            x = self.proj.forward(x)\n","        else:\n","            self.cache['proj_in'] = None\n","        self.cache['mixer'] = []\n","\n","        for token_mixing, channel_mixing in self.mixer_layers:\n","            mixer_cache = {}\n","            mixer_cache['in'] = x.copy()\n","            t_out = token_mixing.forward(x)\n","            mixer_cache['token_cache'] = token_mixing.cache\n","            c_out = channel_mixing.forward(t_out)\n","            mixer_cache['channel_cache'] = channel_mixing.cache\n","            self.cache['mixer'].append(mixer_cache)\n","            x = c_out\n","\n","        self.cache['ln_in'] = x.copy()\n","        x_ln, ln_cache = self.layer_norm.forward(x)\n","        self.cache['ln_cache'] = ln_cache\n","        self.cache['reduce_in'] = x_ln.copy()\n","        x_red = self.reduce.forward(x_ln)\n","        self.cache['clf_in'] = x_red.copy()\n","        x_cls = self.classifier.forward(x_red)\n","        self.cache['output'] = x_cls.copy()\n","        return x_cls\n","        \"\"\"\n","        for tm, cm in self.mixer_layers:\n","            tm.training = cm.training = getattr(x, 'training', True)\n","        if self.proj is not None:\n","            self.cache['proj_in'] = x.copy()\n","            x = self.proj.forward(x)\n","        else:\n","            self.cache['proj_in'] = None\n","        self.cache['mixer'] = []\n","\n","        for token_mixing, channel_mixing in self.mixer_layers:\n","            mixer_cache = {}\n","            mixer_cache['in'] = x.copy()\n","            t_out = token_mixing.forward(x)\n","            mixer_cache['token_keep'] = token_mixing.cache['keep']\n","            mixer_cache['token_cache'] = token_mixing.cache\n","            c_out = channel_mixing.forward(t_out)\n","            mixer_cache['channel_keep'] = channel_mixing.cache['keep']\n","            mixer_cache['channel_cache'] = channel_mixing.cache\n","            self.cache['mixer'].append(mixer_cache)\n","            x = c_out\n","\n","        self.cache['ln_in'] = x.copy()\n","        x_ln, ln_cache = self.layer_norm.forward(x)\n","        self.cache['ln_cache'] = ln_cache\n","        self.cache['reduce_in'] = x_ln.copy()\n","        x_red = self.reduce.forward(x_ln)\n","        self.cache['clf_in'] = x_red.copy()\n","        x_cls = self.classifier.forward(x_red)\n","        self.cache['output'] = x_cls.copy()\n","        return x_cls\n","\n","    def backward(self, grad_output):\n","        d_clf, _ , _ = self.classifier.backward(grad_output, self.cache['clf_in'])\n","        d_reduce = self.reduce.backward(d_clf)\n","        d_ln, _ , _ = self.layer_norm.backward(d_reduce, self.cache['ln_cache'])\n","        grad = d_ln\n","\n","        for (token_mixing, channel_mixing), mixer_cache in zip(self.mixer_layers[::-1],\n","                                                               self.cache['mixer'][::-1]):\n","            grad, _, _, _ = channel_mixing.backward(grad)\n","            grad, _, _, _ = token_mixing.backward(grad)\n","\n","        if self.proj is not None:\n","            grad, _ , _ = self.proj.backward(grad, self.cache['proj_in'])\n","\n","        return grad\n","\n","\n","# ---------- Loss ----------\n","class BCEWithLogits_np:\n","    # Inspired from https://github.com/pytorch/pytorch/blob/c64e006fc399d528bb812ae589789d0365f3daf4/aten/src/ATen/native/Loss.cpp#L214-L259\n","    # binary cross-entropy loss in a more numerically stable way using log-sum-exp when integrating sigmoid ops under this class\n","    def __init__(self, weight=None, pos_weight=None, reduction='mean'):\n","        \"\"\"\n","        Args:\n","            weight (np.ndarray or None): Optional weight tensor, broadcastable to input.\n","            pos_weight (np.ndarray or None): Optional weight for positive examples.\n","            reduction (str): 'mean' or 'sum'\n","        \"\"\"\n","        self.weight = weight\n","        self.pos_weight = pos_weight\n","        self.reduction = reduction\n","        self.cache = {}\n","\n","    def forward(self, input, target):\n","        \"\"\"\n","        Args:\n","            input (np.ndarray): logits (B, 2)\n","            target (np.ndarray): labels (B, 2)\n","        Returns:\n","            loss (float or np.ndarray): reduced loss value if reduction is 'mean' or 'sum'; elementwise loss if reduction is 'none'.\n","        \"\"\"\n","        max_val = np.maximum(-input, 0)\n","\n","        if self.pos_weight is not None:\n","            log_weight = 1 + (self.pos_weight - 1) * target\n","            lse = np.log(np.exp(-max_val) + np.exp(-input - max_val))\n","            loss = (1 - target) * input + log_weight * (lse + max_val)\n","        else:\n","            loss = (1 - target) * input + max_val + np.log(np.exp(-max_val) + np.exp(-input - max_val))\n","\n","        if self.weight is not None:\n","            loss = loss * self.weight\n","\n","        self.cache['input'] = input.copy()\n","        self.cache['target'] = target.copy()\n","\n","        if self.reduction == 'mean':\n","            return np.mean(loss)\n","        elif self.reduction == 'sum':\n","            return np.sum(loss)\n","\n","    def backward(self, grad_output=1.0):\n","        input = self.cache['input']\n","        target = self.cache['target']\n","\n","        # Compute sigmoid(input) in a stable way.\n","        sig = 1 / (1 + np.exp(-input))\n","        if self.pos_weight is not None:\n","            grad_input = (sig * (self.pos_weight * target + (1 - target)) - self.pos_weight * target)\n","        else:\n","            grad_input = sig - target\n","\n","        if self.weight is not None:\n","            grad_input = grad_input * self.weight\n","\n","        # If reduction is mean, scale the gradient by 1/numel.\n","        if self.reduction == 'mean':\n","            grad_input = grad_input * grad_output / input.size\n","        else:\n","            grad_input = grad_input * grad_output\n","\n","        return grad_input\n","\n","\n","# ---------- Optimiser ----------\n","class ADAM_np:\n","    # Inspired from https://gist.github.com/aerinkim/dfe3da1000e67aced1c7d9279351cb88\n","    \"\"\"\n","        Adam optimizer using NumPy, operating on a list of parameter 5-tuples:\n","            (p, param_attr, mod, grad, grad_attr)\n","        where:\n","        - p is the parameter array (e.g. mod.weight or mod.bias)\n","        - param_attr is \"weight\" or \"bias\"\n","        - mod is the module that holds this parameter\n","        - grad is the current gradient stored in mod (may be None if not computed yet)\n","        - grad_attr is the name of the attribute where the gradient is stored (e.g. \"grad_weight\")\n","    \"\"\"\n","    def __init__(self, model, lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n","        self.param_tuples = self.get_pointers_to_param_and_grad(model)\n","        self.lr = lr\n","        self.beta1, self.beta2 = betas\n","        self.eps = eps\n","        self.weight_decay = weight_decay\n","\n","        # Create a state for each parameter\n","        self.state = {}\n","        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n","            self.state[id(p)] = {\n","                'step': 0,\n","                'exp_avg': np.zeros_like(p),\n","                'exp_avg_sq': np.zeros_like(p)\n","            }\n","\n","    def step(self):\n","        \"\"\"\n","        Update p in each tuple in self.param_tuples provided grad\n","        Also update state\n","        \"\"\"\n","        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n","            # Retrieve the gradient dynamically\n","            grad = getattr(mod, grad_attr)\n","            state = self.state[id(p)]\n","            state['step'] += 1\n","\n","            # Apply weight decay (L2 regularization)\n","            if self.weight_decay != 0:\n","                grad = grad + self.weight_decay * p\n","\n","            # Update exponential moving averages.\n","            state['exp_avg'] = self.beta1 * state['exp_avg'] + (1 - self.beta1) * grad\n","            state['exp_avg_sq'] = self.beta2 * state['exp_avg_sq'] + (1 - self.beta2) * (grad * grad)\n","            denom = np.sqrt(state['exp_avg_sq']) + self.eps\n","\n","            # Bias corrections.\n","            bias_correction1 = 1 / (1 - self.beta1 ** state['step'])\n","            bias_correction2 = 1 / (1 - self.beta2 ** state['step'])\n","            adapted_lr = self.lr * bias_correction1 / math.sqrt(bias_correction2)\n","\n","            # Update parameter in place.\n","            p[:] = p - adapted_lr * (state['exp_avg'] / denom)\n","\n","    def copy_updated_params(self):\n","        \"\"\"\n","        Explicitly copy the updated parameter array p back to the module's parameters in place\n","        because I don't trust in-place parameters updating from optimiser.step() and it turnt out to be a good call\n","        \"\"\"\n","        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n","            if param_attr == \"weight\" and hasattr(mod, \"weight\"):\n","                mod.weight[...] = np.copy(p)\n","            elif param_attr == \"bias\" and hasattr(mod, \"bias\"):\n","                mod.bias[...] = np.copy(p)\n","\n","        # Optimiser.zero_grad()\n","        self.param_tuples = {}\n","\n","    def get_pointers_to_param_and_grad(self, module, visited=None):\n","        \"\"\"\n","        Recursively traverses the module hierarchy and returns this tuple in the exact order:\n","        (p, param_attr, module, grad, grad_attr)\n","        - p is np.float32 parameter, either weight or bias of linear layer or layernorm,\n","        - param_attr is a string that tells where to put p in optimiser.step equation\n","        - module is reference pointer to the module that holds this parameter\n","        - grad is np.float32 parameter, either grad_weight or grad_bias of linear layer or layernorm\n","        - grad_attr is a string that tells where to put grad in optimiser.step equation\n","\n","        Skips keys that are irrelevant or duplicative (like params_grad) during recursive search\n","        \"\"\"\n","        if visited is None:\n","            visited = set()\n","        tuples = []\n","        if id(module) in visited:\n","            return tuples\n","        visited.add(id(module))\n","\n","        # --- Get reference pointers to the weight and grad_weight, or bias and grad_bias of a linear layer\n","        if hasattr(module, 'weight') and isinstance(module.weight, np.ndarray):\n","            tuples.append((getattr(module, 'weight', None),\n","                        \"weight\",\n","                        module,\n","                        getattr(module, 'grad_weight', None),\n","                        \"grad_weight\")\n","                        )\n","        if hasattr(module, 'bias') and module.bias is not None and isinstance(module.bias, np.ndarray):\n","            tuples.append((getattr(module, 'bias', None),\n","                        \"bias\",\n","                        module,\n","                        getattr(module, 'grad_bias', None),\n","                        \"grad_bias\")\n","                        )\n","\n","        # ---  Get reference pointers to the weight and grad_weight, or bias and grad_bias of a layernorm\n","        if hasattr(module, 'norm'):\n","            norm_mod = module.norm\n","            if hasattr(norm_mod, 'weight') and isinstance(norm_mod.weight, np.ndarray):\n","                tuples.append((getattr(norm_mod, 'weight', None),\n","                            \"weight\",\n","                            norm_mod,\n","                            getattr(norm_mod, 'grad_weight', None),\n","                            \"grad_weight\")\n","                            )\n","            if hasattr(norm_mod, 'bias') and norm_mod.bias is not None and isinstance(norm_mod.bias, np.ndarray):\n","                tuples.append((getattr(norm_mod, 'bias', None),\n","                            \"bias\",\n","                            norm_mod,\n","                            getattr(norm_mod, 'grad_bias', None),\n","                            \"grad_bias\")\n","                            )\n","\n","        # --- Recursively explore submodules stored as attributes.\n","        for key, value in module.__dict__.items():\n","            if key in {'cache', 'ctx', 'params_grad'}:\n","                continue\n","            if hasattr(value, 'forward') and callable(value.forward) and value is not module:\n","                tuples.extend(self.get_pointers_to_param_and_grad(value, visited))\n","            elif isinstance(value, (list, tuple)):\n","                for item in value:\n","                    if hasattr(item, 'forward') and callable(item.forward):\n","                        tuples.extend(self.get_pointers_to_param_and_grad(item, visited))\n","                    elif isinstance(item, (list, tuple)):\n","                        for subitem in item:\n","                            if hasattr(subitem, 'forward') and callable(subitem.forward):\n","                                tuples.extend(self.get_pointers_to_param_and_grad(subitem, visited))\n","\n","        self.param_tuples = tuples\n","        return tuples\n","\n","    def refresh_grad_per_backprop(self, model):\n","        self.param_tuples = self.get_pointers_to_param_and_grad(model)\n","\n","\n","class CosineAnnealingLR_np:\n","    # Inspired from linear warmup: https://github.com/Tony-Y/pytorch_warmup\n","    # and CosineAnnealingLR from pytorch: https://github.com/pytorch/pytorch/blob/v2.6.0/torch/optim/lr_scheduler.py#L1046\n","    def __init__(self,\n","                optimiser,\n","                T_max,\n","                eta_min=0.0,\n","                warmup_epochs=0,\n","                warmup_start_lr=0.0,\n","                last_epoch=-1):\n","        \"\"\"\n","        Args:\n","            optimizer:       instance of ADAM_np (must have .lr and .initial_lr kept)\n","            T_max (int):     number of cosine-decay epochs\n","            eta_min (float): floor LR after decay\n","            warmup_epochs(int): # of epochs to ramp up\n","            warmup_start_lr(float): LR at epoch=0 of warmup\n","            last_epoch (int): index of last epoch (so first .step() yields epoch=0)\n","        \"\"\"\n","        self.optimiser        = optimiser\n","        self.T_max            = T_max\n","        self.eta_min          = eta_min\n","        self.warmup_epochs    = warmup_epochs\n","        self.warmup_start_lr  = warmup_start_lr\n","        self.initial_lr       = optimiser.lr\n","        self.last_epoch       = last_epoch\n","\n","        if self.last_epoch >= 0:\n","            self.step()\n","\n","    def step(self):\n","        \"\"\"\n","        for each call, update ADAM_np as follows:\n","            lr = eta_min + 0.5*(initial_lr - eta_min)*(1 + cos(pi * epoch / T_max))\n","            if epoch > T_max, lr == eta_min.\n","        \"\"\"\n","        self.last_epoch += 1\n","\n","        # 1) Linear warmup phase\n","        if self.last_epoch < self.warmup_epochs:\n","            alpha = (self.last_epoch + 1) / float(self.warmup_epochs)\n","            new_lr = self.warmup_start_lr + alpha * (self.initial_lr - self.warmup_start_lr)\n","\n","        # 2) Cosine decay phase\n","        elif self.last_epoch <= self.warmup_epochs + self.T_max:\n","            # offset epoch for the cosine schedule\n","            epoch_in_cosine = self.last_epoch - self.warmup_epochs\n","            cos_term = math.cos(math.pi * epoch_in_cosine / self.T_max)\n","            new_lr = self.eta_min + 0.5 * (self.initial_lr - self.eta_min) * (1 + cos_term)\n","\n","        # 3) eta_min phase\n","        else:\n","            new_lr = self.eta_min\n","\n","        self.optimiser.lr = new_lr\n","        return new_lr\n","\n","\n","def clip_grad_global_norm(param_tuples, max_norm=1.0, eps=1e-6):\n","    \"\"\"\n","    This is gradient clipping. It scales down grad a bit like how weight decay scales down grad\n","    \"\"\"\n","    # Compute squared norm\n","    total_norm_sq = 0.0\n","    for _, _, _, grad, _ in param_tuples:\n","        if grad is not None:\n","            total_norm_sq += np.sum(grad * grad)\n","    total_norm = math.sqrt(total_norm_sq)\n","\n","    clip_coef = max_norm / (total_norm + eps)\n","\n","    # scale all gradients by squared norm gradient_max_norm\n","    if clip_coef < 1.0:\n","        for i, (p, param_attr, mod, grad, grad_attr) in enumerate(param_tuples):\n","            if grad is not None:\n","                new_grad = grad * clip_coef\n","                param_tuples[i] = (p, param_attr, mod, new_grad, grad_attr)\n","\n","\n","# ----------Model.zero_grad() ----------\n","def clear_auxiliary(module):\n","    \"\"\"\n","    Recursively clears auxiliary attributes from the module and its submodules.\n","    This includes attributes with keys 'cache', 'ctx', 'params_grad', and 'mask'.\n","\n","    For each key:\n","      - If its value is a dict, it is set to {}.\n","      - Otherwise, it is set to None.\n","    \"\"\"\n","    aux_keys = ('cache', 'ctx', 'params_grad', 'mask')\n","    # Clear any auxiliary attribute on this module.\n","    for key in aux_keys:\n","        if key in module.__dict__:\n","            current = module.__dict__[key]\n","            if isinstance(current, dict):\n","                setattr(module, key, {})\n","            else:\n","                setattr(module, key, None)\n","\n","    # Now recursively traverse submodules.\n","    for key, value in module.__dict__.items():\n","        # Skip aux keys already handled.\n","        if key in aux_keys:\n","            continue\n","\n","        # If the attribute is a list or tuple, iterate through its items.\n","        if isinstance(value, (list, tuple)):\n","            for item in value:\n","                if isinstance(item, tuple):\n","                    for subitem in item:\n","                        if hasattr(subitem, 'forward') and callable(subitem.forward):\n","                            clear_auxiliary(subitem)\n","                elif hasattr(value, 'forward') and callable(value.forward):\n","                    # Unlikely: this checks if the entire list itself has a forward().\n","                    clear_auxiliary(value)\n","                elif hasattr(item, 'forward') and callable(item.forward):\n","                    clear_auxiliary(item)\n","        # If the attribute is a submodule (has callable forward), recurse.\n","        elif hasattr(value, 'forward') and callable(value.forward) and value is not module:\n","            clear_auxiliary(value)\n","\n","\n","# ---------- After training ----------\n","def save_model_and_optimiser(model, optimiser, filename=\"model_optim_state.npy\"):\n","    \"\"\"\n","    Save model parameters and optimizer state to a .npy file.\n","    The final dictionary has the form:\n","\n","        {\n","           \"model_state\": { key: parameter_array, ... },\n","           \"optimizer_state\": optimizer.state\n","        }\n","\n","    Args:\n","        model: Your MLPMixer instance.\n","        optimizer: Your ADAM_np instance.\n","        filename: The file name to save the state (default: \"model_optim_state.npy\").\n","    \"\"\"\n","    param_tuples = optimiser.get_pointers_to_param_and_grad(model)\n","\n","    model_state = {}\n","    # Create a unique key for each parameter.\n","    for (p, param_attr, mod, grad, grad_attr) in param_tuples:\n","        key = f\"{mod.__class__.__name__}_{param_attr}_{id(p)}\"\n","        model_state[key] = p.copy()\n","\n","    optimiser_state = optimiser.state.copy()\n","\n","    state = {\n","        \"model_state\": model_state,\n","        \"optimizer_state\": optimiser_state\n","    }\n","\n","    # Save to a .npy file (when loading, use allow_pickle=True).\n","    np.save(filename, state)\n","    print(f\"Model and optimizer state saved to {filename}\")\n","\n","\n","# ---------- Hyper-params ----------\n","embed_dim = 64\n","num_tokens = nb15_dataset.train_set.shape[1] - 1\n","input_dim=1 if nb15_dataset.train_set.ndim == 2 else nb15_dataset.train_set.shape[-1]\n","depth=10\n","num_classes=1\n","dropout_p=0.2\n","stochastic_depth_p=0.2\n","\n","#accum_steps = 64\n","batch_size = 32\n","num_epochs = 100\n","lr = 0.01\n","weight_decay=0\n","warmup_epochs=5\n","gradient_clip_max_norm=1.0\n","\n","\n","# ---------- Instantiate model and trainer components ----------\n","model = MLPMixer(num_tokens=num_tokens, input_dim=input_dim, dim=embed_dim, depth=depth, num_classes=num_classes, dropout_p=dropout_p, stochastic_depth_p=stochastic_depth_p)\n","loss_fn_np = BCEWithLogits_np(reduction='mean')\n","optimiser_np = ADAM_np(model, lr=lr, weight_decay=weight_decay)\n","scheduler_np = CosineAnnealingLR_np(optimiser_np, T_max=num_epochs, eta_min=1e-6, warmup_epochs=warmup_epochs, warmup_start_lr=0.0)\n","\n","\n","# ---------- Training loop ----------\n","train_losses = []\n","val_losses = []\n","train_iter = 0\n","val_iter = 0\n","# import pdb\n","for epoch in range(num_epochs):\n","    optimiser_np.lr = scheduler_np.step()\n","    np.random.shuffle(nb15_dataset.train_set)\n","    batches = [nb15_dataset.train_set[i:i+batch_size] for i in range(0, nb15_dataset.train_set.shape[0], batch_size)]\n","    print(\"\\nEpoch:\", epoch)\n","\n","    total_train_loss_per_epoch = 0.0\n","    for batch_idx, batch in enumerate(batches):\n","        labels = batch[:, -1].reshape(-1, 1)\n","        features = np.expand_dims(batch[:, :-1], axis=-1)\n","        logits = model.forward(features)\n","\n","        batch_train_loss = loss_fn_np.forward(logits, labels)\n","\n","        grad_logits = loss_fn_np.backward()\n","        # grad_logits /= accum_steps\n","        _ = model.backward(grad_logits)\n","\n","        #before_step_mixer_linear = np.copy(model.mixer_layers[0][0].fn.linear1.weight)\n","        optimiser_np.refresh_grad_per_backprop(model)\n","        clip_grad_global_norm(optimiser_np.param_tuples, max_norm=gradient_clip_max_norm)\n","        #print(optimiser_np.param_tuples)\n","        #pdb.set_trace()\n","\n","        #if ((batch_idx + 1) % accum_steps == 0) or (batch_idx + 1 == len(batches)):\n","        optimiser_np.step()\n","        #after_step_mixer_linear = model.mixer_layers[0][0].fn.linear1.weight\n","        #print(\"Max abs diff between before and after stepping (should be a small difference):\", np.max(np.abs(before_step_mixer_linear - after_step_mixer_linear)))\n","        #pdb.set_trace()\n","        optimiser_np.copy_updated_params()\n","        #after_step_and_copy_mixer_linear = model.mixer_layers[0][0].fn.linear1.weight\n","        #print(\"Max abs diff between before and after update and copy (should be 0):\", np.max(np.abs(after_step_and_copy_mixer_linear - after_step_mixer_linear)))\n","        #pdb.set_trace()\n","        clear_auxiliary(model)\n","\n","        total_train_loss_per_epoch += batch_train_loss\n","\n","        train_iter += 1\n","        if train_iter % 64 == 0:\n","            print(f\"Train Iteration {train_iter}, Loss: {batch_train_loss:.8f}\")\n","    avg_train_loss_per_epoch = total_train_loss_per_epoch / len(batches)\n","    train_losses.append(avg_train_loss_per_epoch)\n","\n","    total_val_loss_per_epoch = 0.0\n","    val_batches = [nb15_dataset.val_set[i:i+batch_size] for i in range(0, nb15_dataset.val_set.shape[0], batch_size)]\n","    for val_batch in val_batches:\n","        labels = val_batch[:, -1].reshape(-1, 1)\n","        features = np.expand_dims(val_batch[:, :-1], axis=-1)\n","        logits = model.forward(features)\n","\n","        batch_val_loss = loss_fn_np.forward(logits, labels)\n","\n","        total_val_loss_per_epoch += batch_val_loss\n","\n","        val_iter += 1\n","        if val_iter % 64 == 0:\n","            print(f\"Val Iteration {val_iter}, Loss: {batch_val_loss:.8f}\")\n","\n","    avg_val_loss_per_epoch = total_val_loss_per_epoch / len(val_batches)\n","    val_losses.append(avg_val_loss_per_epoch)\n","\n","print(\"Average loss:\", np.mean(train_losses))\n","save_model_and_optimiser(model, optimiser_np)"],"metadata":{"id":"yzR1gaj0BZlG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5804f6b-3f64-448a-d5b5-cd3eb7c550b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","Train Iteration 64, Loss: 0.58526525\n","Train Iteration 128, Loss: 0.51119467\n","Train Iteration 192, Loss: 0.58607374\n","Train Iteration 256, Loss: 0.71309258\n","Train Iteration 320, Loss: 0.41930104\n","Train Iteration 384, Loss: 0.57972839\n","Train Iteration 448, Loss: 0.56527807\n","Train Iteration 512, Loss: 0.44950806\n","Train Iteration 576, Loss: 0.58119031\n","Val Iteration 64, Loss: 0.71654813\n","\n","Epoch: 1\n","Train Iteration 640, Loss: 0.59563601\n","Train Iteration 704, Loss: 0.71484854\n","Train Iteration 768, Loss: 0.58424763\n","Train Iteration 832, Loss: 0.51170565\n","Train Iteration 896, Loss: 0.65952744\n","Train Iteration 960, Loss: 0.57619811\n","Train Iteration 1024, Loss: 0.51687606\n","Train Iteration 1088, Loss: 0.68356020\n","Train Iteration 1152, Loss: 0.54411861\n","Train Iteration 1216, Loss: 0.60878049\n","Val Iteration 128, Loss: 0.57970976\n","Val Iteration 192, Loss: 0.38968865\n","\n","Epoch: 2\n","Train Iteration 1280, Loss: 0.61029104\n","Train Iteration 1344, Loss: 0.57534251\n","Train Iteration 1408, Loss: 0.54751515\n","Train Iteration 1472, Loss: 0.62150544\n","Train Iteration 1536, Loss: 0.64164173\n","Train Iteration 1600, Loss: 0.42524128\n","Train Iteration 1664, Loss: 0.41357175\n","Train Iteration 1728, Loss: 0.69341492\n","Train Iteration 1792, Loss: 0.48074709\n","Train Iteration 1856, Loss: 0.68502102\n","Val Iteration 256, Loss: 0.46929290\n","Val Iteration 320, Loss: 0.45657945\n","\n","Epoch: 3\n","Train Iteration 1920, Loss: 0.49252693\n","Train Iteration 1984, Loss: 0.51331409\n","Train Iteration 2048, Loss: 0.51336306\n","Train Iteration 2112, Loss: 0.70794092\n","Train Iteration 2176, Loss: 0.47085385\n","Train Iteration 2240, Loss: 0.56700113\n","Train Iteration 2304, Loss: 0.58621936\n","Train Iteration 2368, Loss: 0.59713446\n","Train Iteration 2432, Loss: 0.64093147\n","Train Iteration 2496, Loss: 0.62347593\n","Val Iteration 384, Loss: 0.39388248\n","Val Iteration 448, Loss: 0.58758583\n","\n","Epoch: 4\n","Train Iteration 2560, Loss: 0.53651407\n","Train Iteration 2624, Loss: 0.79624234\n","Train Iteration 2688, Loss: 0.50498787\n","Train Iteration 2752, Loss: 0.33308030\n","Train Iteration 2816, Loss: 0.50962476\n","Train Iteration 2880, Loss: 0.34468651\n","Train Iteration 2944, Loss: 0.51199488\n","Train Iteration 3008, Loss: 0.51210483\n","Train Iteration 3072, Loss: 0.40740769\n","Val Iteration 512, Loss: 0.54402649\n","Val Iteration 576, Loss: 0.52861062\n","\n","Epoch: 5\n","Train Iteration 3136, Loss: 0.35231331\n","Train Iteration 3200, Loss: 0.49136426\n","Train Iteration 3264, Loss: 0.41298253\n","Train Iteration 3328, Loss: 0.63273219\n","Train Iteration 3392, Loss: 0.57230660\n","Train Iteration 3456, Loss: 0.59157738\n","Train Iteration 3520, Loss: 0.48748660\n","Train Iteration 3584, Loss: 0.39003349\n","Train Iteration 3648, Loss: 0.41910671\n","Train Iteration 3712, Loss: 0.71770632\n","Val Iteration 640, Loss: 0.42797599\n","Val Iteration 704, Loss: 0.48048311\n","\n","Epoch: 6\n","Train Iteration 3776, Loss: 0.48263963\n","Train Iteration 3840, Loss: 0.70822525\n","Train Iteration 3904, Loss: 0.50156449\n","Train Iteration 3968, Loss: 0.47033359\n","Train Iteration 4032, Loss: 0.61172166\n","Train Iteration 4096, Loss: 0.42121158\n","Train Iteration 4160, Loss: 0.45696634\n","Train Iteration 4224, Loss: 0.27837344\n","Train Iteration 4288, Loss: 0.56285050\n","Train Iteration 4352, Loss: 0.57296182\n","Val Iteration 768, Loss: 0.55843016\n","Val Iteration 832, Loss: 0.53049051\n","\n","Epoch: 7\n","Train Iteration 4416, Loss: 0.56490447\n","Train Iteration 4480, Loss: 0.59538236\n","Train Iteration 4544, Loss: 0.45410371\n","Train Iteration 4608, Loss: 0.54650026\n","Train Iteration 4672, Loss: 0.46869437\n","Train Iteration 4736, Loss: 0.68786772\n","Train Iteration 4800, Loss: 0.48596559\n","Train Iteration 4864, Loss: 0.61102200\n","Train Iteration 4928, Loss: 0.57376762\n","Train Iteration 4992, Loss: 0.38764989\n","Val Iteration 896, Loss: 0.53558962\n","Val Iteration 960, Loss: 0.57917845\n","\n","Epoch: 8\n","Train Iteration 5056, Loss: 0.63144620\n","Train Iteration 5120, Loss: 0.58390908\n","Train Iteration 5184, Loss: 0.41471064\n","Train Iteration 5248, Loss: 0.57736584\n","Train Iteration 5312, Loss: 0.60333862\n","Train Iteration 5376, Loss: 0.45736770\n","Train Iteration 5440, Loss: 0.57948983\n","Train Iteration 5504, Loss: 0.56011090\n","Train Iteration 5568, Loss: 0.65253791\n","Val Iteration 1024, Loss: 0.57607026\n","Val Iteration 1088, Loss: 0.43434165\n","\n","Epoch: 9\n","Train Iteration 5632, Loss: 0.51344571\n","Train Iteration 5696, Loss: 0.42875858\n","Train Iteration 5760, Loss: 0.45031618\n","Train Iteration 5824, Loss: 0.67227489\n","Train Iteration 5888, Loss: 0.63350118\n","Train Iteration 5952, Loss: 0.53234168\n","Train Iteration 6016, Loss: 0.60749908\n","Train Iteration 6080, Loss: 0.44848170\n","Train Iteration 6144, Loss: 0.53652046\n","Train Iteration 6208, Loss: 0.52491497\n","Val Iteration 1152, Loss: 0.41747335\n","Val Iteration 1216, Loss: 0.52325526\n","\n","Epoch: 10\n","Train Iteration 6272, Loss: 0.75910330\n","Train Iteration 6336, Loss: 0.41536474\n","Train Iteration 6400, Loss: 0.52562019\n","Train Iteration 6464, Loss: 0.50594782\n","Train Iteration 6528, Loss: 0.58388223\n","Train Iteration 6592, Loss: 0.38373031\n","Train Iteration 6656, Loss: 0.49627856\n","Train Iteration 6720, Loss: 0.66960069\n","Train Iteration 6784, Loss: 0.43601044\n","Train Iteration 6848, Loss: 0.56165826\n","Val Iteration 1280, Loss: 0.50083168\n","Val Iteration 1344, Loss: 0.52883153\n","\n","Epoch: 11\n","Train Iteration 6912, Loss: 0.49771495\n","Train Iteration 6976, Loss: 0.50696055\n","Train Iteration 7040, Loss: 0.48765762\n","Train Iteration 7104, Loss: 0.50108112\n","Train Iteration 7168, Loss: 0.43079170\n","Train Iteration 7232, Loss: 0.42694919\n","Train Iteration 7296, Loss: 0.44930874\n","Train Iteration 7360, Loss: 0.49137530\n","Train Iteration 7424, Loss: 0.69031313\n","Train Iteration 7488, Loss: 0.46910584\n","Val Iteration 1408, Loss: 0.63851425\n","Val Iteration 1472, Loss: 0.66295812\n","\n","Epoch: 12\n","Train Iteration 7552, Loss: 0.57408584\n","Train Iteration 7616, Loss: 0.39964011\n","Train Iteration 7680, Loss: 0.48558858\n","Train Iteration 7744, Loss: 0.40831154\n","Train Iteration 7808, Loss: 0.69694917\n","Train Iteration 7872, Loss: 0.42258736\n","Train Iteration 7936, Loss: 0.65728884\n","Train Iteration 8000, Loss: 0.45649970\n","Train Iteration 8064, Loss: 0.52853758\n","Val Iteration 1536, Loss: 0.40520436\n","Val Iteration 1600, Loss: 0.47373068\n","\n","Epoch: 13\n","Train Iteration 8128, Loss: 0.55995352\n","Train Iteration 8192, Loss: 0.58432258\n","Train Iteration 8256, Loss: 0.58074984\n","Train Iteration 8320, Loss: 0.42414100\n","Train Iteration 8384, Loss: 0.44294776\n","Train Iteration 8448, Loss: 0.50955255\n","Train Iteration 8512, Loss: 0.58804760\n","Train Iteration 8576, Loss: 0.59086903\n","Train Iteration 8640, Loss: 0.59947145\n","Train Iteration 8704, Loss: 0.55714774\n","Val Iteration 1664, Loss: 0.42025929\n","Val Iteration 1728, Loss: 0.49977628\n","\n","Epoch: 14\n","Train Iteration 8768, Loss: 0.46442801\n","Train Iteration 8832, Loss: 0.47095761\n","Train Iteration 8896, Loss: 0.45996798\n","Train Iteration 8960, Loss: 0.39938306\n","Train Iteration 9024, Loss: 0.45097425\n","Train Iteration 9088, Loss: 0.43581510\n","Train Iteration 9152, Loss: 0.56199287\n","Train Iteration 9216, Loss: 0.50645760\n","Train Iteration 9280, Loss: 0.47669683\n","Train Iteration 9344, Loss: 0.57544155\n","Val Iteration 1792, Loss: 0.67694917\n","Val Iteration 1856, Loss: 0.49117458\n","\n","Epoch: 15\n","Train Iteration 9408, Loss: 0.59505734\n","Train Iteration 9472, Loss: 0.65568967\n","Train Iteration 9536, Loss: 0.46928515\n","Train Iteration 9600, Loss: 0.45871846\n","Train Iteration 9664, Loss: 0.54412844\n","Train Iteration 9728, Loss: 0.75306786\n","Train Iteration 9792, Loss: 0.54566812\n","Train Iteration 9856, Loss: 0.75375568\n","Train Iteration 9920, Loss: 0.43836696\n","Train Iteration 9984, Loss: 0.43586105\n","Val Iteration 1920, Loss: 0.51619785\n","Val Iteration 1984, Loss: 0.47864941\n","\n","Epoch: 16\n","Train Iteration 10048, Loss: 0.45209528\n","Train Iteration 10112, Loss: 0.34789926\n","Train Iteration 10176, Loss: 0.47316506\n","Train Iteration 10240, Loss: 0.61934787\n","Train Iteration 10304, Loss: 0.37838357\n","Train Iteration 10368, Loss: 0.40275954\n","Train Iteration 10432, Loss: 0.59862384\n","Train Iteration 10496, Loss: 0.66069429\n","Train Iteration 10560, Loss: 0.47814830\n","Train Iteration 10624, Loss: 0.51613462\n","Val Iteration 2048, Loss: 0.46785598\n","Val Iteration 2112, Loss: 0.47477786\n","\n","Epoch: 17\n","Train Iteration 10688, Loss: 0.50836977\n","Train Iteration 10752, Loss: 0.50964758\n","Train Iteration 10816, Loss: 0.42624211\n","Train Iteration 10880, Loss: 0.34870391\n","Train Iteration 10944, Loss: 0.54200708\n","Train Iteration 11008, Loss: 0.40836038\n","Train Iteration 11072, Loss: 0.55809455\n","Train Iteration 11136, Loss: 0.52015967\n","Train Iteration 11200, Loss: 0.53020959\n","Val Iteration 2176, Loss: 0.49137385\n","Val Iteration 2240, Loss: 0.55906060\n","\n","Epoch: 18\n","Train Iteration 11264, Loss: 0.53157027\n","Train Iteration 11328, Loss: 0.41428338\n","Train Iteration 11392, Loss: 0.33655232\n","Train Iteration 11456, Loss: 0.48019265\n","Train Iteration 11520, Loss: 0.49715705\n","Train Iteration 11584, Loss: 0.54543547\n","Train Iteration 11648, Loss: 0.55584661\n","Train Iteration 11712, Loss: 0.37414123\n","Train Iteration 11776, Loss: 0.62166025\n","Train Iteration 11840, Loss: 0.62652665\n","Val Iteration 2304, Loss: 0.51617990\n","Val Iteration 2368, Loss: 0.45082457\n","\n","Epoch: 19\n","Train Iteration 11904, Loss: 0.39021381\n","Train Iteration 11968, Loss: 0.48265039\n","Train Iteration 12032, Loss: 0.64348735\n","Train Iteration 12096, Loss: 0.33770735\n","Train Iteration 12160, Loss: 0.50760151\n","Train Iteration 12224, Loss: 0.56928625\n","Train Iteration 12288, Loss: 0.63787097\n","Train Iteration 12352, Loss: 0.43532298\n","Train Iteration 12416, Loss: 0.55039215\n","Train Iteration 12480, Loss: 0.53555009\n","Val Iteration 2432, Loss: 0.40491657\n","Val Iteration 2496, Loss: 0.52452929\n","\n","Epoch: 20\n","Train Iteration 12544, Loss: 0.53779193\n","Train Iteration 12608, Loss: 0.34753117\n","Train Iteration 12672, Loss: 0.45510935\n","Train Iteration 12736, Loss: 0.43699221\n","Train Iteration 12800, Loss: 0.68820087\n","Train Iteration 12864, Loss: 0.63122647\n","Train Iteration 12928, Loss: 0.44393568\n","Train Iteration 12992, Loss: 0.45472763\n","Train Iteration 13056, Loss: 0.63868816\n","Train Iteration 13120, Loss: 0.53768397\n","Val Iteration 2560, Loss: 0.40732522\n","Val Iteration 2624, Loss: 0.58239894\n","\n","Epoch: 21\n","Train Iteration 13184, Loss: 0.46809267\n","Train Iteration 13248, Loss: 0.47653234\n","Train Iteration 13312, Loss: 0.47794106\n","Train Iteration 13376, Loss: 0.56944928\n","Train Iteration 13440, Loss: 0.46588220\n","Train Iteration 13504, Loss: 0.43931604\n","Train Iteration 13568, Loss: 0.48613900\n","Train Iteration 13632, Loss: 0.59086780\n","Train Iteration 13696, Loss: 0.49778876\n","Val Iteration 2688, Loss: 0.63298598\n","\n","Epoch: 22\n","Train Iteration 13760, Loss: 0.56841821\n","Train Iteration 13824, Loss: 0.54524667\n","Train Iteration 13888, Loss: 0.52775445\n","Train Iteration 13952, Loss: 0.44688039\n","Train Iteration 14016, Loss: 0.46593944\n","Train Iteration 14080, Loss: 0.53135181\n","Train Iteration 14144, Loss: 0.53119047\n","Train Iteration 14208, Loss: 0.44268031\n","Train Iteration 14272, Loss: 0.56394559\n","Train Iteration 14336, Loss: 0.38926720\n","Val Iteration 2752, Loss: 0.42163930\n","Val Iteration 2816, Loss: 0.39491522\n","\n","Epoch: 23\n","Train Iteration 14400, Loss: 0.61226911\n","Train Iteration 14464, Loss: 0.55612464\n","Train Iteration 14528, Loss: 0.55390173\n","Train Iteration 14592, Loss: 0.48409763\n","Train Iteration 14656, Loss: 0.60496049\n","Train Iteration 14720, Loss: 0.53178962\n","Train Iteration 14784, Loss: 0.53523453\n","Train Iteration 14848, Loss: 0.58972037\n","Train Iteration 14912, Loss: 0.79379740\n","Train Iteration 14976, Loss: 0.51163401\n","Val Iteration 2880, Loss: 0.48672874\n","Val Iteration 2944, Loss: 0.54866310\n","\n","Epoch: 24\n","Train Iteration 15040, Loss: 0.52629425\n","Train Iteration 15104, Loss: 0.49494276\n","Train Iteration 15168, Loss: 0.58407657\n","Train Iteration 15232, Loss: 0.48880197\n","Train Iteration 15296, Loss: 0.65107302\n","Train Iteration 15360, Loss: 0.37730372\n","Train Iteration 15424, Loss: 0.46907106\n","Train Iteration 15488, Loss: 0.59914807\n","Train Iteration 15552, Loss: 0.61903678\n","Train Iteration 15616, Loss: 0.45256810\n","Val Iteration 3008, Loss: 0.62481266\n","Val Iteration 3072, Loss: 0.37816002\n","\n","Epoch: 25\n","Train Iteration 15680, Loss: 0.58661402\n","Train Iteration 15744, Loss: 0.45337880\n","Train Iteration 15808, Loss: 0.70648816\n","Train Iteration 15872, Loss: 0.52073143\n","Train Iteration 15936, Loss: 0.42096765\n","Train Iteration 16000, Loss: 0.46263445\n","Train Iteration 16064, Loss: 0.46617298\n","Train Iteration 16128, Loss: 0.49312325\n","Train Iteration 16192, Loss: 0.45931045\n","Val Iteration 3136, Loss: 0.34751975\n","Val Iteration 3200, Loss: 0.44552291\n","\n","Epoch: 26\n","Train Iteration 16256, Loss: 0.47129864\n","Train Iteration 16320, Loss: 0.60204083\n","Train Iteration 16384, Loss: 0.34989034\n","Train Iteration 16448, Loss: 0.39030606\n","Train Iteration 16512, Loss: 0.48385101\n","Train Iteration 16576, Loss: 0.42640811\n","Train Iteration 16640, Loss: 0.49297628\n","Train Iteration 16704, Loss: 0.55550748\n","Train Iteration 16768, Loss: 0.44119034\n","Train Iteration 16832, Loss: 0.36764045\n","Val Iteration 3264, Loss: 0.42716728\n","Val Iteration 3328, Loss: 0.53215011\n","\n","Epoch: 27\n","Train Iteration 16896, Loss: 0.39118041\n","Train Iteration 16960, Loss: 0.34731999\n","Train Iteration 17024, Loss: 0.46767218\n","Train Iteration 17088, Loss: 0.50031756\n","Train Iteration 17152, Loss: 0.47508254\n","Train Iteration 17216, Loss: 0.79072236\n","Train Iteration 17280, Loss: 0.36794466\n","Train Iteration 17344, Loss: 0.39349114\n","Train Iteration 17408, Loss: 0.44444277\n","Train Iteration 17472, Loss: 0.34814264\n","Val Iteration 3392, Loss: 0.50018649\n","Val Iteration 3456, Loss: 0.45519369\n","\n","Epoch: 28\n","Train Iteration 17536, Loss: 0.37280483\n","Train Iteration 17600, Loss: 0.53685796\n","Train Iteration 17664, Loss: 0.57832868\n","Train Iteration 17728, Loss: 0.46354564\n","Train Iteration 17792, Loss: 0.59279366\n","Train Iteration 17856, Loss: 0.50797979\n","Train Iteration 17920, Loss: 0.51229446\n","Train Iteration 17984, Loss: 0.41865594\n","Train Iteration 18048, Loss: 0.41303877\n","Train Iteration 18112, Loss: 0.36770887\n","Val Iteration 3520, Loss: 0.58147776\n","Val Iteration 3584, Loss: 0.50419716\n","\n","Epoch: 29\n","Train Iteration 18176, Loss: 0.49611800\n","Train Iteration 18240, Loss: 0.44181216\n","Train Iteration 18304, Loss: 0.37733357\n","Train Iteration 18368, Loss: 0.60560811\n","Train Iteration 18432, Loss: 0.55286180\n","Train Iteration 18496, Loss: 0.53461276\n","Train Iteration 18560, Loss: 0.45283247\n","Train Iteration 18624, Loss: 0.70742324\n","Train Iteration 18688, Loss: 0.59938973\n","Val Iteration 3648, Loss: 0.38925772\n","Val Iteration 3712, Loss: 0.54701658\n","\n","Epoch: 30\n","Train Iteration 18752, Loss: 0.38200621\n","Train Iteration 18816, Loss: 0.49999524\n","Train Iteration 18880, Loss: 0.44246336\n","Train Iteration 18944, Loss: 0.35740062\n","Train Iteration 19008, Loss: 0.36610890\n","Train Iteration 19072, Loss: 0.37050887\n","Train Iteration 19136, Loss: 0.47239350\n","Train Iteration 19200, Loss: 0.53110999\n","Train Iteration 19264, Loss: 0.37506026\n","Train Iteration 19328, Loss: 0.42405309\n","Val Iteration 3776, Loss: 0.37818329\n","Val Iteration 3840, Loss: 0.41512979\n","\n","Epoch: 31\n","Train Iteration 19392, Loss: 0.51234184\n","Train Iteration 19456, Loss: 0.54347704\n","Train Iteration 19520, Loss: 0.41942757\n","Train Iteration 19584, Loss: 0.37733437\n","Train Iteration 19648, Loss: 0.51427221\n","Train Iteration 19712, Loss: 0.44493341\n","Train Iteration 19776, Loss: 0.44629673\n","Train Iteration 19840, Loss: 0.41837266\n","Train Iteration 19904, Loss: 0.45542533\n","Train Iteration 19968, Loss: 0.37871633\n","Val Iteration 3904, Loss: 0.40281515\n","Val Iteration 3968, Loss: 0.43639867\n","\n","Epoch: 32\n","Train Iteration 20032, Loss: 0.53605186\n","Train Iteration 20096, Loss: 0.52193244\n","Train Iteration 20160, Loss: 0.55542798\n","Train Iteration 20224, Loss: 0.41691552\n","Train Iteration 20288, Loss: 0.47142256\n","Train Iteration 20352, Loss: 0.46713594\n","Train Iteration 20416, Loss: 0.38811442\n","Train Iteration 20480, Loss: 0.44705861\n","Train Iteration 20544, Loss: 0.52655990\n","Train Iteration 20608, Loss: 0.51220192\n","Val Iteration 4032, Loss: 0.29446868\n","Val Iteration 4096, Loss: 0.37781264\n","\n","Epoch: 33\n","Train Iteration 20672, Loss: 0.46885038\n","Train Iteration 20736, Loss: 0.44782308\n","Train Iteration 20800, Loss: 0.44446019\n","Train Iteration 20864, Loss: 0.45246024\n","Train Iteration 20928, Loss: 0.41349170\n","Train Iteration 20992, Loss: 0.41569403\n","Train Iteration 21056, Loss: 0.40923986\n","Train Iteration 21120, Loss: 0.62329484\n","Train Iteration 21184, Loss: 0.36868589\n","Train Iteration 21248, Loss: 0.58398224\n","Val Iteration 4160, Loss: 0.41815278\n","Val Iteration 4224, Loss: 0.46298151\n","\n","Epoch: 34\n","Train Iteration 21312, Loss: 0.43924263\n","Train Iteration 21376, Loss: 0.38442220\n","Train Iteration 21440, Loss: 0.44609907\n","Train Iteration 21504, Loss: 0.59467030\n","Train Iteration 21568, Loss: 0.49886630\n","Train Iteration 21632, Loss: 0.38091395\n","Train Iteration 21696, Loss: 0.41075534\n","Train Iteration 21760, Loss: 0.58443708\n","Train Iteration 21824, Loss: 0.59774822\n","Val Iteration 4288, Loss: 0.46464251\n","Val Iteration 4352, Loss: 0.33581435\n","\n","Epoch: 35\n","Train Iteration 21888, Loss: 0.48423165\n","Train Iteration 21952, Loss: 0.59951633\n","Train Iteration 22016, Loss: 0.47432964\n","Train Iteration 22080, Loss: 0.47916893\n","Train Iteration 22144, Loss: 0.42005213\n","Train Iteration 22208, Loss: 0.44234481\n","Train Iteration 22272, Loss: 0.42854661\n","Train Iteration 22336, Loss: 0.58482330\n","Train Iteration 22400, Loss: 0.58014115\n","Train Iteration 22464, Loss: 0.37211802\n","Val Iteration 4416, Loss: 0.35616436\n","Val Iteration 4480, Loss: 0.34344870\n","\n","Epoch: 36\n","Train Iteration 22528, Loss: 0.46541667\n","Train Iteration 22592, Loss: 0.47644893\n","Train Iteration 22656, Loss: 0.52376225\n","Train Iteration 22720, Loss: 0.56684156\n","Train Iteration 22784, Loss: 0.45852779\n","Train Iteration 22848, Loss: 0.51209192\n","Train Iteration 22912, Loss: 0.57226878\n","Train Iteration 22976, Loss: 0.49357476\n","Train Iteration 23040, Loss: 0.36974882\n","Train Iteration 23104, Loss: 0.58304850\n","Val Iteration 4544, Loss: 0.44021308\n","Val Iteration 4608, Loss: 0.43803174\n","\n","Epoch: 37\n","Train Iteration 23168, Loss: 0.47851619\n","Train Iteration 23232, Loss: 0.54926577\n","Train Iteration 23296, Loss: 0.40705722\n","Train Iteration 23360, Loss: 0.42508206\n","Train Iteration 23424, Loss: 0.36399664\n","Train Iteration 23488, Loss: 0.44314569\n","Train Iteration 23552, Loss: 0.46151527\n","Train Iteration 23616, Loss: 0.52140776\n","Train Iteration 23680, Loss: 0.39876804\n","Train Iteration 23744, Loss: 0.60131639\n","Val Iteration 4672, Loss: 0.42360668\n","Val Iteration 4736, Loss: 0.46326894\n","\n","Epoch: 38\n","Train Iteration 23808, Loss: 0.51915825\n","Train Iteration 23872, Loss: 0.43182299\n","Train Iteration 23936, Loss: 0.42172199\n","Train Iteration 24000, Loss: 0.40225948\n","Train Iteration 24064, Loss: 0.46778653\n","Train Iteration 24128, Loss: 0.37969286\n","Train Iteration 24192, Loss: 0.54850396\n","Train Iteration 24256, Loss: 0.40986647\n","Train Iteration 24320, Loss: 0.47829604\n","Val Iteration 4800, Loss: 0.41350109\n","Val Iteration 4864, Loss: 0.36405096\n","\n","Epoch: 39\n","Train Iteration 24384, Loss: 0.44795732\n","Train Iteration 24448, Loss: 0.31176768\n","Train Iteration 24512, Loss: 0.61145003\n","Train Iteration 24576, Loss: 0.48002113\n","Train Iteration 24640, Loss: 0.41020161\n","Train Iteration 24704, Loss: 0.42130351\n","Train Iteration 24768, Loss: 0.30290808\n","Train Iteration 24832, Loss: 0.42873449\n","Train Iteration 24896, Loss: 0.39280475\n","Train Iteration 24960, Loss: 0.52344296\n","Val Iteration 4928, Loss: 0.37325649\n","Val Iteration 4992, Loss: 0.37834900\n","\n","Epoch: 40\n","Train Iteration 25024, Loss: 0.57330408\n","Train Iteration 25088, Loss: 0.40805161\n","Train Iteration 25152, Loss: 0.53581747\n","Train Iteration 25216, Loss: 0.34341654\n","Train Iteration 25280, Loss: 0.59332194\n","Train Iteration 25344, Loss: 0.36269444\n","Train Iteration 25408, Loss: 0.40347150\n","Train Iteration 25472, Loss: 0.45964089\n","Train Iteration 25536, Loss: 0.41406466\n","Train Iteration 25600, Loss: 0.55719848\n","Val Iteration 5056, Loss: 0.34577098\n","Val Iteration 5120, Loss: 0.32994009\n","\n","Epoch: 41\n","Train Iteration 25664, Loss: 0.43256858\n","Train Iteration 25728, Loss: 0.46148522\n","Train Iteration 25792, Loss: 0.37411933\n","Train Iteration 25856, Loss: 0.43017143\n","Train Iteration 25920, Loss: 0.56761363\n","Train Iteration 25984, Loss: 0.38675044\n","Train Iteration 26048, Loss: 0.48344970\n","Train Iteration 26112, Loss: 0.37295148\n","Train Iteration 26176, Loss: 0.30989116\n","Train Iteration 26240, Loss: 0.42902592\n","Val Iteration 5184, Loss: 0.43212483\n","Val Iteration 5248, Loss: 0.39168079\n","\n","Epoch: 42\n","Train Iteration 26304, Loss: 0.53936533\n","Train Iteration 26368, Loss: 0.42269811\n","Train Iteration 26432, Loss: 0.58701430\n","Train Iteration 26496, Loss: 0.36931200\n","Train Iteration 26560, Loss: 0.40745616\n","Train Iteration 26624, Loss: 0.49364474\n","Train Iteration 26688, Loss: 0.51637326\n","Train Iteration 26752, Loss: 0.42387499\n","Train Iteration 26816, Loss: 0.47650898\n","Val Iteration 5312, Loss: 0.30987165\n","\n","Epoch: 43\n","Train Iteration 26880, Loss: 0.41695300\n","Train Iteration 26944, Loss: 0.51401709\n","Train Iteration 27008, Loss: 0.47810515\n","Train Iteration 27072, Loss: 0.44784628\n","Train Iteration 27136, Loss: 0.47488883\n","Train Iteration 27200, Loss: 0.43188756\n","Train Iteration 27264, Loss: 0.45404482\n","Train Iteration 27328, Loss: 0.56712332\n","Train Iteration 27392, Loss: 0.34038796\n","Train Iteration 27456, Loss: 0.50028136\n","Val Iteration 5376, Loss: 0.43749943\n","Val Iteration 5440, Loss: 0.43266635\n","\n","Epoch: 44\n","Train Iteration 27520, Loss: 0.36320814\n","Train Iteration 27584, Loss: 0.48868932\n","Train Iteration 27648, Loss: 0.40868201\n","Train Iteration 27712, Loss: 0.39618922\n","Train Iteration 27776, Loss: 0.54879724\n","Train Iteration 27840, Loss: 0.42395280\n","Train Iteration 27904, Loss: 0.27810117\n","Train Iteration 27968, Loss: 0.37451378\n","Train Iteration 28032, Loss: 0.40741104\n","Train Iteration 28096, Loss: 0.57457680\n","Val Iteration 5504, Loss: 0.35636162\n","Val Iteration 5568, Loss: 0.44920540\n","\n","Epoch: 45\n","Train Iteration 28160, Loss: 0.54547531\n","Train Iteration 28224, Loss: 0.52735573\n","Train Iteration 28288, Loss: 0.47117891\n","Train Iteration 28352, Loss: 0.37264860\n","Train Iteration 28416, Loss: 0.35050614\n","Train Iteration 28480, Loss: 0.38346194\n","Train Iteration 28544, Loss: 0.53934878\n","Train Iteration 28608, Loss: 0.45906184\n","Train Iteration 28672, Loss: 0.35801231\n","Train Iteration 28736, Loss: 0.75004928\n","Val Iteration 5632, Loss: 0.52416939\n","Val Iteration 5696, Loss: 0.43676456\n","\n","Epoch: 46\n","Train Iteration 28800, Loss: 0.56553885\n","Train Iteration 28864, Loss: 0.38715999\n","Train Iteration 28928, Loss: 0.38686500\n","Train Iteration 28992, Loss: 0.24355761\n","Train Iteration 29056, Loss: 0.33936289\n","Train Iteration 29120, Loss: 0.56465166\n","Train Iteration 29184, Loss: 0.49793416\n","Train Iteration 29248, Loss: 0.51764717\n","Train Iteration 29312, Loss: 0.66123481\n","Val Iteration 5760, Loss: 0.29440803\n","Val Iteration 5824, Loss: 0.46609609\n","\n","Epoch: 47\n","Train Iteration 29376, Loss: 0.49569269\n","Train Iteration 29440, Loss: 0.49213339\n","Train Iteration 29504, Loss: 0.52331336\n","Train Iteration 29568, Loss: 0.39216117\n","Train Iteration 29632, Loss: 0.44135249\n","Train Iteration 29696, Loss: 0.53464810\n","Train Iteration 29760, Loss: 0.43637193\n","Train Iteration 29824, Loss: 0.53553726\n","Train Iteration 29888, Loss: 0.50251142\n","Train Iteration 29952, Loss: 0.55974890\n","Val Iteration 5888, Loss: 0.45570439\n","Val Iteration 5952, Loss: 0.25122799\n","\n","Epoch: 48\n","Train Iteration 30016, Loss: 0.52772179\n","Train Iteration 30080, Loss: 0.51209642\n","Train Iteration 30144, Loss: 0.48665825\n","Train Iteration 30208, Loss: 0.42074451\n","Train Iteration 30272, Loss: 0.43418117\n","Train Iteration 30336, Loss: 0.61667325\n","Train Iteration 30400, Loss: 0.37397760\n","Train Iteration 30464, Loss: 0.50174377\n","Train Iteration 30528, Loss: 0.64713769\n","Train Iteration 30592, Loss: 0.45456779\n","Val Iteration 6016, Loss: 0.32959759\n","Val Iteration 6080, Loss: 0.45176452\n","\n","Epoch: 49\n","Train Iteration 30656, Loss: 0.53061136\n","Train Iteration 30720, Loss: 0.51292717\n","Train Iteration 30784, Loss: 0.63816453\n","Train Iteration 30848, Loss: 0.52455674\n","Train Iteration 30912, Loss: 0.48912131\n","Train Iteration 30976, Loss: 0.42307465\n","Train Iteration 31040, Loss: 0.48014144\n","Train Iteration 31104, Loss: 0.42455870\n","Train Iteration 31168, Loss: 0.64788886\n","Train Iteration 31232, Loss: 0.43779709\n","Val Iteration 6144, Loss: 0.35552540\n","Val Iteration 6208, Loss: 0.41800443\n","\n","Epoch: 50\n","Train Iteration 31296, Loss: 0.44782400\n","Train Iteration 31360, Loss: 0.70472074\n","Train Iteration 31424, Loss: 0.58501656\n","Train Iteration 31488, Loss: 0.45805975\n","Train Iteration 31552, Loss: 0.50369478\n","Train Iteration 31616, Loss: 0.45449109\n","Train Iteration 31680, Loss: 0.50319514\n","Train Iteration 31744, Loss: 0.44478742\n","Train Iteration 31808, Loss: 0.56249891\n","Train Iteration 31872, Loss: 0.39081101\n","Val Iteration 6272, Loss: 0.43448434\n","Val Iteration 6336, Loss: 0.35576894\n","\n","Epoch: 51\n","Train Iteration 31936, Loss: 0.34320048\n","Train Iteration 32000, Loss: 0.50123392\n","Train Iteration 32064, Loss: 0.58409022\n","Train Iteration 32128, Loss: 0.41755657\n","Train Iteration 32192, Loss: 0.37717511\n","Train Iteration 32256, Loss: 0.35110476\n","Train Iteration 32320, Loss: 0.46324926\n","Train Iteration 32384, Loss: 0.46488164\n","Train Iteration 32448, Loss: 0.41372227\n","Val Iteration 6400, Loss: 0.52879996\n","Val Iteration 6464, Loss: 0.36444406\n","\n","Epoch: 52\n","Train Iteration 32512, Loss: 0.50754240\n","Train Iteration 32576, Loss: 0.47152696\n","Train Iteration 32640, Loss: 0.53019427\n","Train Iteration 32704, Loss: 0.59692148\n","Train Iteration 32768, Loss: 0.27591129\n","Train Iteration 32832, Loss: 0.49697214\n","Train Iteration 32896, Loss: 0.46339659\n","Train Iteration 32960, Loss: 0.43207517\n","Train Iteration 33024, Loss: 0.38960472\n","Train Iteration 33088, Loss: 0.28060068\n","Val Iteration 6528, Loss: 0.34494492\n","Val Iteration 6592, Loss: 0.34978101\n","\n","Epoch: 53\n","Train Iteration 33152, Loss: 0.54571044\n","Train Iteration 33216, Loss: 0.36499572\n","Train Iteration 33280, Loss: 0.51682290\n","Train Iteration 33344, Loss: 0.35211360\n","Train Iteration 33408, Loss: 0.49575449\n","Train Iteration 33472, Loss: 0.53842812\n","Train Iteration 33536, Loss: 0.29276353\n","Train Iteration 33600, Loss: 0.43837927\n","Train Iteration 33664, Loss: 0.60594004\n","Train Iteration 33728, Loss: 0.40280654\n","Val Iteration 6656, Loss: 0.34762567\n","Val Iteration 6720, Loss: 0.45085803\n","\n","Epoch: 54\n","Train Iteration 33792, Loss: 0.35616145\n","Train Iteration 33856, Loss: 0.56123485\n","Train Iteration 33920, Loss: 0.45314576\n","Train Iteration 33984, Loss: 0.46158771\n","Train Iteration 34048, Loss: 0.29192031\n","Train Iteration 34112, Loss: 0.34688054\n","Train Iteration 34176, Loss: 0.61925761\n","Train Iteration 34240, Loss: 0.40382680\n","Train Iteration 34304, Loss: 0.35092752\n","Train Iteration 34368, Loss: 0.37990089\n","Val Iteration 6784, Loss: 0.44976779\n","Val Iteration 6848, Loss: 0.40514991\n","\n","Epoch: 55\n","Train Iteration 34432, Loss: 0.36148094\n","Train Iteration 34496, Loss: 0.54688895\n","Train Iteration 34560, Loss: 0.45922606\n","Train Iteration 34624, Loss: 0.53280158\n","Train Iteration 34688, Loss: 0.53757413\n","Train Iteration 34752, Loss: 0.54189759\n","Train Iteration 34816, Loss: 0.34734822\n","Train Iteration 34880, Loss: 0.40801613\n","Train Iteration 34944, Loss: 0.44362012\n","Val Iteration 6912, Loss: 0.40148109\n","Val Iteration 6976, Loss: 0.34227402\n","\n","Epoch: 56\n","Train Iteration 35008, Loss: 0.47121233\n","Train Iteration 35072, Loss: 0.34388371\n","Train Iteration 35136, Loss: 0.49328216\n","Train Iteration 35200, Loss: 0.59968014\n","Train Iteration 35264, Loss: 0.34419745\n","Train Iteration 35328, Loss: 0.51590436\n","Train Iteration 35392, Loss: 0.42071784\n","Train Iteration 35456, Loss: 0.52109848\n","Train Iteration 35520, Loss: 0.47429898\n","Train Iteration 35584, Loss: 0.45019020\n","Val Iteration 7040, Loss: 0.34655433\n","Val Iteration 7104, Loss: 0.44212417\n","\n","Epoch: 57\n","Train Iteration 35648, Loss: 0.53241197\n","Train Iteration 35712, Loss: 0.43320348\n","Train Iteration 35776, Loss: 0.45008024\n","Train Iteration 35840, Loss: 0.50742247\n","Train Iteration 35904, Loss: 0.46694347\n","Train Iteration 35968, Loss: 0.40138246\n","Train Iteration 36032, Loss: 0.29238687\n","Train Iteration 36096, Loss: 0.55889471\n","Train Iteration 36160, Loss: 0.50147097\n","Train Iteration 36224, Loss: 0.34054704\n","Val Iteration 7168, Loss: 0.57298412\n","Val Iteration 7232, Loss: 0.51777074\n","\n","Epoch: 58\n","Train Iteration 36288, Loss: 0.49146670\n","Train Iteration 36352, Loss: 0.46046908\n","Train Iteration 36416, Loss: 0.50166524\n","Train Iteration 36480, Loss: 0.51859334\n","Train Iteration 36544, Loss: 0.57425783\n","Train Iteration 36608, Loss: 0.45980193\n","Train Iteration 36672, Loss: 0.46884337\n","Train Iteration 36736, Loss: 0.50441233\n","Train Iteration 36800, Loss: 0.72380283\n","Train Iteration 36864, Loss: 0.52692361\n","Val Iteration 7296, Loss: 0.28565866\n","Val Iteration 7360, Loss: 0.40747539\n","\n","Epoch: 59\n","Train Iteration 36928, Loss: 0.52935338\n","Train Iteration 36992, Loss: 0.42215654\n","Train Iteration 37056, Loss: 0.44399306\n","Train Iteration 37120, Loss: 0.60179802\n","Train Iteration 37184, Loss: 0.51929631\n","Train Iteration 37248, Loss: 0.39722762\n","Train Iteration 37312, Loss: 0.41397461\n","Train Iteration 37376, Loss: 0.46033966\n","Train Iteration 37440, Loss: 0.49426314\n","Val Iteration 7424, Loss: 0.34852088\n","Val Iteration 7488, Loss: 0.32647687\n","\n","Epoch: 60\n","Train Iteration 37504, Loss: 0.47132801\n","Train Iteration 37568, Loss: 0.42349323\n","Train Iteration 37632, Loss: 0.41334682\n","Train Iteration 37696, Loss: 0.44269941\n","Train Iteration 37760, Loss: 0.47281009\n","Train Iteration 37824, Loss: 0.38756056\n","Train Iteration 37888, Loss: 0.39459979\n","Train Iteration 37952, Loss: 0.42620098\n","Train Iteration 38016, Loss: 0.54755804\n","Train Iteration 38080, Loss: 0.29561359\n","Val Iteration 7552, Loss: 0.36153632\n","Val Iteration 7616, Loss: 0.48384531\n","\n","Epoch: 61\n","Train Iteration 38144, Loss: 0.38154090\n","Train Iteration 38208, Loss: 0.51655015\n","Train Iteration 38272, Loss: 0.46538655\n","Train Iteration 38336, Loss: 0.35038166\n","Train Iteration 38400, Loss: 0.41350368\n","Train Iteration 38464, Loss: 0.59992613\n","Train Iteration 38528, Loss: 0.48800684\n","Train Iteration 38592, Loss: 0.43111728\n","Train Iteration 38656, Loss: 0.44857858\n","Train Iteration 38720, Loss: 0.44224816\n","Val Iteration 7680, Loss: 0.33975600\n","Val Iteration 7744, Loss: 0.33680293\n","\n","Epoch: 62\n","Train Iteration 38784, Loss: 0.37581342\n","Train Iteration 38848, Loss: 0.44440114\n","Train Iteration 38912, Loss: 0.43563311\n","Train Iteration 38976, Loss: 0.48036660\n","Train Iteration 39040, Loss: 0.67501920\n","Train Iteration 39104, Loss: 0.45799462\n","Train Iteration 39168, Loss: 0.48802797\n","Train Iteration 39232, Loss: 0.58073681\n","Train Iteration 39296, Loss: 0.35728494\n","Train Iteration 39360, Loss: 0.39484997\n","Val Iteration 7808, Loss: 0.38860739\n","Val Iteration 7872, Loss: 0.41327799\n","\n","Epoch: 63\n","Train Iteration 39424, Loss: 0.41790875\n","Train Iteration 39488, Loss: 0.43041233\n","Train Iteration 39552, Loss: 0.42758680\n","Train Iteration 39616, Loss: 0.52070714\n","Train Iteration 39680, Loss: 0.33847679\n","Train Iteration 39744, Loss: 0.37794750\n","Train Iteration 39808, Loss: 0.38298380\n","Train Iteration 39872, Loss: 0.51107505\n","Train Iteration 39936, Loss: 0.41062838\n","Train Iteration 40000, Loss: 0.32548245\n","Val Iteration 7936, Loss: 0.34944283\n","Val Iteration 8000, Loss: 0.19505511\n","\n","Epoch: 64\n","Train Iteration 40064, Loss: 0.31282104\n","Train Iteration 40128, Loss: 0.48179539\n","Train Iteration 40192, Loss: 0.34376099\n","Train Iteration 40256, Loss: 0.39744906\n","Train Iteration 40320, Loss: 0.36470244\n","Train Iteration 40384, Loss: 0.52315090\n","Train Iteration 40448, Loss: 0.59864822\n","Train Iteration 40512, Loss: 0.36979227\n","Train Iteration 40576, Loss: 0.45808556\n","Val Iteration 8064, Loss: 0.59352972\n","\n","Epoch: 65\n","Train Iteration 40640, Loss: 0.39351495\n","Train Iteration 40704, Loss: 0.53248368\n","Train Iteration 40768, Loss: 0.31288153\n","Train Iteration 40832, Loss: 0.38259122\n","Train Iteration 40896, Loss: 0.47950123\n","Train Iteration 40960, Loss: 0.40800331\n","Train Iteration 41024, Loss: 0.36708142\n","Train Iteration 41088, Loss: 0.46361680\n"]}]},{"cell_type":"code","source":["# print(optimiser_np.state)"],"metadata":{"id":"AkvF5DIjpq5t","executionInfo":{"status":"aborted","timestamp":1745181644145,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(optimiser_np.param_tuples)"],"metadata":{"id":"4UacOPuFe6QF","executionInfo":{"status":"aborted","timestamp":1745181644162,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clear_auxiliary(model)"],"metadata":{"id":"SX5nvX0WwJX_","executionInfo":{"status":"ok","timestamp":1745181683553,"user_tz":-60,"elapsed":78,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cv6DppIDRwI8","executionInfo":{"status":"ok","timestamp":1745181686621,"user_tz":-60,"elapsed":3,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"7c6a321b-0a3e-4cc4-af9e-cf4077961d91"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# ***3. Model Performance Evaluation (Task 3)***"],"metadata":{"id":"QQ4bBu-PQ3-g"}},{"cell_type":"code","source":["############################################################\n","#put your code for model performance evaluation here\n","def load_model_and_optimiser(model, optimiser, filename=\"model_optim_state.npy\"):\n","    \"\"\"\n","    Load pretrained model parameters and optimizer state from a file.\n","\n","    This function loads the state dictionary saved using 'save_model_and_optimiser', updates\n","    the model's parameters (in place) and sets the optimizer's state accordingly.\n","\n","    Args:\n","        model: The model instance whose parameters should be updated.\n","        optimiser: The instance of ADAM_np managing the model parameters.\n","        filename: The filename of the saved state (default is \"model_optim_state.npy\").\n","\n","    Raises:\n","        KeyError: If a parameter key from the model is not found in the saved state.\n","    \"\"\"\n","    import numpy as np\n","\n","    # Load the saved state dictionary with allow_pickle=True\n","    state = np.load(filename, allow_pickle=True).item()\n","    saved_model_state = state[\"model_state\"]\n","    saved_optimiser_state = state[\"optimizer_state\"]\n","\n","    # Refresh the mapping (the same order as used when saving is assumed)\n","    param_tuples = optimiser.get_pointers_to_param_and_grad(model)\n","\n","    # Update each parameter in the model using the saved state\n","    for (p, param_attr, mod, grad, grad_attr) in param_tuples:\n","        # Recreate the key used during saving\n","        key = f\"{mod.__class__.__name__}_{param_attr}_{id(p)}\"\n","        if key in saved_model_state:\n","            # In-place update of the parameter array\n","            p[...] = saved_model_state[key]\n","        else:\n","            # If key not found, it's likely there's a mismatch between model architectures\n","            raise KeyError(f\"Parameter key '{key}' not found in saved state. \"\n","                           \"Ensure that the model architecture and the optimizer pointers \"\n","                           \"match those used when saving.\")\n","\n","    # Update the optimizer state (this assumes the optimizer's internal state structure remains consistent)\n","    optimiser.state = saved_optimiser_state.copy()\n","\n","    print(f\"Loaded model and optimizer state from '{filename}'\")\n","\n","load_model_and_optimiser(model, optimiser_np)\n","all_labels = []\n","all_predictions = []\n","\n","for val_batch in val_batches:\n","    labels = val_batch[:, -1].reshape(-1, 1)\n","    features = np.expand_dims(val_batch[:, :-1], axis=-1)\n","    logits = model.forward(features)\n","\n","    probabilities = 1 / (1 + np.exp(-logits))\n","\n","    predictions = (probabilities >= 0.5).astype(int)\n","\n","    all_labels.append(labels)\n","    all_predictions.append(predictions)\n","\n","all_labels = np.concatenate(all_labels, axis=0)\n","all_predictions = np.concatenate(all_predictions, axis=0)\n","\n","TP = np.sum((all_predictions == 1) & (all_labels == 1))\n","FP = np.sum((all_predictions == 1) & (all_labels == 0))\n","TN = np.sum((all_predictions == 0) & (all_labels == 0))\n","FN = np.sum((all_predictions == 0) & (all_labels == 1))\n","\n","precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n","\n","recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n","\n","F1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n","\n","specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n","\n","balanced_accuracy = (recall + specificity) / 2\n","\n","# Print the results.\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {F1:.4f}\")\n","print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n","############################################################\n","\n","import matplotlib.pyplot as plt\n","# Assuming train_losses and val_losses are lists of loss values per epoch.\n","epochs = range(1, len(train_losses) + 1)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n","plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n","\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Loss Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","test_features = np.expand_dims(nb15_dataset.test_set, axis=-1)\n","test_logits = model.forward(test_features)\n","test_probabilities = 1 / (1 + np.exp(-test_logits))\n","test_predictions = (test_probabilities >= 0.5).astype(int)\n","print(test_predictions)"],"metadata":{"id":"filj8Tb2Bav9","executionInfo":{"status":"aborted","timestamp":1745181644167,"user_tz":-60,"elapsed":8442,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***4. Performance Evaluation***"],"metadata":{"id":"1iCUL48FBwlQ"}},{"cell_type":"markdown","source":["**Please report the classification accuracy your model achieved on data samples in the testing set 1 below (in percentage)**"],"metadata":{"id":"hda1q3KOBM_f"}},{"cell_type":"markdown","source":["95.92%\n","\n"],"metadata":{"id":"ePqjGPnHmYWv"}},{"cell_type":"markdown","source":["**Please provide the predicted class labels of the data samples in the testing set 1 below (0 or 1)**"],"metadata":{"id":"MpMWHu9bCFSo"}},{"cell_type":"markdown","source":["| **Sample ID** |**Predicted Label** |\n","| --- | --- |\n","| 1 | 1 |\n","| 2 | 0 |\n","| 3 | 1 |\n","| 4 | 1 |\n","| 5 | 0  |\n","| 6 | 0  |\n","| 7 | 0  |\n","| 8 | 0  |\n","| 9 | 0  |\n","| 10 |0   |\n","| 11 |0   |\n","| 12 |0   |\n","| 13 |1   |\n","| 14 |1   |\n","| 15 |0   |\n","| 16 |0   |\n","| 17 |1   |\n","| 18 |1   |\n","| 19 |0   |\n","| 20 |0   |\n","| 21 |0   |\n","| 22 |1   |\n","| 23 |0   |\n","| 24 |0   |\n","| 25 |1   |"],"metadata":{"id":"R_8B_MxvCnf3"}}]}