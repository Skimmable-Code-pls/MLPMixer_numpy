{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1wcnjQODYQ4"
   },
   "source": [
    "# ***0. Data Loading***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3VVMd4Fj301"
   },
   "source": [
    "Import NB15 dataset on your Google Drive then run this cell\n",
    "\n",
    "To-do:\n",
    "- [x] Verify pandas.DataFrame = polars.DataFrame with no floating value difference\n",
    "- [x] Verify polars is faster than pandas in reading csv, around 70-110ms difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "n6MJyx0bDlFa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (20_000, 44)\n",
       " ┌───────┬──────────┬───────┬─────────┬───┬────────────┬────────────┬─────────────────┬───────┐\n",
       " │ id    ┆ dur      ┆ proto ┆ service ┆ … ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_ports ┆ label │\n",
       " │ ---   ┆ ---      ┆ ---   ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---             ┆ ---   │\n",
       " │ i64   ┆ f64      ┆ str   ┆ str     ┆   ┆ i64        ┆ i64        ┆ i64             ┆ i64   │\n",
       " ╞═══════╪══════════╪═══════╪═════════╪═══╪════════════╪════════════╪═════════════════╪═══════╡\n",
       " │ 1     ┆ 0.000003 ┆ unas  ┆ -       ┆ … ┆ 8          ┆ 11         ┆ 0               ┆ 1     │\n",
       " │ 2     ┆ 0.885807 ┆ tcp   ┆ ftp     ┆ … ┆ 5          ┆ 1          ┆ 0               ┆ 0     │\n",
       " │ 3     ┆ 0.538781 ┆ tcp   ┆ http    ┆ … ┆ 2          ┆ 6          ┆ 0               ┆ 0     │\n",
       " │ 4     ┆ 0.000008 ┆ udp   ┆ dns     ┆ … ┆ 27         ┆ 34         ┆ 0               ┆ 1     │\n",
       " │ 5     ┆ 0.448734 ┆ tcp   ┆ ftp     ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 1     │\n",
       " │ …     ┆ …        ┆ …     ┆ …       ┆ … ┆ …          ┆ …          ┆ …               ┆ …     │\n",
       " │ 19996 ┆ 0.000008 ┆ sctp  ┆ -       ┆ … ┆ 2          ┆ 5          ┆ 0               ┆ 1     │\n",
       " │ 19997 ┆ 0.789808 ┆ tcp   ┆ http    ┆ … ┆ 2          ┆ 1          ┆ 0               ┆ 1     │\n",
       " │ 19998 ┆ 0.539269 ┆ tcp   ┆ http    ┆ … ┆ 6          ┆ 2          ┆ 0               ┆ 1     │\n",
       " │ 19999 ┆ 0.931055 ┆ tcp   ┆ -       ┆ … ┆ 2          ┆ 7          ┆ 0               ┆ 0     │\n",
       " │ 20000 ┆ 2.947155 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 0     │\n",
       " └───────┴──────────┴───────┴─────────┴───┴────────────┴────────────┴─────────────────┴───────┘,\n",
       " shape: (4_000, 44)\n",
       " ┌──────┬──────────┬───────┬─────────┬───┬────────────┬────────────┬─────────────────┬───────┐\n",
       " │ id   ┆ dur      ┆ proto ┆ service ┆ … ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_ports ┆ label │\n",
       " │ ---  ┆ ---      ┆ ---   ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---             ┆ ---   │\n",
       " │ i64  ┆ f64      ┆ str   ┆ str     ┆   ┆ i64        ┆ i64        ┆ i64             ┆ i64   │\n",
       " ╞══════╪══════════╪═══════╪═════════╪═══╪════════════╪════════════╪═════════════════╪═══════╡\n",
       " │ 1    ┆ 0.000009 ┆ sep   ┆ -       ┆ … ┆ 1          ┆ 6          ┆ 0               ┆ 1     │\n",
       " │ 2    ┆ 0.965595 ┆ tcp   ┆ ftp     ┆ … ┆ 2          ┆ 1          ┆ 0               ┆ 0     │\n",
       " │ 3    ┆ 0.000008 ┆ udp   ┆ dns     ┆ … ┆ 22         ┆ 35         ┆ 0               ┆ 1     │\n",
       " │ 4    ┆ 0.012175 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 8          ┆ 0               ┆ 0     │\n",
       " │ 5    ┆ 0.000004 ┆ udp   ┆ dns     ┆ … ┆ 26         ┆ 26         ┆ 0               ┆ 1     │\n",
       " │ …    ┆ …        ┆ …     ┆ …       ┆ … ┆ …          ┆ …          ┆ …               ┆ …     │\n",
       " │ 3996 ┆ 0.689171 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 6          ┆ 0               ┆ 0     │\n",
       " │ 3997 ┆ 1.091111 ┆ tcp   ┆ pop3    ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 1     │\n",
       " │ 3998 ┆ 0.017941 ┆ tcp   ┆ -       ┆ … ┆ 10         ┆ 8          ┆ 0               ┆ 0     │\n",
       " │ 3999 ┆ 0.000008 ┆ unas  ┆ -       ┆ … ┆ 4          ┆ 6          ┆ 0               ┆ 1     │\n",
       " │ 4000 ┆ 0.00095  ┆ udp   ┆ dns     ┆ … ┆ 2          ┆ 2          ┆ 0               ┆ 0     │\n",
       " └──────┴──────────┴───────┴─────────┴───┴────────────┴────────────┴─────────────────┴───────┘,\n",
       " shape: (25, 43)\n",
       " ┌─────┬──────────┬───────────┬─────────┬───┬──────────────┬────────────┬────────────┬──────────────┐\n",
       " │ id  ┆ dur      ┆ proto     ┆ service ┆ … ┆ ct_flw_http_ ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_po │\n",
       " │ --- ┆ ---      ┆ ---       ┆ ---     ┆   ┆ mthd         ┆ ---        ┆ ---        ┆ rts          │\n",
       " │ i64 ┆ f64      ┆ str       ┆ str     ┆   ┆ ---          ┆ i64        ┆ i64        ┆ ---          │\n",
       " │     ┆          ┆           ┆         ┆   ┆ i64          ┆            ┆            ┆ i64          │\n",
       " ╞═════╪══════════╪═══════════╪═════════╪═══╪══════════════╪════════════╪════════════╪══════════════╡\n",
       " │ 1   ┆ 0.000003 ┆ udp       ┆ -       ┆ … ┆ 0            ┆ 1          ┆ 3          ┆ 0            │\n",
       " │ 2   ┆ 0.015833 ┆ tcp       ┆ -       ┆ … ┆ 0            ┆ 2          ┆ 6          ┆ 0            │\n",
       " │ 3   ┆ 0.000003 ┆ merit-inp ┆ -       ┆ … ┆ 0            ┆ 2          ┆ 4          ┆ 0            │\n",
       " │ 4   ┆ 0.000003 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 32         ┆ 33         ┆ 0            │\n",
       " │ 5   ┆ 0.00106  ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 3          ┆ 3          ┆ 0            │\n",
       " │ …   ┆ …        ┆ …         ┆ …       ┆ … ┆ …            ┆ …          ┆ …          ┆ …            │\n",
       " │ 21  ┆ 0.916548 ┆ tcp       ┆ ftp     ┆ … ┆ 0            ┆ 2          ┆ 1          ┆ 0            │\n",
       " │ 22  ┆ 0.000009 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 12         ┆ 20         ┆ 0            │\n",
       " │ 23  ┆ 1.005819 ┆ tcp       ┆ http    ┆ … ┆ 1            ┆ 3          ┆ 3          ┆ 0            │\n",
       " │ 24  ┆ 0.590464 ┆ tcp       ┆ ftp     ┆ … ┆ 0            ┆ 2          ┆ 2          ┆ 0            │\n",
       " │ 25  ┆ 0.000008 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 27         ┆ 39         ┆ 0            │\n",
       " └─────┴──────────┴───────────┴─────────┴───┴──────────────┴────────────┴────────────┴──────────────┘)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "def polar_read_csv(csv_train, csv_val, csv_test):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        csv_train (str): name of train set csv file\n",
    "        csv_val (str): name of val set csv file\n",
    "        csv_test (str): name of test set csv file\n",
    "\n",
    "    Output:\n",
    "        read train, val, test sets in DataFrame format\n",
    "    \"\"\"\n",
    "    df_train = pl.read_csv(csv_train)\n",
    "    df_val = pl.read_csv(csv_val)\n",
    "    df_test = pl.read_csv(csv_test)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "polar_read_csv(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYce1Z49mIsz"
   },
   "source": [
    "# ***1. Data Pre-Processing (Task 1)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY_MbXp2cEsj"
   },
   "source": [
    "TO-DO:\n",
    "- [x] Convert DataFrame to np.float32\n",
    "- [x] Sanity check if min-max rescaling will squash small number. Observation: np.float32 will pick up the difference in (small_number+1)/big_number, but not when small_number/(big_number+1). Looking at column feature contain big numbers like dur, sload, dload, rate: their value vary greater than 1000 between different samples. This is something np.float32 can pick up. Last but not least, this tells us that we shoudl add noise to dataset after normalisation, not before\n",
    "- [x] Sanity check fuse rescale-normalise == rescale first then normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1745224887035,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "EpksctcIiBjB",
    "outputId": "7d244e2b-7420-4f70-ccb7-7baadb5b5b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set mean: [[0.01006994]]\n",
      "train set std: [[0.07261028]]\n",
      "\n",
      "Batch 1 shape: (64, 43)\n",
      "Batch 1 feature shape: (64, 42, 1)\n",
      "Batch 1 label shape: (64, 1)\n",
      "\n",
      "Batch 2 shape: (64, 43)\n",
      "Batch 2 feature shape: (64, 42, 1)\n",
      "Batch 2 label shape: (64, 1)\n",
      "\n",
      "Batch 3 shape: (64, 43)\n",
      "Batch 3 feature shape: (64, 42, 1)\n",
      "Batch 3 label shape: (64, 1)\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#put your code for data preprocessing here\n",
    "############################################################\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "class NIDdataset():\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file,\n",
    "        val_file,\n",
    "        test_file\n",
    "    ):\n",
    "        assert train_file.endswith(\".csv\"), print(\"train dataset should be either .csv in this project\")\n",
    "        assert val_file.endswith(\".csv\"), print(\"val dataset should be either .csv in this project\")\n",
    "        assert test_file.endswith(\".csv\"), print(\"test dataset should be either .csv in this project\")\n",
    "\n",
    "        if train_file.endswith(\".csv\") and val_file.endswith(\".csv\") and test_file.endswith(\".csv\"):\n",
    "            train_df_encoded, val_df_encoded, test_df_encoded = self.encode_category_attribute(train_file, val_file, test_file, [\"proto\", \"service\", \"state\"])\n",
    "            train_arr = train_df_encoded.to_numpy().astype(np.float32)\n",
    "            val_arr = val_df_encoded.to_numpy().astype(np.float32)\n",
    "            test_arr = test_df_encoded.to_numpy().astype(np.float32)\n",
    "\n",
    "            train_features = train_arr[:, 1:-1]\n",
    "            val_features = val_arr[:, 1:-1]\n",
    "            test_features = test_arr[:, 1:]\n",
    "\n",
    "            self.get_mean_std_after_rescaled(train_features)\n",
    "\n",
    "        self.train_set = np.concatenate([self._transform(train_features), train_arr[:, -1].reshape(-1, 1)], axis=1)\n",
    "        self.val_set = np.concatenate([self._transform(val_features), val_arr[:, -1].reshape(-1, 1)], axis=1)\n",
    "        self.test_set = self._transform(test_features)\n",
    "\n",
    "    def _transform(self, input: np.ndarray):\n",
    "        processed_input = self.fused_rescale_normalise(input)\n",
    "        return processed_input\n",
    "\n",
    "    def encode_category_attribute(self, csv_train, csv_val, csv_test, attribute_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_train (str): train set as .csv\n",
    "            csv_val (str): val set as .csv\n",
    "            csv_test (str): test set as .csv\n",
    "\n",
    "        Return:\n",
    "            encoded version of train set, val set, test set as polars.DataFrame using encoding dictionary for train set\n",
    "        \"\"\"\n",
    "        df_train, df_val, df_test = polar_read_csv(csv_train, csv_val, csv_test)\n",
    "\n",
    "        attribute_dict = {}\n",
    "        for attribute in attribute_list:\n",
    "            if attribute in df_train.columns:\n",
    "                unique_vals = (df_train.select(attribute).drop_nulls().unique().to_series().to_list())\n",
    "                attribute_dict[attribute] = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "            else:\n",
    "                print(f\"Warning: '{attribute}' not found in training CSV.\")\n",
    "\n",
    "        def apply_encoding(df, attribute_dict):\n",
    "            for attribute, val_to_idx in attribute_dict.items():\n",
    "                if attribute in df.columns:\n",
    "                    df = df.with_columns(pl.col(attribute).replace(val_to_idx).alias(attribute))\n",
    "            return df\n",
    "        df_train_encoded = apply_encoding(df_train, attribute_dict)\n",
    "        df_val_encoded = apply_encoding(df_val, attribute_dict)\n",
    "        df_val_encoded = df_val_encoded.filter(pl.col(\"state\") != \"RST\")\n",
    "        df_test_encoded = apply_encoding(df_test, attribute_dict)\n",
    "\n",
    "        return df_train_encoded, df_val_encoded, df_test_encoded\n",
    "\n",
    "    def get_mean_std_after_rescaled(self, x: np.ndarray):\n",
    "        scale = np.max(x)\n",
    "        x_rescaled = x/scale\n",
    "        # usually we get mean, std per channel. But nb15 doesn't channel so we get 1 mean, std over anything to not destroy local inductive bias in train_features\n",
    "        self.train_mean = np.mean(x_rescaled, axis=(0,1), keepdims=True)\n",
    "        self.train_std = np.std(x_rescaled, axis=(0,1), keepdims=True)\n",
    "\n",
    "    def fused_rescale_normalise(self, x: np.ndarray):\n",
    "        mean = self.train_mean.copy()\n",
    "        std = self.train_std.copy()\n",
    "        scaling_factor = np.max(x)\n",
    "        return (x - mean*scaling_factor) / (std*scaling_factor)\n",
    "\n",
    "nb15_dataset = NIDdataset(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")\n",
    "batch_size = 64\n",
    "#print(f\"Train set shape: {nb15_dataset.train_set.shape}\")\n",
    "#print(f\"Val set shape: {nb15_dataset.val_set.shape}\")\n",
    "#print(f\"Test set shape: {nb15_dataset.test_set.shape}\")\n",
    "print(f\"train set mean: {nb15_dataset.train_mean}\")\n",
    "print(f\"train set std: {nb15_dataset.train_std}\")\n",
    "np.random.shuffle(nb15_dataset.train_set)\n",
    "batches = [nb15_dataset.train_set[i:i+batch_size] for i in range(0, nb15_dataset.train_set.shape[0], batch_size)]\n",
    "\n",
    "# Check shapes of initial 3 batches.\n",
    "for idx, batch in enumerate(batches[:3]):\n",
    "    labels = batch[:, -1].reshape(-1, 1)\n",
    "    features = np.expand_dims(batch[:, :-1], axis=-1)\n",
    "    print(f\"\\nBatch {idx+1} shape:\", batch.shape)\n",
    "    print(f\"Batch {idx+1} feature shape:\", features.shape)\n",
    "    print(f\"Batch {idx+1} label shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1745181637977,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "TtVP4_0pB2mp",
    "outputId": "faea4879-782b-49e0-c8b2-c5408643d2da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n"
     ]
    }
   ],
   "source": [
    "dummy_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "for _, _, c in dummy_tuples:\n",
    "    c = c * 10\n",
    "\n",
    "print(dummy_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4uQmeM_Albh"
   },
   "source": [
    "# ***2. Model Implementation and Training (Task 2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzR1gaj0BZlG",
    "outputId": "d5804f6b-3f64-448a-d5b5-cd3eb7c550b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train Iteration 32, Loss: 1.07451530\n",
      "Train Iteration 64, Loss: 0.64781717\n",
      "Train Iteration 96, Loss: 0.73128348\n",
      "Train Iteration 128, Loss: 0.67949529\n",
      "Train Iteration 160, Loss: 0.68393221\n",
      "Train Iteration 192, Loss: 0.75028791\n",
      "Train Iteration 224, Loss: 0.67773861\n",
      "Train Iteration 256, Loss: 0.67228224\n",
      "Train Iteration 288, Loss: 0.69684413\n",
      "Train Iteration 320, Loss: 0.67142988\n",
      "Train Iteration 352, Loss: 0.71375275\n",
      "Train Iteration 384, Loss: 0.68468277\n",
      "Train Iteration 416, Loss: 0.71628630\n",
      "Train Iteration 448, Loss: 0.69056887\n",
      "Train Iteration 480, Loss: 0.67996009\n",
      "Train Iteration 512, Loss: 0.68620585\n",
      "Train Iteration 544, Loss: 0.73684651\n",
      "Train Iteration 576, Loss: 0.69215431\n",
      "Train Iteration 608, Loss: 0.69402719\n",
      "Val Iteration 32, Loss: 0.67746535\n",
      "Val Iteration 64, Loss: 0.67632577\n",
      "Val Iteration 96, Loss: 0.59081686\n",
      "\n",
      "Epoch: 1\n",
      "Train Iteration 640, Loss: 0.72312917\n",
      "Train Iteration 672, Loss: 0.71276066\n",
      "Train Iteration 704, Loss: 0.65857247\n",
      "Train Iteration 736, Loss: 0.73611902\n",
      "Train Iteration 768, Loss: 0.67186711\n",
      "Train Iteration 800, Loss: 0.58304087\n",
      "Train Iteration 832, Loss: 0.71537346\n",
      "Train Iteration 864, Loss: 0.69565153\n",
      "Train Iteration 896, Loss: 0.52637042\n",
      "Train Iteration 928, Loss: 0.61709505\n",
      "Train Iteration 960, Loss: 0.70486436\n",
      "Train Iteration 992, Loss: 0.61870836\n",
      "Train Iteration 1024, Loss: 0.71311065\n",
      "Train Iteration 1056, Loss: 0.58375818\n",
      "Train Iteration 1088, Loss: 0.70587576\n",
      "Train Iteration 1120, Loss: 0.42033298\n",
      "Train Iteration 1152, Loss: 0.59261755\n",
      "Train Iteration 1184, Loss: 0.62858894\n",
      "Train Iteration 1216, Loss: 0.59208567\n",
      "Train Iteration 1248, Loss: 0.50267322\n",
      "Val Iteration 128, Loss: 0.64687189\n",
      "Val Iteration 160, Loss: 0.50331976\n",
      "Val Iteration 192, Loss: 0.40931327\n",
      "Val Iteration 224, Loss: 0.61511065\n",
      "\n",
      "Epoch: 2\n",
      "Train Iteration 1280, Loss: 0.64590276\n",
      "Train Iteration 1312, Loss: 0.72457717\n",
      "Train Iteration 1344, Loss: 0.66424787\n",
      "Train Iteration 1376, Loss: 0.65841039\n",
      "Train Iteration 1408, Loss: 0.49492608\n",
      "Train Iteration 1440, Loss: 0.57888100\n",
      "Train Iteration 1472, Loss: 0.58924987\n",
      "Train Iteration 1504, Loss: 0.47133684\n",
      "Train Iteration 1536, Loss: 0.64978502\n",
      "Train Iteration 1568, Loss: 0.67085598\n",
      "Train Iteration 1600, Loss: 0.44841287\n",
      "Train Iteration 1632, Loss: 0.53902760\n",
      "Train Iteration 1664, Loss: 0.63052157\n",
      "Train Iteration 1696, Loss: 0.74586312\n",
      "Train Iteration 1728, Loss: 0.73559811\n",
      "Train Iteration 1760, Loss: 0.60452992\n",
      "Train Iteration 1792, Loss: 0.54563612\n",
      "Train Iteration 1824, Loss: 0.41641768\n",
      "Train Iteration 1856, Loss: 0.68916181\n",
      "Val Iteration 256, Loss: 0.43563184\n",
      "Val Iteration 288, Loss: 0.66453721\n",
      "Val Iteration 320, Loss: 0.59862088\n",
      "Val Iteration 352, Loss: 0.64117310\n",
      "\n",
      "Epoch: 3\n",
      "Train Iteration 1888, Loss: 0.51471265\n",
      "Train Iteration 1920, Loss: 0.54590575\n",
      "Train Iteration 1952, Loss: 0.58381924\n",
      "Train Iteration 1984, Loss: 0.50202232\n",
      "Train Iteration 2016, Loss: 0.58936331\n",
      "Train Iteration 2048, Loss: 0.41229051\n",
      "Train Iteration 2080, Loss: 0.74204346\n",
      "Train Iteration 2112, Loss: 0.44238228\n",
      "Train Iteration 2144, Loss: 0.62459517\n",
      "Train Iteration 2176, Loss: 0.34745511\n",
      "Train Iteration 2208, Loss: 0.54416401\n",
      "Train Iteration 2240, Loss: 0.48021081\n",
      "Train Iteration 2272, Loss: 0.68267780\n",
      "Train Iteration 2304, Loss: 0.46429009\n",
      "Train Iteration 2336, Loss: 0.47803463\n",
      "Train Iteration 2368, Loss: 0.57890359\n",
      "Train Iteration 2400, Loss: 0.44594519\n",
      "Train Iteration 2432, Loss: 0.61314233\n",
      "Train Iteration 2464, Loss: 0.59557928\n",
      "Train Iteration 2496, Loss: 0.45048407\n",
      "Val Iteration 384, Loss: 0.44843738\n",
      "Val Iteration 416, Loss: 0.43175423\n",
      "Val Iteration 448, Loss: 0.64225274\n",
      "Val Iteration 480, Loss: 0.47379132\n",
      "\n",
      "Epoch: 4\n",
      "Train Iteration 2528, Loss: 0.62965672\n",
      "Train Iteration 2560, Loss: 0.67027233\n",
      "Train Iteration 2592, Loss: 0.42831653\n",
      "Train Iteration 2624, Loss: 0.44994835\n",
      "Train Iteration 2656, Loss: 0.58618777\n",
      "Train Iteration 2688, Loss: 0.50160132\n",
      "Train Iteration 2720, Loss: 0.52187000\n",
      "Train Iteration 2752, Loss: 0.50484297\n",
      "Train Iteration 2784, Loss: 0.78221214\n",
      "Train Iteration 2816, Loss: 0.67576194\n",
      "Train Iteration 2848, Loss: 0.60935141\n",
      "Train Iteration 2880, Loss: 0.36653213\n",
      "Train Iteration 2912, Loss: 0.63163751\n",
      "Train Iteration 2944, Loss: 0.55810332\n",
      "Train Iteration 2976, Loss: 0.62352733\n",
      "Train Iteration 3008, Loss: 0.53486424\n",
      "Train Iteration 3040, Loss: 0.51800268\n",
      "Train Iteration 3072, Loss: 0.61441897\n",
      "Train Iteration 3104, Loss: 0.64315361\n",
      "Val Iteration 512, Loss: 0.53500763\n",
      "Val Iteration 544, Loss: 0.48772267\n",
      "Val Iteration 576, Loss: 0.42558094\n",
      "Val Iteration 608, Loss: 0.45593369\n",
      "\n",
      "Epoch: 5\n",
      "Train Iteration 3136, Loss: 0.33115932\n",
      "Train Iteration 3168, Loss: 0.68272411\n",
      "Train Iteration 3200, Loss: 0.74637610\n",
      "Train Iteration 3232, Loss: 0.62872856\n",
      "Train Iteration 3264, Loss: 0.49354827\n",
      "Train Iteration 3296, Loss: 0.70332996\n",
      "Train Iteration 3328, Loss: 0.33391520\n",
      "Train Iteration 3360, Loss: 0.34556818\n",
      "Train Iteration 3392, Loss: 0.41977065\n",
      "Train Iteration 3424, Loss: 0.77301334\n",
      "Train Iteration 3456, Loss: 0.52651945\n",
      "Train Iteration 3488, Loss: 0.38639459\n",
      "Train Iteration 3520, Loss: 0.40454512\n",
      "Train Iteration 3552, Loss: 0.58735210\n",
      "Train Iteration 3584, Loss: 0.73440588\n",
      "Train Iteration 3616, Loss: 0.51470770\n",
      "Train Iteration 3648, Loss: 0.55079374\n",
      "Train Iteration 3680, Loss: 0.54978212\n",
      "Train Iteration 3712, Loss: 0.59093324\n",
      "Train Iteration 3744, Loss: 0.60184118\n",
      "Val Iteration 640, Loss: 0.46589073\n",
      "Val Iteration 672, Loss: 0.56271139\n",
      "Val Iteration 704, Loss: 0.52060706\n",
      "Val Iteration 736, Loss: 0.68699057\n",
      "\n",
      "Epoch: 6\n",
      "Train Iteration 3776, Loss: 0.44648414\n",
      "Train Iteration 3808, Loss: 0.48671596\n",
      "Train Iteration 3840, Loss: 0.39862561\n",
      "Train Iteration 3872, Loss: 0.61305721\n",
      "Train Iteration 3904, Loss: 0.42878013\n",
      "Train Iteration 3936, Loss: 0.49362172\n",
      "Train Iteration 3968, Loss: 0.46557839\n",
      "Train Iteration 4000, Loss: 0.67095735\n",
      "Train Iteration 4032, Loss: 0.41824360\n",
      "Train Iteration 4064, Loss: 0.77934033\n",
      "Train Iteration 4096, Loss: 0.36342985\n",
      "Train Iteration 4128, Loss: 0.47127798\n",
      "Train Iteration 4160, Loss: 0.60863790\n",
      "Train Iteration 4192, Loss: 0.36509402\n",
      "Train Iteration 4224, Loss: 0.53497112\n",
      "Train Iteration 4256, Loss: 0.49985645\n",
      "Train Iteration 4288, Loss: 0.49240455\n",
      "Train Iteration 4320, Loss: 0.61157762\n",
      "Train Iteration 4352, Loss: 0.62146779\n",
      "Val Iteration 768, Loss: 0.65178620\n",
      "Val Iteration 800, Loss: 0.59057460\n",
      "Val Iteration 832, Loss: 0.58707187\n",
      "Val Iteration 864, Loss: 0.42135978\n",
      "\n",
      "Epoch: 7\n",
      "Train Iteration 4384, Loss: 0.49324729\n",
      "Train Iteration 4416, Loss: 0.64845526\n",
      "Train Iteration 4448, Loss: 0.52972832\n",
      "Train Iteration 4480, Loss: 0.67362662\n",
      "Train Iteration 4512, Loss: 0.57439565\n",
      "Train Iteration 4544, Loss: 0.65774552\n",
      "Train Iteration 4576, Loss: 0.52006460\n",
      "Train Iteration 4608, Loss: 0.53045886\n",
      "Train Iteration 4640, Loss: 0.53098815\n",
      "Train Iteration 4672, Loss: 0.44407870\n",
      "Train Iteration 4704, Loss: 0.61884194\n",
      "Train Iteration 4736, Loss: 0.49264487\n",
      "Train Iteration 4768, Loss: 0.63136380\n",
      "Train Iteration 4800, Loss: 0.70256768\n",
      "Train Iteration 4832, Loss: 0.73219062\n",
      "Train Iteration 4864, Loss: 0.59908210\n",
      "Train Iteration 4896, Loss: 0.56247189\n",
      "Train Iteration 4928, Loss: 0.58251011\n",
      "Train Iteration 4960, Loss: 0.56929336\n",
      "Train Iteration 4992, Loss: 0.42778173\n",
      "Val Iteration 896, Loss: 0.56373903\n",
      "Val Iteration 928, Loss: 0.52375117\n",
      "Val Iteration 960, Loss: 0.61478318\n",
      "Val Iteration 992, Loss: 0.52486892\n",
      "\n",
      "Epoch: 8\n",
      "Train Iteration 5024, Loss: 0.43190002\n",
      "Train Iteration 5056, Loss: 0.48184208\n",
      "Train Iteration 5088, Loss: 0.53243322\n",
      "Train Iteration 5120, Loss: 0.34300784\n",
      "Train Iteration 5152, Loss: 0.70982391\n",
      "Train Iteration 5184, Loss: 0.39606878\n",
      "Train Iteration 5216, Loss: 0.48043458\n",
      "Train Iteration 5248, Loss: 0.46930036\n",
      "Train Iteration 5280, Loss: 0.58679867\n",
      "Train Iteration 5312, Loss: 0.56258798\n",
      "Train Iteration 5344, Loss: 0.51407329\n",
      "Train Iteration 5376, Loss: 0.52637377\n",
      "Train Iteration 5408, Loss: 0.62040707\n",
      "Train Iteration 5440, Loss: 0.60267637\n",
      "Train Iteration 5472, Loss: 0.38541019\n",
      "Train Iteration 5504, Loss: 0.38312228\n",
      "Train Iteration 5536, Loss: 0.47349816\n",
      "Train Iteration 5568, Loss: 0.50482867\n",
      "Train Iteration 5600, Loss: 0.54176810\n",
      "Val Iteration 1024, Loss: 0.61875583\n",
      "Val Iteration 1056, Loss: 0.49839823\n",
      "Val Iteration 1088, Loss: 0.51366377\n",
      "Val Iteration 1120, Loss: 0.55020889\n",
      "\n",
      "Epoch: 9\n",
      "Train Iteration 5632, Loss: 0.52024725\n",
      "Train Iteration 5664, Loss: 0.63758630\n",
      "Train Iteration 5696, Loss: 0.66797991\n",
      "Train Iteration 5728, Loss: 0.37546984\n",
      "Train Iteration 5760, Loss: 0.58510211\n",
      "Train Iteration 5792, Loss: 0.63828004\n",
      "Train Iteration 5824, Loss: 0.51260079\n",
      "Train Iteration 5856, Loss: 0.49671096\n",
      "Train Iteration 5888, Loss: 0.56289220\n",
      "Train Iteration 5920, Loss: 0.42370536\n",
      "Train Iteration 5952, Loss: 0.49524961\n",
      "Train Iteration 5984, Loss: 0.51465602\n",
      "Train Iteration 6016, Loss: 0.52310909\n",
      "Train Iteration 6048, Loss: 0.51326598\n",
      "Train Iteration 6080, Loss: 0.53387916\n",
      "Train Iteration 6112, Loss: 0.35351000\n",
      "Train Iteration 6144, Loss: 0.49078963\n",
      "Train Iteration 6176, Loss: 0.36083960\n",
      "Train Iteration 6208, Loss: 0.44742460\n",
      "Train Iteration 6240, Loss: 0.50281633\n",
      "Val Iteration 1152, Loss: 0.37119383\n",
      "Val Iteration 1184, Loss: 0.51055904\n",
      "Val Iteration 1216, Loss: 0.50113521\n",
      "Val Iteration 1248, Loss: 0.49234322\n",
      "\n",
      "Epoch: 10\n",
      "Train Iteration 6272, Loss: 0.39216405\n",
      "Train Iteration 6304, Loss: 0.57588412\n",
      "Train Iteration 6336, Loss: 0.50110639\n",
      "Train Iteration 6368, Loss: 0.69971496\n",
      "Train Iteration 6400, Loss: 0.44127497\n",
      "Train Iteration 6432, Loss: 0.35788334\n",
      "Train Iteration 6464, Loss: 0.52770309\n",
      "Train Iteration 6496, Loss: 0.61688150\n",
      "Train Iteration 6528, Loss: 0.40881104\n",
      "Train Iteration 6560, Loss: 0.62689532\n",
      "Train Iteration 6592, Loss: 0.55077474\n",
      "Train Iteration 6624, Loss: 0.46771704\n",
      "Train Iteration 6656, Loss: 0.45816142\n",
      "Train Iteration 6688, Loss: 0.50875122\n",
      "Train Iteration 6720, Loss: 0.49377739\n",
      "Train Iteration 6752, Loss: 0.59248244\n",
      "Train Iteration 6784, Loss: 0.57764015\n",
      "Train Iteration 6816, Loss: 0.44496288\n",
      "Train Iteration 6848, Loss: 0.62992063\n",
      "Val Iteration 1280, Loss: 0.54085042\n",
      "Val Iteration 1312, Loss: 0.42708418\n",
      "Val Iteration 1344, Loss: 0.51665347\n",
      "\n",
      "Epoch: 11\n",
      "Train Iteration 6880, Loss: 0.44888516\n",
      "Train Iteration 6912, Loss: 0.51509858\n",
      "Train Iteration 6944, Loss: 0.51856331\n",
      "Train Iteration 6976, Loss: 0.55824687\n",
      "Train Iteration 7008, Loss: 0.53988846\n",
      "Train Iteration 7040, Loss: 0.60768946\n",
      "Train Iteration 7072, Loss: 0.49907241\n",
      "Train Iteration 7104, Loss: 0.40727239\n",
      "Train Iteration 7136, Loss: 0.52351418\n",
      "Train Iteration 7168, Loss: 0.46221210\n",
      "Train Iteration 7200, Loss: 0.40668646\n",
      "Train Iteration 7232, Loss: 0.53716805\n",
      "Train Iteration 7264, Loss: 0.44492491\n",
      "Train Iteration 7296, Loss: 0.43192174\n",
      "Train Iteration 7328, Loss: 0.67031322\n",
      "Train Iteration 7360, Loss: 0.59407610\n",
      "Train Iteration 7392, Loss: 0.53438763\n",
      "Train Iteration 7424, Loss: 0.48362400\n",
      "Train Iteration 7456, Loss: 0.41331405\n",
      "Train Iteration 7488, Loss: 0.48601074\n",
      "Val Iteration 1376, Loss: 0.44907971\n",
      "Val Iteration 1408, Loss: 0.57910653\n",
      "Val Iteration 1440, Loss: 0.48450765\n",
      "Val Iteration 1472, Loss: 0.61866535\n",
      "\n",
      "Epoch: 12\n",
      "Train Iteration 7520, Loss: 0.55527170\n",
      "Train Iteration 7552, Loss: 0.58992997\n",
      "Train Iteration 7584, Loss: 0.45016083\n",
      "Train Iteration 7616, Loss: 0.50563093\n",
      "Train Iteration 7648, Loss: 0.40394014\n",
      "Train Iteration 7680, Loss: 0.64902973\n",
      "Train Iteration 7712, Loss: 0.36038570\n",
      "Train Iteration 7744, Loss: 0.32677842\n",
      "Train Iteration 7776, Loss: 0.45763097\n",
      "Train Iteration 7808, Loss: 0.53490815\n",
      "Train Iteration 7840, Loss: 0.52301004\n",
      "Train Iteration 7872, Loss: 0.50198261\n",
      "Train Iteration 7904, Loss: 0.46839865\n",
      "Train Iteration 7936, Loss: 0.55740369\n",
      "Train Iteration 7968, Loss: 0.60063604\n",
      "Train Iteration 8000, Loss: 0.38961887\n",
      "Train Iteration 8032, Loss: 0.42757299\n",
      "Train Iteration 8064, Loss: 0.51827405\n",
      "Train Iteration 8096, Loss: 0.40613956\n",
      "Val Iteration 1504, Loss: 0.37513547\n",
      "Val Iteration 1536, Loss: 0.42923498\n",
      "Val Iteration 1568, Loss: 0.54901113\n",
      "Val Iteration 1600, Loss: 0.48419499\n",
      "\n",
      "Epoch: 13\n",
      "Train Iteration 8128, Loss: 0.53406681\n",
      "Train Iteration 8160, Loss: 0.53698931\n",
      "Train Iteration 8192, Loss: 0.58945638\n",
      "Train Iteration 8224, Loss: 0.53677553\n",
      "Train Iteration 8256, Loss: 0.52697900\n",
      "Train Iteration 8288, Loss: 0.68042929\n",
      "Train Iteration 8320, Loss: 0.48086956\n",
      "Train Iteration 8352, Loss: 0.49869467\n",
      "Train Iteration 8384, Loss: 0.41872623\n",
      "Train Iteration 8416, Loss: 0.47940686\n",
      "Train Iteration 8448, Loss: 0.51983407\n",
      "Train Iteration 8480, Loss: 0.50247997\n",
      "Train Iteration 8512, Loss: 0.47342789\n",
      "Train Iteration 8544, Loss: 0.49010336\n",
      "Train Iteration 8576, Loss: 0.57178106\n",
      "Train Iteration 8608, Loss: 0.44367354\n",
      "Train Iteration 8640, Loss: 0.54035177\n",
      "Train Iteration 8672, Loss: 0.44402034\n",
      "Train Iteration 8704, Loss: 0.44231218\n",
      "Train Iteration 8736, Loss: 0.67094749\n",
      "Val Iteration 1632, Loss: 0.42785311\n",
      "Val Iteration 1664, Loss: 0.40950238\n",
      "Val Iteration 1696, Loss: 0.49438813\n",
      "Val Iteration 1728, Loss: 0.44384981\n",
      "\n",
      "Epoch: 14\n",
      "Train Iteration 8768, Loss: 0.46194119\n",
      "Train Iteration 8800, Loss: 0.54030850\n",
      "Train Iteration 8832, Loss: 0.52023340\n",
      "Train Iteration 8864, Loss: 0.67127209\n",
      "Train Iteration 8896, Loss: 0.53328032\n",
      "Train Iteration 8928, Loss: 0.36739245\n",
      "Train Iteration 8960, Loss: 0.74794912\n",
      "Train Iteration 8992, Loss: 0.60685034\n",
      "Train Iteration 9024, Loss: 0.34737850\n",
      "Train Iteration 9056, Loss: 0.60927943\n",
      "Train Iteration 9088, Loss: 0.48813179\n",
      "Train Iteration 9120, Loss: 0.61920561\n",
      "Train Iteration 9152, Loss: 0.50910472\n",
      "Train Iteration 9184, Loss: 0.52536619\n",
      "Train Iteration 9216, Loss: 0.41402303\n",
      "Train Iteration 9248, Loss: 0.43418331\n",
      "Train Iteration 9280, Loss: 0.52812065\n",
      "Train Iteration 9312, Loss: 0.52971828\n",
      "Train Iteration 9344, Loss: 0.47616464\n",
      "Val Iteration 1760, Loss: 0.40906335\n",
      "Val Iteration 1792, Loss: 0.70709428\n",
      "Val Iteration 1824, Loss: 0.62095068\n",
      "Val Iteration 1856, Loss: 0.48900172\n",
      "\n",
      "Epoch: 15\n",
      "Train Iteration 9376, Loss: 0.59400596\n",
      "Train Iteration 9408, Loss: 0.49004457\n",
      "Train Iteration 9440, Loss: 0.52506275\n",
      "Train Iteration 9472, Loss: 0.43352191\n",
      "Train Iteration 9504, Loss: 0.38219705\n",
      "Train Iteration 9536, Loss: 0.62683680\n",
      "Train Iteration 9568, Loss: 0.61503225\n",
      "Train Iteration 9600, Loss: 0.49275697\n",
      "Train Iteration 9632, Loss: 0.46428698\n",
      "Train Iteration 9664, Loss: 0.41917370\n",
      "Train Iteration 9696, Loss: 0.60683206\n",
      "Train Iteration 9728, Loss: 0.53504492\n",
      "Train Iteration 9760, Loss: 0.57402318\n",
      "Train Iteration 9792, Loss: 0.64739715\n",
      "Train Iteration 9824, Loss: 0.49523256\n",
      "Train Iteration 9856, Loss: 0.56351425\n",
      "Train Iteration 9888, Loss: 0.64215223\n",
      "Train Iteration 9920, Loss: 0.44933381\n",
      "Train Iteration 9952, Loss: 0.58271030\n",
      "Train Iteration 9984, Loss: 0.44094966\n",
      "Val Iteration 1888, Loss: 0.52859989\n",
      "Val Iteration 1920, Loss: 0.51005908\n",
      "Val Iteration 1952, Loss: 0.30863402\n",
      "Val Iteration 1984, Loss: 0.51739877\n",
      "\n",
      "Epoch: 16\n",
      "Train Iteration 10016, Loss: 0.63177379\n",
      "Train Iteration 10048, Loss: 0.44290255\n",
      "Train Iteration 10080, Loss: 0.45003669\n",
      "Train Iteration 10112, Loss: 0.64822043\n",
      "Train Iteration 10144, Loss: 0.50167473\n",
      "Train Iteration 10176, Loss: 0.41894188\n",
      "Train Iteration 10208, Loss: 0.49443701\n",
      "Train Iteration 10240, Loss: 0.59348813\n",
      "Train Iteration 10272, Loss: 0.54250211\n",
      "Train Iteration 10304, Loss: 0.63854121\n",
      "Train Iteration 10336, Loss: 0.53440421\n",
      "Train Iteration 10368, Loss: 0.36392003\n",
      "Train Iteration 10400, Loss: 0.56532776\n",
      "Train Iteration 10432, Loss: 0.51040483\n",
      "Train Iteration 10464, Loss: 0.48078812\n",
      "Train Iteration 10496, Loss: 0.55330294\n",
      "Train Iteration 10528, Loss: 0.56654039\n",
      "Train Iteration 10560, Loss: 0.55996055\n",
      "Train Iteration 10592, Loss: 0.65508042\n",
      "Train Iteration 10624, Loss: 0.56348558\n",
      "Val Iteration 2016, Loss: 0.47427630\n",
      "Val Iteration 2048, Loss: 0.43198781\n",
      "Val Iteration 2080, Loss: 0.52969619\n",
      "Val Iteration 2112, Loss: 0.44331508\n",
      "\n",
      "Epoch: 17\n",
      "Train Iteration 10656, Loss: 0.49140165\n",
      "Train Iteration 10688, Loss: 0.51966879\n",
      "Train Iteration 10720, Loss: 0.43408812\n",
      "Train Iteration 10752, Loss: 0.52176261\n",
      "Train Iteration 10784, Loss: 0.45535626\n",
      "Train Iteration 10816, Loss: 0.49304407\n",
      "Train Iteration 10848, Loss: 0.42288158\n",
      "Train Iteration 10880, Loss: 0.54306471\n",
      "Train Iteration 10912, Loss: 0.61849815\n",
      "Train Iteration 10944, Loss: 0.55516906\n",
      "Train Iteration 10976, Loss: 0.46595467\n",
      "Train Iteration 11008, Loss: 0.59688279\n",
      "Train Iteration 11040, Loss: 0.49445299\n",
      "Train Iteration 11072, Loss: 0.65968108\n",
      "Train Iteration 11104, Loss: 0.36450542\n",
      "Train Iteration 11136, Loss: 0.52516008\n",
      "Train Iteration 11168, Loss: 0.43306196\n",
      "Train Iteration 11200, Loss: 0.41076961\n",
      "Train Iteration 11232, Loss: 0.38709780\n",
      "Val Iteration 2144, Loss: 0.42143533\n",
      "Val Iteration 2176, Loss: 0.45822590\n",
      "Val Iteration 2208, Loss: 0.58197102\n",
      "Val Iteration 2240, Loss: 0.60034367\n",
      "\n",
      "Epoch: 18\n",
      "Train Iteration 11264, Loss: 0.62884865\n",
      "Train Iteration 11296, Loss: 0.60353260\n",
      "Train Iteration 11328, Loss: 0.51417866\n",
      "Train Iteration 11360, Loss: 0.51576499\n",
      "Train Iteration 11392, Loss: 0.55201871\n",
      "Train Iteration 11424, Loss: 0.55216956\n",
      "Train Iteration 11456, Loss: 0.47290842\n",
      "Train Iteration 11488, Loss: 0.49727248\n",
      "Train Iteration 11520, Loss: 0.45240381\n",
      "Train Iteration 11552, Loss: 0.46977019\n",
      "Train Iteration 11584, Loss: 0.66447050\n",
      "Train Iteration 11616, Loss: 0.60776102\n",
      "Train Iteration 11648, Loss: 0.40329166\n",
      "Train Iteration 11680, Loss: 0.49194850\n",
      "Train Iteration 11712, Loss: 0.67188132\n",
      "Train Iteration 11744, Loss: 0.53562140\n",
      "Train Iteration 11776, Loss: 0.48672599\n",
      "Train Iteration 11808, Loss: 0.30974814\n",
      "Train Iteration 11840, Loss: 0.62961806\n",
      "Train Iteration 11872, Loss: 0.57337756\n",
      "Val Iteration 2272, Loss: 0.55305378\n",
      "Val Iteration 2304, Loss: 0.50014095\n",
      "Val Iteration 2336, Loss: 0.43927291\n",
      "Val Iteration 2368, Loss: 0.43812341\n",
      "\n",
      "Epoch: 19\n",
      "Train Iteration 11904, Loss: 0.43012828\n",
      "Train Iteration 11936, Loss: 0.43358304\n",
      "Train Iteration 11968, Loss: 0.41912284\n",
      "Train Iteration 12000, Loss: 0.50414664\n",
      "Train Iteration 12032, Loss: 0.56507834\n",
      "Train Iteration 12064, Loss: 0.49814501\n",
      "Train Iteration 12096, Loss: 0.50809707\n",
      "Train Iteration 12128, Loss: 0.44179108\n",
      "Train Iteration 12160, Loss: 0.61189599\n",
      "Train Iteration 12192, Loss: 0.62112204\n",
      "Train Iteration 12224, Loss: 0.46883368\n",
      "Train Iteration 12256, Loss: 0.52597567\n",
      "Train Iteration 12288, Loss: 0.51810194\n",
      "Train Iteration 12320, Loss: 0.58394919\n",
      "Train Iteration 12352, Loss: 0.51601116\n",
      "Train Iteration 12384, Loss: 0.73959978\n",
      "Train Iteration 12416, Loss: 0.55792732\n",
      "Train Iteration 12448, Loss: 0.54960762\n",
      "Train Iteration 12480, Loss: 0.49991286\n",
      "Val Iteration 2400, Loss: 0.53840122\n",
      "Val Iteration 2432, Loss: 0.41899888\n",
      "Val Iteration 2464, Loss: 0.45653978\n",
      "Val Iteration 2496, Loss: 0.51417871\n",
      "\n",
      "Epoch: 20\n",
      "Train Iteration 12512, Loss: 0.47797365\n",
      "Train Iteration 12544, Loss: 0.55628938\n",
      "Train Iteration 12576, Loss: 0.50230236\n",
      "Train Iteration 12608, Loss: 0.43260288\n",
      "Train Iteration 12640, Loss: 0.50900810\n",
      "Train Iteration 12672, Loss: 0.57018804\n",
      "Train Iteration 12704, Loss: 0.43767480\n",
      "Train Iteration 12736, Loss: 0.57721905\n",
      "Train Iteration 12768, Loss: 0.60247517\n",
      "Train Iteration 12800, Loss: 0.53931150\n",
      "Train Iteration 12832, Loss: 0.36203955\n",
      "Train Iteration 12864, Loss: 0.43740259\n",
      "Train Iteration 12896, Loss: 0.53356486\n",
      "Train Iteration 12928, Loss: 0.47669412\n",
      "Train Iteration 12960, Loss: 0.38417965\n",
      "Train Iteration 12992, Loss: 0.47003744\n",
      "Train Iteration 13024, Loss: 0.46360778\n",
      "Train Iteration 13056, Loss: 0.30776972\n",
      "Train Iteration 13088, Loss: 0.35983022\n",
      "Train Iteration 13120, Loss: 0.39399504\n",
      "Val Iteration 2528, Loss: 0.44415166\n",
      "Val Iteration 2560, Loss: 0.39944252\n",
      "Val Iteration 2592, Loss: 0.42103618\n",
      "Val Iteration 2624, Loss: 0.60191234\n",
      "\n",
      "Epoch: 21\n",
      "Train Iteration 13152, Loss: 0.41151435\n",
      "Train Iteration 13184, Loss: 0.58006626\n",
      "Train Iteration 13216, Loss: 0.39385425\n",
      "Train Iteration 13248, Loss: 0.48620539\n",
      "Train Iteration 13280, Loss: 0.49300114\n",
      "Train Iteration 13312, Loss: 0.65050131\n",
      "Train Iteration 13344, Loss: 0.45451083\n",
      "Train Iteration 13376, Loss: 0.56056159\n",
      "Train Iteration 13408, Loss: 0.43108660\n",
      "Train Iteration 13440, Loss: 0.56506610\n",
      "Train Iteration 13472, Loss: 0.43723912\n",
      "Train Iteration 13504, Loss: 0.45815788\n",
      "Train Iteration 13536, Loss: 0.59702848\n",
      "Train Iteration 13568, Loss: 0.47362040\n",
      "Train Iteration 13600, Loss: 0.48243928\n",
      "Train Iteration 13632, Loss: 0.42091414\n",
      "Train Iteration 13664, Loss: 0.65173920\n",
      "Train Iteration 13696, Loss: 0.36422515\n",
      "Train Iteration 13728, Loss: 0.47662563\n",
      "Val Iteration 2656, Loss: 0.42964527\n",
      "Val Iteration 2688, Loss: 0.62619714\n",
      "Val Iteration 2720, Loss: 0.60220705\n",
      "\n",
      "Epoch: 22\n",
      "Train Iteration 13760, Loss: 0.50481736\n",
      "Train Iteration 13792, Loss: 0.49432217\n",
      "Train Iteration 13824, Loss: 0.45184791\n",
      "Train Iteration 13856, Loss: 0.54939943\n",
      "Train Iteration 13888, Loss: 0.66121667\n",
      "Train Iteration 13920, Loss: 0.58678710\n",
      "Train Iteration 13952, Loss: 0.52666860\n",
      "Train Iteration 13984, Loss: 0.55264962\n",
      "Train Iteration 14016, Loss: 0.40407088\n",
      "Train Iteration 14048, Loss: 0.51044142\n",
      "Train Iteration 14080, Loss: 0.62877537\n",
      "Train Iteration 14112, Loss: 0.51020602\n",
      "Train Iteration 14144, Loss: 0.61282490\n",
      "Train Iteration 14176, Loss: 0.34029576\n",
      "Train Iteration 14208, Loss: 0.50862075\n",
      "Train Iteration 14240, Loss: 0.53911142\n",
      "Train Iteration 14272, Loss: 0.53313504\n",
      "Train Iteration 14304, Loss: 0.63595743\n",
      "Train Iteration 14336, Loss: 0.51996262\n",
      "Train Iteration 14368, Loss: 0.47075825\n",
      "Val Iteration 2752, Loss: 0.43453732\n",
      "Val Iteration 2784, Loss: 0.52539597\n",
      "Val Iteration 2816, Loss: 0.42009336\n",
      "Val Iteration 2848, Loss: 0.44721231\n",
      "\n",
      "Epoch: 23\n",
      "Train Iteration 14400, Loss: 0.61038455\n",
      "Train Iteration 14432, Loss: 0.59112032\n",
      "Train Iteration 14464, Loss: 0.50053255\n",
      "Train Iteration 14496, Loss: 0.55007986\n",
      "Train Iteration 14528, Loss: 0.44350526\n",
      "Train Iteration 14560, Loss: 0.63605536\n",
      "Train Iteration 14592, Loss: 0.41379695\n",
      "Train Iteration 14624, Loss: 0.34500213\n",
      "Train Iteration 14656, Loss: 0.53042749\n",
      "Train Iteration 14688, Loss: 0.51587735\n",
      "Train Iteration 14720, Loss: 0.52327689\n",
      "Train Iteration 14752, Loss: 0.41957409\n",
      "Train Iteration 14784, Loss: 0.48503991\n",
      "Train Iteration 14816, Loss: 0.39672931\n",
      "Train Iteration 14848, Loss: 0.60831229\n",
      "Train Iteration 14880, Loss: 0.42678463\n",
      "Train Iteration 14912, Loss: 0.53980797\n",
      "Train Iteration 14944, Loss: 0.49437193\n",
      "Train Iteration 14976, Loss: 0.42487725\n",
      "Val Iteration 2880, Loss: 0.47870614\n",
      "Val Iteration 2912, Loss: 0.50928349\n",
      "Val Iteration 2944, Loss: 0.55353475\n",
      "Val Iteration 2976, Loss: 0.44443632\n",
      "\n",
      "Epoch: 24\n",
      "Train Iteration 15008, Loss: 0.51786717\n",
      "Train Iteration 15040, Loss: 0.57194598\n",
      "Train Iteration 15072, Loss: 0.50710969\n",
      "Train Iteration 15104, Loss: 0.54418145\n",
      "Train Iteration 15136, Loss: 0.55317627\n",
      "Train Iteration 15168, Loss: 0.45582346\n",
      "Train Iteration 15200, Loss: 0.53816335\n",
      "Train Iteration 15232, Loss: 0.39697123\n",
      "Train Iteration 15264, Loss: 0.53969731\n",
      "Train Iteration 15296, Loss: 0.40959600\n",
      "Train Iteration 15328, Loss: 0.51644575\n",
      "Train Iteration 15360, Loss: 0.59340528\n",
      "Train Iteration 15392, Loss: 0.55222487\n",
      "Train Iteration 15424, Loss: 0.52284083\n",
      "Train Iteration 15456, Loss: 0.61781869\n",
      "Train Iteration 15488, Loss: 0.62285828\n",
      "Train Iteration 15520, Loss: 0.41560269\n",
      "Train Iteration 15552, Loss: 0.42191999\n",
      "Train Iteration 15584, Loss: 0.54669841\n",
      "Train Iteration 15616, Loss: 0.54769442\n",
      "Val Iteration 3008, Loss: 0.58761078\n",
      "Val Iteration 3040, Loss: 0.39432880\n",
      "Val Iteration 3072, Loss: 0.38277428\n",
      "Val Iteration 3104, Loss: 0.55473454\n",
      "\n",
      "Epoch: 25\n",
      "Train Iteration 15648, Loss: 0.55533121\n",
      "Train Iteration 15680, Loss: 0.51129483\n",
      "Train Iteration 15712, Loss: 0.39119704\n",
      "Train Iteration 15744, Loss: 0.51577946\n",
      "Train Iteration 15776, Loss: 0.60006903\n",
      "Train Iteration 15808, Loss: 0.44509411\n",
      "Train Iteration 15840, Loss: 0.49903355\n",
      "Train Iteration 15872, Loss: 0.46430286\n",
      "Train Iteration 15904, Loss: 0.44202112\n",
      "Train Iteration 15936, Loss: 0.41645699\n",
      "Train Iteration 15968, Loss: 0.49100746\n",
      "Train Iteration 16000, Loss: 0.57517164\n",
      "Train Iteration 16032, Loss: 0.48057125\n",
      "Train Iteration 16064, Loss: 0.49509685\n",
      "Train Iteration 16096, Loss: 0.49006137\n",
      "Train Iteration 16128, Loss: 0.50115715\n",
      "Train Iteration 16160, Loss: 0.50550730\n",
      "Train Iteration 16192, Loss: 0.38418033\n",
      "Train Iteration 16224, Loss: 0.44725334\n",
      "Val Iteration 3136, Loss: 0.36128108\n",
      "Val Iteration 3168, Loss: 0.68174599\n",
      "Val Iteration 3200, Loss: 0.49067201\n",
      "Val Iteration 3232, Loss: 0.52673826\n",
      "\n",
      "Epoch: 26\n",
      "Train Iteration 16256, Loss: 0.53548527\n",
      "Train Iteration 16288, Loss: 0.76877284\n",
      "Train Iteration 16320, Loss: 0.56977470\n",
      "Train Iteration 16352, Loss: 0.52053236\n",
      "Train Iteration 16384, Loss: 0.53614169\n",
      "Train Iteration 16416, Loss: 0.49279464\n",
      "Train Iteration 16448, Loss: 0.48590774\n",
      "Train Iteration 16480, Loss: 0.42902320\n",
      "Train Iteration 16512, Loss: 0.46706431\n",
      "Train Iteration 16544, Loss: 0.52689287\n",
      "Train Iteration 16576, Loss: 0.46448564\n",
      "Train Iteration 16608, Loss: 0.32822598\n",
      "Train Iteration 16640, Loss: 0.53902774\n",
      "Train Iteration 16672, Loss: 0.50779365\n",
      "Train Iteration 16704, Loss: 0.64099550\n",
      "Train Iteration 16736, Loss: 0.50121322\n",
      "Train Iteration 16768, Loss: 0.49816686\n",
      "Train Iteration 16800, Loss: 0.56875759\n",
      "Train Iteration 16832, Loss: 0.62211968\n",
      "Train Iteration 16864, Loss: 0.60283431\n",
      "Val Iteration 3264, Loss: 0.49612066\n",
      "Val Iteration 3296, Loss: 0.33612008\n",
      "Val Iteration 3328, Loss: 0.52093279\n",
      "Val Iteration 3360, Loss: 0.44611111\n",
      "\n",
      "Epoch: 27\n",
      "Train Iteration 16896, Loss: 0.49239280\n",
      "Train Iteration 16928, Loss: 0.58781554\n",
      "Train Iteration 16960, Loss: 0.33338573\n",
      "Train Iteration 16992, Loss: 0.58320138\n",
      "Train Iteration 17024, Loss: 0.61692619\n",
      "Train Iteration 17056, Loss: 0.42868128\n",
      "Train Iteration 17088, Loss: 0.59105463\n",
      "Train Iteration 17120, Loss: 0.48933071\n",
      "Train Iteration 17152, Loss: 0.62279512\n",
      "Train Iteration 17184, Loss: 0.48541823\n",
      "Train Iteration 17216, Loss: 0.57041705\n",
      "Train Iteration 17248, Loss: 0.41631839\n",
      "Train Iteration 17280, Loss: 0.50052120\n",
      "Train Iteration 17312, Loss: 0.54986870\n",
      "Train Iteration 17344, Loss: 0.53859084\n",
      "Train Iteration 17376, Loss: 0.54227239\n",
      "Train Iteration 17408, Loss: 0.48447828\n",
      "Train Iteration 17440, Loss: 0.42332310\n",
      "Train Iteration 17472, Loss: 0.40027004\n",
      "Val Iteration 3392, Loss: 0.55013738\n",
      "Val Iteration 3424, Loss: 0.48743062\n",
      "Val Iteration 3456, Loss: 0.46670761\n",
      "Val Iteration 3488, Loss: 0.45026279\n",
      "\n",
      "Epoch: 28\n",
      "Train Iteration 17504, Loss: 0.55319651\n",
      "Train Iteration 17536, Loss: 0.50651970\n",
      "Train Iteration 17568, Loss: 0.58499277\n",
      "Train Iteration 17600, Loss: 0.58554222\n",
      "Train Iteration 17632, Loss: 0.58514562\n",
      "Train Iteration 17664, Loss: 0.58307456\n",
      "Train Iteration 17696, Loss: 0.51001685\n",
      "Train Iteration 17728, Loss: 0.58351449\n",
      "Train Iteration 17760, Loss: 0.51138571\n",
      "Train Iteration 17792, Loss: 0.40420103\n",
      "Train Iteration 17824, Loss: 0.46580808\n",
      "Train Iteration 17856, Loss: 0.43933458\n",
      "Train Iteration 17888, Loss: 0.51525090\n",
      "Train Iteration 17920, Loss: 0.39034031\n",
      "Train Iteration 17952, Loss: 0.54368546\n",
      "Train Iteration 17984, Loss: 0.60442331\n",
      "Train Iteration 18016, Loss: 0.58045175\n",
      "Train Iteration 18048, Loss: 0.55212766\n",
      "Train Iteration 18080, Loss: 0.39819929\n",
      "Train Iteration 18112, Loss: 0.57656060\n",
      "Val Iteration 3520, Loss: 0.63454887\n",
      "Val Iteration 3552, Loss: 0.51321664\n",
      "Val Iteration 3584, Loss: 0.58405402\n",
      "Val Iteration 3616, Loss: 0.48782985\n",
      "\n",
      "Epoch: 29\n",
      "Train Iteration 18144, Loss: 0.55722415\n",
      "Train Iteration 18176, Loss: 0.63527210\n",
      "Train Iteration 18208, Loss: 0.50761385\n",
      "Train Iteration 18240, Loss: 0.65063124\n",
      "Train Iteration 18272, Loss: 0.58549956\n",
      "Train Iteration 18304, Loss: 0.51846340\n",
      "Train Iteration 18336, Loss: 0.45973830\n",
      "Train Iteration 18368, Loss: 0.45873166\n",
      "Train Iteration 18400, Loss: 0.62420089\n",
      "Train Iteration 18432, Loss: 0.55641452\n",
      "Train Iteration 18464, Loss: 0.47360299\n",
      "Train Iteration 18496, Loss: 0.45024660\n",
      "Train Iteration 18528, Loss: 0.48283512\n",
      "Train Iteration 18560, Loss: 0.43503620\n",
      "Train Iteration 18592, Loss: 0.49737830\n",
      "Train Iteration 18624, Loss: 0.51162705\n",
      "Train Iteration 18656, Loss: 0.53357599\n",
      "Train Iteration 18688, Loss: 0.49612430\n",
      "Train Iteration 18720, Loss: 0.45950041\n",
      "Val Iteration 3648, Loss: 0.48989699\n",
      "Val Iteration 3680, Loss: 0.47052478\n",
      "Val Iteration 3712, Loss: 0.58876713\n",
      "Val Iteration 3744, Loss: 0.42998406\n",
      "\n",
      "Epoch: 30\n",
      "Train Iteration 18752, Loss: 0.41751762\n",
      "Train Iteration 18784, Loss: 0.43784025\n",
      "Train Iteration 18816, Loss: 0.44404506\n",
      "Train Iteration 18848, Loss: 0.49210016\n",
      "Train Iteration 18880, Loss: 0.46296767\n",
      "Train Iteration 18912, Loss: 0.47438415\n",
      "Train Iteration 18944, Loss: 0.38548749\n",
      "Train Iteration 18976, Loss: 0.60392332\n",
      "Train Iteration 19008, Loss: 0.61681363\n",
      "Train Iteration 19040, Loss: 0.47698855\n",
      "Train Iteration 19072, Loss: 0.42001712\n",
      "Train Iteration 19104, Loss: 0.69447647\n",
      "Train Iteration 19136, Loss: 0.70520984\n",
      "Train Iteration 19168, Loss: 0.46974436\n",
      "Train Iteration 19200, Loss: 0.43778799\n",
      "Train Iteration 19232, Loss: 0.56596497\n",
      "Train Iteration 19264, Loss: 0.80274358\n",
      "Train Iteration 19296, Loss: 0.51331716\n",
      "Train Iteration 19328, Loss: 0.60519909\n",
      "Train Iteration 19360, Loss: 0.46436489\n",
      "Val Iteration 3776, Loss: 0.41197265\n",
      "Val Iteration 3808, Loss: 0.55855347\n",
      "Val Iteration 3840, Loss: 0.49132216\n",
      "Val Iteration 3872, Loss: 0.52660076\n",
      "\n",
      "Epoch: 31\n",
      "Train Iteration 19392, Loss: 0.38370156\n",
      "Train Iteration 19424, Loss: 0.57338325\n",
      "Train Iteration 19456, Loss: 0.52302981\n",
      "Train Iteration 19488, Loss: 0.52806333\n",
      "Train Iteration 19520, Loss: 0.70667741\n",
      "Train Iteration 19552, Loss: 0.75723002\n",
      "Train Iteration 19584, Loss: 0.54020211\n",
      "Train Iteration 19616, Loss: 0.46743186\n",
      "Train Iteration 19648, Loss: 0.54543267\n",
      "Train Iteration 19680, Loss: 0.51691868\n",
      "Train Iteration 19712, Loss: 0.45788539\n",
      "Train Iteration 19744, Loss: 0.41191935\n",
      "Train Iteration 19776, Loss: 0.56813217\n",
      "Train Iteration 19808, Loss: 0.62743077\n",
      "Train Iteration 19840, Loss: 0.46240089\n",
      "Train Iteration 19872, Loss: 0.43677486\n",
      "Train Iteration 19904, Loss: 0.50865092\n",
      "Train Iteration 19936, Loss: 0.44475338\n",
      "Train Iteration 19968, Loss: 0.42847924\n",
      "Train Iteration 20000, Loss: 0.56532431\n",
      "Val Iteration 3904, Loss: 0.49047941\n",
      "Val Iteration 3936, Loss: 0.46315619\n",
      "Val Iteration 3968, Loss: 0.52679267\n",
      "Val Iteration 4000, Loss: 0.30963298\n",
      "\n",
      "Epoch: 32\n",
      "Train Iteration 20032, Loss: 0.52484832\n",
      "Train Iteration 20064, Loss: 0.41087019\n",
      "Train Iteration 20096, Loss: 0.56915736\n",
      "Train Iteration 20128, Loss: 0.47190094\n",
      "Train Iteration 20160, Loss: 0.53001923\n",
      "Train Iteration 20192, Loss: 0.42039285\n",
      "Train Iteration 20224, Loss: 0.58015894\n",
      "Train Iteration 20256, Loss: 0.52370191\n",
      "Train Iteration 20288, Loss: 0.67161278\n",
      "Train Iteration 20320, Loss: 0.55648155\n",
      "Train Iteration 20352, Loss: 0.41758348\n",
      "Train Iteration 20384, Loss: 0.61516176\n",
      "Train Iteration 20416, Loss: 0.60082531\n",
      "Train Iteration 20448, Loss: 0.47837862\n",
      "Train Iteration 20480, Loss: 0.61197477\n",
      "Train Iteration 20512, Loss: 0.34044753\n",
      "Train Iteration 20544, Loss: 0.56343948\n",
      "Train Iteration 20576, Loss: 0.50050934\n",
      "Train Iteration 20608, Loss: 0.61527445\n",
      "Val Iteration 4032, Loss: 0.39358141\n",
      "Val Iteration 4064, Loss: 0.63651100\n",
      "Val Iteration 4096, Loss: 0.41127765\n",
      "\n",
      "Epoch: 33\n",
      "Train Iteration 20640, Loss: 0.53886952\n",
      "Train Iteration 20672, Loss: 0.39619075\n",
      "Train Iteration 20704, Loss: 0.60018123\n",
      "Train Iteration 20736, Loss: 0.39870029\n",
      "Train Iteration 20768, Loss: 0.46787916\n",
      "Train Iteration 20800, Loss: 0.55700688\n",
      "Train Iteration 20832, Loss: 0.50265577\n",
      "Train Iteration 20864, Loss: 0.66913947\n",
      "Train Iteration 20896, Loss: 0.48149899\n",
      "Train Iteration 20928, Loss: 0.53047462\n",
      "Train Iteration 20960, Loss: 0.49200017\n",
      "Train Iteration 20992, Loss: 0.51429240\n",
      "Train Iteration 21024, Loss: 0.39610464\n",
      "Train Iteration 21056, Loss: 0.55017575\n",
      "Train Iteration 21088, Loss: 0.46780590\n",
      "Train Iteration 21120, Loss: 0.38309443\n",
      "Train Iteration 21152, Loss: 0.32570728\n",
      "Train Iteration 21184, Loss: 0.44141540\n",
      "Train Iteration 21216, Loss: 0.50923598\n",
      "Train Iteration 21248, Loss: 0.38823023\n",
      "Val Iteration 4128, Loss: 0.56131184\n",
      "Val Iteration 4160, Loss: 0.51506401\n",
      "Val Iteration 4192, Loss: 0.40981105\n",
      "Val Iteration 4224, Loss: 0.52943644\n",
      "\n",
      "Epoch: 34\n",
      "Train Iteration 21280, Loss: 0.55084449\n",
      "Train Iteration 21312, Loss: 0.33781905\n",
      "Train Iteration 21344, Loss: 0.47583602\n",
      "Train Iteration 21376, Loss: 0.51600572\n",
      "Train Iteration 21408, Loss: 0.60940405\n",
      "Train Iteration 21440, Loss: 0.35158036\n",
      "Train Iteration 21472, Loss: 0.46243172\n",
      "Train Iteration 21504, Loss: 0.58243892\n",
      "Train Iteration 21536, Loss: 0.37514751\n",
      "Train Iteration 21568, Loss: 0.50512081\n",
      "Train Iteration 21600, Loss: 0.38516576\n",
      "Train Iteration 21632, Loss: 0.48576476\n",
      "Train Iteration 21664, Loss: 0.44060791\n",
      "Train Iteration 21696, Loss: 0.59405522\n",
      "Train Iteration 21728, Loss: 0.45975422\n",
      "Train Iteration 21760, Loss: 0.29179847\n",
      "Train Iteration 21792, Loss: 0.54354034\n",
      "Train Iteration 21824, Loss: 0.46586134\n",
      "Train Iteration 21856, Loss: 0.42206412\n",
      "Val Iteration 4256, Loss: 0.43305509\n",
      "Val Iteration 4288, Loss: 0.52745771\n",
      "Val Iteration 4320, Loss: 0.46530214\n",
      "Val Iteration 4352, Loss: 0.44190352\n",
      "\n",
      "Epoch: 35\n",
      "Train Iteration 21888, Loss: 0.46356879\n",
      "Train Iteration 21920, Loss: 0.42239791\n",
      "Train Iteration 21952, Loss: 0.54558088\n",
      "Train Iteration 21984, Loss: 0.48763994\n",
      "Train Iteration 22016, Loss: 0.54186978\n",
      "Train Iteration 22048, Loss: 0.48957346\n",
      "Train Iteration 22080, Loss: 0.37435831\n",
      "Train Iteration 22112, Loss: 0.44799437\n",
      "Train Iteration 22144, Loss: 0.57065857\n",
      "Train Iteration 22176, Loss: 0.49464553\n",
      "Train Iteration 22208, Loss: 0.48988284\n",
      "Train Iteration 22240, Loss: 0.47566878\n",
      "Train Iteration 22272, Loss: 0.50649852\n",
      "Train Iteration 22304, Loss: 0.60158992\n",
      "Train Iteration 22336, Loss: 0.61801678\n",
      "Train Iteration 22368, Loss: 0.62546259\n",
      "Train Iteration 22400, Loss: 0.71788288\n",
      "Train Iteration 22432, Loss: 0.38807670\n",
      "Train Iteration 22464, Loss: 0.48637274\n",
      "Train Iteration 22496, Loss: 0.44827894\n",
      "Val Iteration 4384, Loss: 0.35439271\n",
      "Val Iteration 4416, Loss: 0.44236031\n",
      "Val Iteration 4448, Loss: 0.59458066\n",
      "Val Iteration 4480, Loss: 0.45398419\n",
      "\n",
      "Epoch: 36\n",
      "Train Iteration 22528, Loss: 0.60285897\n",
      "Train Iteration 22560, Loss: 0.37675661\n",
      "Train Iteration 22592, Loss: 0.56156238\n",
      "Train Iteration 22624, Loss: 0.73504252\n",
      "Train Iteration 22656, Loss: 0.49338220\n",
      "Train Iteration 22688, Loss: 0.54609353\n",
      "Train Iteration 22720, Loss: 0.58074956\n",
      "Train Iteration 22752, Loss: 0.60207534\n",
      "Train Iteration 22784, Loss: 0.49855208\n",
      "Train Iteration 22816, Loss: 0.46396709\n",
      "Train Iteration 22848, Loss: 0.63742276\n",
      "Train Iteration 22880, Loss: 0.45298278\n",
      "Train Iteration 22912, Loss: 0.59352416\n",
      "Train Iteration 22944, Loss: 0.38222557\n",
      "Train Iteration 22976, Loss: 0.46262152\n",
      "Train Iteration 23008, Loss: 0.50887826\n",
      "Train Iteration 23040, Loss: 0.44375096\n",
      "Train Iteration 23072, Loss: 0.60569972\n",
      "Train Iteration 23104, Loss: 0.42118175\n",
      "Val Iteration 4512, Loss: 0.51971554\n",
      "Val Iteration 4544, Loss: 0.51788844\n",
      "Val Iteration 4576, Loss: 0.52195172\n",
      "Val Iteration 4608, Loss: 0.45066229\n",
      "\n",
      "Epoch: 37\n",
      "Train Iteration 23136, Loss: 0.58354073\n",
      "Train Iteration 23168, Loss: 0.56891048\n",
      "Train Iteration 23200, Loss: 0.58863384\n",
      "Train Iteration 23232, Loss: 0.45629375\n",
      "Train Iteration 23264, Loss: 0.51074581\n",
      "Train Iteration 23296, Loss: 0.61835985\n",
      "Train Iteration 23328, Loss: 0.45855551\n",
      "Train Iteration 23360, Loss: 0.55479477\n",
      "Train Iteration 23392, Loss: 0.58206295\n",
      "Train Iteration 23424, Loss: 0.55388947\n",
      "Train Iteration 23456, Loss: 0.41384273\n",
      "Train Iteration 23488, Loss: 0.69545652\n",
      "Train Iteration 23520, Loss: 0.78910164\n",
      "Train Iteration 23552, Loss: 0.54428030\n",
      "Train Iteration 23584, Loss: 0.51167833\n",
      "Train Iteration 23616, Loss: 0.44306832\n",
      "Train Iteration 23648, Loss: 0.46699892\n",
      "Train Iteration 23680, Loss: 0.44104037\n",
      "Train Iteration 23712, Loss: 0.38997229\n",
      "Train Iteration 23744, Loss: 0.48389354\n",
      "Val Iteration 4640, Loss: 0.41190341\n",
      "Val Iteration 4672, Loss: 0.49417818\n",
      "Val Iteration 4704, Loss: 0.50263899\n",
      "Val Iteration 4736, Loss: 0.54040575\n",
      "\n",
      "Epoch: 38\n",
      "Train Iteration 23776, Loss: 0.49380068\n",
      "Train Iteration 23808, Loss: 0.56204911\n",
      "Train Iteration 23840, Loss: 0.55499736\n",
      "Train Iteration 23872, Loss: 0.47159686\n",
      "Train Iteration 23904, Loss: 0.52614366\n",
      "Train Iteration 23936, Loss: 0.40220491\n",
      "Train Iteration 23968, Loss: 0.60973348\n",
      "Train Iteration 24000, Loss: 0.59081497\n",
      "Train Iteration 24032, Loss: 0.43109485\n",
      "Train Iteration 24064, Loss: 0.54275594\n",
      "Train Iteration 24096, Loss: 0.58664871\n",
      "Train Iteration 24128, Loss: 0.57615776\n",
      "Train Iteration 24160, Loss: 0.41126576\n",
      "Train Iteration 24192, Loss: 0.49701123\n",
      "Train Iteration 24224, Loss: 0.46866818\n",
      "Train Iteration 24256, Loss: 0.53116804\n",
      "Train Iteration 24288, Loss: 0.67400221\n",
      "Train Iteration 24320, Loss: 0.49107148\n",
      "Train Iteration 24352, Loss: 0.53084712\n",
      "Val Iteration 4768, Loss: 0.57165382\n",
      "Val Iteration 4800, Loss: 0.51306099\n",
      "Val Iteration 4832, Loss: 0.54210550\n",
      "Val Iteration 4864, Loss: 0.43352917\n",
      "\n",
      "Epoch: 39\n",
      "Train Iteration 24384, Loss: 0.35604442\n",
      "Train Iteration 24416, Loss: 0.59862347\n",
      "Train Iteration 24448, Loss: 0.62466799\n",
      "Train Iteration 24480, Loss: 0.52063361\n",
      "Train Iteration 24512, Loss: 0.48551773\n",
      "Train Iteration 24544, Loss: 0.53196824\n",
      "Train Iteration 24576, Loss: 0.65702987\n",
      "Train Iteration 24608, Loss: 0.55032588\n",
      "Train Iteration 24640, Loss: 0.36653478\n",
      "Train Iteration 24672, Loss: 0.57924508\n",
      "Train Iteration 24704, Loss: 0.59394043\n",
      "Train Iteration 24736, Loss: 0.60865362\n",
      "Train Iteration 24768, Loss: 0.63461429\n",
      "Train Iteration 24800, Loss: 0.51146914\n",
      "Train Iteration 24832, Loss: 0.50452556\n",
      "Train Iteration 24864, Loss: 0.60392092\n",
      "Train Iteration 24896, Loss: 0.54688813\n",
      "Train Iteration 24928, Loss: 0.68213639\n",
      "Train Iteration 24960, Loss: 0.70375675\n",
      "Train Iteration 24992, Loss: 0.52345507\n",
      "Val Iteration 4896, Loss: 0.54486571\n",
      "Val Iteration 4928, Loss: 0.45306149\n",
      "Val Iteration 4960, Loss: 0.52958017\n",
      "Val Iteration 4992, Loss: 0.38339568\n",
      "\n",
      "Epoch: 40\n",
      "Train Iteration 25024, Loss: 0.55423770\n",
      "Train Iteration 25056, Loss: 0.52760695\n",
      "Train Iteration 25088, Loss: 0.50616793\n",
      "Train Iteration 25120, Loss: 0.53908587\n",
      "Train Iteration 25152, Loss: 0.62856263\n",
      "Train Iteration 25184, Loss: 0.57714404\n",
      "Train Iteration 25216, Loss: 0.54520337\n",
      "Train Iteration 25248, Loss: 0.56964597\n",
      "Train Iteration 25280, Loss: 0.38976071\n",
      "Train Iteration 25312, Loss: 0.47972593\n",
      "Train Iteration 25344, Loss: 0.55372745\n",
      "Train Iteration 25376, Loss: 0.58941012\n",
      "Train Iteration 25408, Loss: 0.51279517\n",
      "Train Iteration 25440, Loss: 0.45907992\n",
      "Train Iteration 25472, Loss: 0.41163161\n",
      "Train Iteration 25504, Loss: 0.44902800\n",
      "Train Iteration 25536, Loss: 0.54516937\n",
      "Train Iteration 25568, Loss: 0.69700508\n",
      "Train Iteration 25600, Loss: 0.67853108\n",
      "Val Iteration 5024, Loss: 0.54917945\n",
      "Val Iteration 5056, Loss: 0.44998904\n",
      "Val Iteration 5088, Loss: 0.43511587\n",
      "Val Iteration 5120, Loss: 0.49865724\n",
      "\n",
      "Epoch: 41\n",
      "Train Iteration 25632, Loss: 0.56400038\n",
      "Train Iteration 25664, Loss: 0.48492798\n",
      "Train Iteration 25696, Loss: 0.56356041\n",
      "Train Iteration 25728, Loss: 0.58890318\n",
      "Train Iteration 25760, Loss: 0.38141651\n",
      "Train Iteration 25792, Loss: 0.68856784\n",
      "Train Iteration 25824, Loss: 0.37205148\n",
      "Train Iteration 25856, Loss: 0.62751471\n",
      "Train Iteration 25888, Loss: 0.34482159\n",
      "Train Iteration 25920, Loss: 0.51471276\n",
      "Train Iteration 25952, Loss: 0.51501428\n",
      "Train Iteration 25984, Loss: 0.49735842\n",
      "Train Iteration 26016, Loss: 0.47760957\n",
      "Train Iteration 26048, Loss: 0.41741272\n",
      "Train Iteration 26080, Loss: 0.55046335\n",
      "Train Iteration 26112, Loss: 0.48602026\n",
      "Train Iteration 26144, Loss: 0.59656453\n",
      "Train Iteration 26176, Loss: 0.79409885\n",
      "Train Iteration 26208, Loss: 0.52471165\n",
      "Train Iteration 26240, Loss: 0.47226672\n",
      "Val Iteration 5152, Loss: 0.37972121\n",
      "Val Iteration 5184, Loss: 0.51791395\n",
      "Val Iteration 5216, Loss: 0.52687988\n",
      "Val Iteration 5248, Loss: 0.51186183\n",
      "\n",
      "Epoch: 42\n",
      "Train Iteration 26272, Loss: 0.48272820\n",
      "Train Iteration 26304, Loss: 0.43824428\n",
      "Train Iteration 26336, Loss: 0.53986105\n",
      "Train Iteration 26368, Loss: 0.58164558\n",
      "Train Iteration 26400, Loss: 0.60058489\n",
      "Train Iteration 26432, Loss: 0.49310792\n",
      "Train Iteration 26464, Loss: 0.56218021\n",
      "Train Iteration 26496, Loss: 0.47928770\n",
      "Train Iteration 26528, Loss: 0.61586958\n",
      "Train Iteration 26560, Loss: 0.56823151\n",
      "Train Iteration 26592, Loss: 0.40202246\n",
      "Train Iteration 26624, Loss: 0.54786170\n",
      "Train Iteration 26656, Loss: 0.39905469\n",
      "Train Iteration 26688, Loss: 0.57303996\n",
      "Train Iteration 26720, Loss: 0.43911240\n",
      "Train Iteration 26752, Loss: 0.48687886\n",
      "Train Iteration 26784, Loss: 0.40576213\n",
      "Train Iteration 26816, Loss: 0.49311832\n",
      "Train Iteration 26848, Loss: 0.44285216\n",
      "Val Iteration 5280, Loss: 0.51168036\n",
      "Val Iteration 5312, Loss: 0.42425163\n",
      "Val Iteration 5344, Loss: 0.51940357\n",
      "\n",
      "Epoch: 43\n",
      "Train Iteration 26880, Loss: 0.68032338\n",
      "Train Iteration 26912, Loss: 0.52689958\n",
      "Train Iteration 26944, Loss: 0.50301590\n",
      "Train Iteration 26976, Loss: 0.44072150\n",
      "Train Iteration 27008, Loss: 0.53489324\n",
      "Train Iteration 27040, Loss: 0.52635029\n",
      "Train Iteration 27072, Loss: 0.53934537\n",
      "Train Iteration 27104, Loss: 0.57335886\n",
      "Train Iteration 27136, Loss: 0.49521081\n",
      "Train Iteration 27168, Loss: 0.34332524\n",
      "Train Iteration 27200, Loss: 0.46093681\n",
      "Train Iteration 27232, Loss: 0.46797228\n",
      "Train Iteration 27264, Loss: 0.39266179\n",
      "Train Iteration 27296, Loss: 0.57072827\n",
      "Train Iteration 27328, Loss: 0.38047325\n",
      "Train Iteration 27360, Loss: 0.45475363\n",
      "Train Iteration 27392, Loss: 0.49282110\n",
      "Train Iteration 27424, Loss: 0.57923799\n",
      "Train Iteration 27456, Loss: 0.45942161\n",
      "Train Iteration 27488, Loss: 0.30960127\n",
      "Val Iteration 5376, Loss: 0.43911956\n",
      "Val Iteration 5408, Loss: 0.58809637\n",
      "Val Iteration 5440, Loss: 0.47613820\n",
      "Val Iteration 5472, Loss: 0.63773971\n",
      "\n",
      "Epoch: 44\n",
      "Train Iteration 27520, Loss: 0.39482434\n",
      "Train Iteration 27552, Loss: 0.38807185\n",
      "Train Iteration 27584, Loss: 0.45559808\n",
      "Train Iteration 27616, Loss: 0.56666412\n",
      "Train Iteration 27648, Loss: 0.60743117\n",
      "Train Iteration 27680, Loss: 0.72612726\n",
      "Train Iteration 27712, Loss: 0.61094501\n",
      "Train Iteration 27744, Loss: 0.61297592\n",
      "Train Iteration 27776, Loss: 0.41034045\n",
      "Train Iteration 27808, Loss: 0.60582647\n",
      "Train Iteration 27840, Loss: 0.44227978\n",
      "Train Iteration 27872, Loss: 0.52793939\n",
      "Train Iteration 27904, Loss: 0.36960802\n",
      "Train Iteration 27936, Loss: 0.43705832\n",
      "Train Iteration 27968, Loss: 0.41005367\n",
      "Train Iteration 28000, Loss: 0.40909802\n",
      "Train Iteration 28032, Loss: 0.52144628\n",
      "Train Iteration 28064, Loss: 0.42560121\n",
      "Train Iteration 28096, Loss: 0.50023257\n",
      "Val Iteration 5504, Loss: 0.39247018\n",
      "Val Iteration 5536, Loss: 0.41530340\n",
      "Val Iteration 5568, Loss: 0.55299042\n",
      "Val Iteration 5600, Loss: 0.46807613\n",
      "\n",
      "Epoch: 45\n",
      "Train Iteration 28128, Loss: 0.50700782\n",
      "Train Iteration 28160, Loss: 0.50966322\n",
      "Train Iteration 28192, Loss: 0.40161610\n",
      "Train Iteration 28224, Loss: 0.66498297\n",
      "Train Iteration 28256, Loss: 0.45398272\n",
      "Train Iteration 28288, Loss: 0.36303795\n",
      "Train Iteration 28320, Loss: 0.47111861\n",
      "Train Iteration 28352, Loss: 0.67566739\n",
      "Train Iteration 28384, Loss: 0.47786485\n",
      "Train Iteration 28416, Loss: 0.46219260\n",
      "Train Iteration 28448, Loss: 0.42488617\n",
      "Train Iteration 28480, Loss: 0.49478199\n",
      "Train Iteration 28512, Loss: 0.45209327\n",
      "Train Iteration 28544, Loss: 0.59816391\n",
      "Train Iteration 28576, Loss: 0.54923059\n",
      "Train Iteration 28608, Loss: 0.37204781\n",
      "Train Iteration 28640, Loss: 0.52626901\n",
      "Train Iteration 28672, Loss: 0.66793972\n",
      "Train Iteration 28704, Loss: 0.41862626\n",
      "Train Iteration 28736, Loss: 0.45748271\n",
      "Val Iteration 5632, Loss: 0.46401324\n",
      "Val Iteration 5664, Loss: 0.39293516\n",
      "Val Iteration 5696, Loss: 0.51406779\n",
      "Val Iteration 5728, Loss: 0.46918897\n",
      "\n",
      "Epoch: 46\n",
      "Train Iteration 28768, Loss: 0.61867093\n",
      "Train Iteration 28800, Loss: 0.50642179\n",
      "Train Iteration 28832, Loss: 0.65902587\n",
      "Train Iteration 28864, Loss: 0.47393469\n",
      "Train Iteration 28896, Loss: 0.54663088\n",
      "Train Iteration 28928, Loss: 0.44511900\n",
      "Train Iteration 28960, Loss: 0.54444158\n",
      "Train Iteration 28992, Loss: 0.51363255\n",
      "Train Iteration 29024, Loss: 0.46328693\n",
      "Train Iteration 29056, Loss: 0.42953953\n",
      "Train Iteration 29088, Loss: 0.40367789\n",
      "Train Iteration 29120, Loss: 0.42042431\n",
      "Train Iteration 29152, Loss: 0.60390138\n",
      "Train Iteration 29184, Loss: 0.45989189\n",
      "Train Iteration 29216, Loss: 0.47007089\n",
      "Train Iteration 29248, Loss: 0.53838928\n",
      "Train Iteration 29280, Loss: 0.42530222\n",
      "Train Iteration 29312, Loss: 0.56220400\n",
      "Train Iteration 29344, Loss: 0.60874263\n",
      "Val Iteration 5760, Loss: 0.37959720\n",
      "Val Iteration 5792, Loss: 0.77505680\n",
      "Val Iteration 5824, Loss: 0.59363763\n",
      "Val Iteration 5856, Loss: 0.47330608\n",
      "\n",
      "Epoch: 47\n",
      "Train Iteration 29376, Loss: 0.46831020\n",
      "Train Iteration 29408, Loss: 0.61114199\n",
      "Train Iteration 29440, Loss: 0.50968645\n",
      "Train Iteration 29472, Loss: 0.58965329\n",
      "Train Iteration 29504, Loss: 0.63762403\n",
      "Train Iteration 29536, Loss: 0.45333847\n",
      "Train Iteration 29568, Loss: 0.49737779\n",
      "Train Iteration 29600, Loss: 0.57801019\n",
      "Train Iteration 29632, Loss: 0.45765356\n",
      "Train Iteration 29664, Loss: 0.53198109\n",
      "Train Iteration 29696, Loss: 0.58827313\n",
      "Train Iteration 29728, Loss: 0.49388863\n",
      "Train Iteration 29760, Loss: 0.37776740\n",
      "Train Iteration 29792, Loss: 0.50526844\n",
      "Train Iteration 29824, Loss: 0.57015612\n",
      "Train Iteration 29856, Loss: 0.60267449\n",
      "Train Iteration 29888, Loss: 0.58198948\n",
      "Train Iteration 29920, Loss: 0.32954831\n",
      "Train Iteration 29952, Loss: 0.45271928\n",
      "Train Iteration 29984, Loss: 0.46211316\n",
      "Val Iteration 5888, Loss: 0.49412622\n",
      "Val Iteration 5920, Loss: 0.49341851\n",
      "Val Iteration 5952, Loss: 0.32037943\n",
      "Val Iteration 5984, Loss: 0.49901185\n",
      "\n",
      "Epoch: 48\n",
      "Train Iteration 30016, Loss: 0.45930551\n",
      "Train Iteration 30048, Loss: 0.39947066\n",
      "Train Iteration 30080, Loss: 0.32386293\n",
      "Train Iteration 30112, Loss: 0.50964344\n",
      "Train Iteration 30144, Loss: 0.51972897\n",
      "Train Iteration 30176, Loss: 0.46283596\n",
      "Train Iteration 30208, Loss: 0.55956703\n",
      "Train Iteration 30240, Loss: 0.49895995\n",
      "Train Iteration 30272, Loss: 0.41078498\n",
      "Train Iteration 30304, Loss: 0.63536721\n",
      "Train Iteration 30336, Loss: 0.60849663\n",
      "Train Iteration 30368, Loss: 0.37432173\n",
      "Train Iteration 30400, Loss: 0.65045374\n",
      "Train Iteration 30432, Loss: 0.38441129\n",
      "Train Iteration 30464, Loss: 0.39215400\n",
      "Train Iteration 30496, Loss: 0.65116877\n",
      "Train Iteration 30528, Loss: 0.45424276\n",
      "Train Iteration 30560, Loss: 0.44954346\n",
      "Train Iteration 30592, Loss: 0.49354434\n",
      "Train Iteration 30624, Loss: 0.52496966\n",
      "Val Iteration 6016, Loss: 0.47086797\n",
      "Val Iteration 6048, Loss: 0.45469611\n",
      "Val Iteration 6080, Loss: 0.51065321\n",
      "Val Iteration 6112, Loss: 0.45610070\n",
      "\n",
      "Epoch: 49\n",
      "Train Iteration 30656, Loss: 0.50004467\n",
      "Train Iteration 30688, Loss: 0.43373571\n",
      "Train Iteration 30720, Loss: 0.42193642\n",
      "Train Iteration 30752, Loss: 0.56984258\n",
      "Train Iteration 30784, Loss: 0.52314164\n",
      "Train Iteration 30816, Loss: 0.53489582\n",
      "Train Iteration 30848, Loss: 0.57474177\n",
      "Train Iteration 30880, Loss: 0.50955648\n",
      "Train Iteration 30912, Loss: 0.62358323\n",
      "Train Iteration 30944, Loss: 0.45957193\n",
      "Train Iteration 30976, Loss: 0.46430180\n",
      "Train Iteration 31008, Loss: 0.60854148\n",
      "Train Iteration 31040, Loss: 0.46277963\n",
      "Train Iteration 31072, Loss: 0.46849032\n",
      "Train Iteration 31104, Loss: 0.41624994\n",
      "Train Iteration 31136, Loss: 0.51051180\n",
      "Train Iteration 31168, Loss: 0.46618832\n",
      "Train Iteration 31200, Loss: 0.54574095\n",
      "Train Iteration 31232, Loss: 0.63413696\n",
      "Val Iteration 6144, Loss: 0.42640811\n",
      "Val Iteration 6176, Loss: 0.46646599\n",
      "Val Iteration 6208, Loss: 0.58986719\n",
      "Val Iteration 6240, Loss: 0.58679798\n",
      "\n",
      "Epoch: 50\n",
      "Train Iteration 31264, Loss: 0.51008406\n",
      "Train Iteration 31296, Loss: 0.55319316\n",
      "Train Iteration 31328, Loss: 0.66680694\n",
      "Train Iteration 31360, Loss: 0.55935373\n",
      "Train Iteration 31392, Loss: 0.51300342\n",
      "Train Iteration 31424, Loss: 0.46724427\n",
      "Train Iteration 31456, Loss: 0.71944602\n",
      "Train Iteration 31488, Loss: 0.57188760\n",
      "Train Iteration 31520, Loss: 0.52400840\n",
      "Train Iteration 31552, Loss: 0.32969440\n",
      "Train Iteration 31584, Loss: 0.60114809\n",
      "Train Iteration 31616, Loss: 0.59452114\n",
      "Train Iteration 31648, Loss: 0.41735359\n",
      "Train Iteration 31680, Loss: 0.48660975\n",
      "Train Iteration 31712, Loss: 0.47739160\n",
      "Train Iteration 31744, Loss: 0.54139601\n",
      "Train Iteration 31776, Loss: 0.33076097\n",
      "Train Iteration 31808, Loss: 0.57264145\n",
      "Train Iteration 31840, Loss: 0.65178804\n",
      "Train Iteration 31872, Loss: 0.54104176\n",
      "Val Iteration 6272, Loss: 0.55505467\n",
      "Val Iteration 6304, Loss: 0.51183040\n",
      "Val Iteration 6336, Loss: 0.44017686\n",
      "Val Iteration 6368, Loss: 0.42230136\n",
      "\n",
      "Epoch: 51\n",
      "Train Iteration 31904, Loss: 0.59308839\n",
      "Train Iteration 31936, Loss: 0.40550938\n",
      "Train Iteration 31968, Loss: 0.49341489\n",
      "Train Iteration 32000, Loss: 0.44296056\n",
      "Train Iteration 32032, Loss: 0.59294874\n",
      "Train Iteration 32064, Loss: 0.63546595\n",
      "Train Iteration 32096, Loss: 0.48714716\n",
      "Train Iteration 32128, Loss: 0.44376795\n",
      "Train Iteration 32160, Loss: 0.40520163\n",
      "Train Iteration 32192, Loss: 0.49030040\n",
      "Train Iteration 32224, Loss: 0.65685662\n",
      "Train Iteration 32256, Loss: 0.53499680\n",
      "Train Iteration 32288, Loss: 0.46551880\n",
      "Train Iteration 32320, Loss: 0.49426798\n",
      "Train Iteration 32352, Loss: 0.52996053\n",
      "Train Iteration 32384, Loss: 0.58871737\n",
      "Train Iteration 32416, Loss: 0.53180090\n",
      "Train Iteration 32448, Loss: 0.47501726\n",
      "Train Iteration 32480, Loss: 0.44025439\n",
      "Val Iteration 6400, Loss: 0.52383157\n",
      "Val Iteration 6432, Loss: 0.41819056\n",
      "Val Iteration 6464, Loss: 0.41581185\n",
      "Val Iteration 6496, Loss: 0.50472237\n",
      "\n",
      "Epoch: 52\n",
      "Train Iteration 32512, Loss: 0.52084106\n",
      "Train Iteration 32544, Loss: 0.62403143\n",
      "Train Iteration 32576, Loss: 0.69318503\n",
      "Train Iteration 32608, Loss: 0.46795936\n",
      "Train Iteration 32640, Loss: 0.54399614\n",
      "Train Iteration 32672, Loss: 0.76535666\n",
      "Train Iteration 32704, Loss: 0.67579376\n",
      "Train Iteration 32736, Loss: 0.56760554\n",
      "Train Iteration 32768, Loss: 0.38567360\n",
      "Train Iteration 32800, Loss: 0.42473816\n",
      "Train Iteration 32832, Loss: 0.50409414\n",
      "Train Iteration 32864, Loss: 0.56047636\n",
      "Train Iteration 32896, Loss: 0.44788506\n",
      "Train Iteration 32928, Loss: 0.48707537\n",
      "Train Iteration 32960, Loss: 0.49420236\n",
      "Train Iteration 32992, Loss: 0.64863088\n",
      "Train Iteration 33024, Loss: 0.44243288\n",
      "Train Iteration 33056, Loss: 0.50454948\n",
      "Train Iteration 33088, Loss: 0.50248015\n",
      "Train Iteration 33120, Loss: 0.61337847\n",
      "Val Iteration 6528, Loss: 0.44003166\n",
      "Val Iteration 6560, Loss: 0.42076782\n",
      "Val Iteration 6592, Loss: 0.41982798\n",
      "Val Iteration 6624, Loss: 0.57651818\n",
      "\n",
      "Epoch: 53\n",
      "Train Iteration 33152, Loss: 0.48432922\n",
      "Train Iteration 33184, Loss: 0.39961247\n",
      "Train Iteration 33216, Loss: 0.43917866\n",
      "Train Iteration 33248, Loss: 0.46866271\n",
      "Train Iteration 33280, Loss: 0.43293442\n",
      "Train Iteration 33312, Loss: 0.45468649\n",
      "Train Iteration 33344, Loss: 0.56174829\n",
      "Train Iteration 33376, Loss: 0.61271327\n",
      "Train Iteration 33408, Loss: 0.50221281\n",
      "Train Iteration 33440, Loss: 0.37701376\n",
      "Train Iteration 33472, Loss: 0.47074373\n",
      "Train Iteration 33504, Loss: 0.39505079\n",
      "Train Iteration 33536, Loss: 0.60047745\n",
      "Train Iteration 33568, Loss: 0.46558905\n",
      "Train Iteration 33600, Loss: 0.50553724\n",
      "Train Iteration 33632, Loss: 0.55773564\n",
      "Train Iteration 33664, Loss: 0.49761227\n",
      "Train Iteration 33696, Loss: 0.39675292\n",
      "Train Iteration 33728, Loss: 0.49863516\n",
      "Val Iteration 6656, Loss: 0.44144881\n",
      "Val Iteration 6688, Loss: 0.59177295\n",
      "Val Iteration 6720, Loss: 0.59638701\n",
      "\n",
      "Epoch: 54\n",
      "Train Iteration 33760, Loss: 0.39564713\n",
      "Train Iteration 33792, Loss: 0.51257230\n",
      "Train Iteration 33824, Loss: 0.50631066\n",
      "Train Iteration 33856, Loss: 0.49492366\n",
      "Train Iteration 33888, Loss: 0.32901470\n",
      "Train Iteration 33920, Loss: 0.51442147\n",
      "Train Iteration 33952, Loss: 0.60902181\n",
      "Train Iteration 33984, Loss: 0.49838676\n",
      "Train Iteration 34016, Loss: 0.50946476\n",
      "Train Iteration 34048, Loss: 0.52558608\n",
      "Train Iteration 34080, Loss: 0.62830423\n",
      "Train Iteration 34112, Loss: 0.50262168\n",
      "Train Iteration 34144, Loss: 0.52315774\n",
      "Train Iteration 34176, Loss: 0.46819506\n",
      "Train Iteration 34208, Loss: 0.52979467\n",
      "Train Iteration 34240, Loss: 0.52415897\n",
      "Train Iteration 34272, Loss: 0.53461664\n",
      "Train Iteration 34304, Loss: 0.43440088\n",
      "Train Iteration 34336, Loss: 0.34476029\n",
      "Train Iteration 34368, Loss: 0.40140637\n",
      "Val Iteration 6752, Loss: 0.41883042\n",
      "Val Iteration 6784, Loss: 0.51175260\n",
      "Val Iteration 6816, Loss: 0.41513359\n",
      "Val Iteration 6848, Loss: 0.45034902\n",
      "\n",
      "Epoch: 55\n",
      "Train Iteration 34400, Loss: 0.45335408\n",
      "Train Iteration 34432, Loss: 0.60866231\n",
      "Train Iteration 34464, Loss: 0.74043950\n",
      "Train Iteration 34496, Loss: 0.60378920\n",
      "Train Iteration 34528, Loss: 0.51774110\n",
      "Train Iteration 34560, Loss: 0.36083854\n",
      "Train Iteration 34592, Loss: 0.55009366\n",
      "Train Iteration 34624, Loss: 0.51611321\n",
      "Train Iteration 34656, Loss: 0.49390777\n",
      "Train Iteration 34688, Loss: 0.55882042\n",
      "Train Iteration 34720, Loss: 0.49684886\n",
      "Train Iteration 34752, Loss: 0.48883455\n",
      "Train Iteration 34784, Loss: 0.38574181\n",
      "Train Iteration 34816, Loss: 0.62681171\n",
      "Train Iteration 34848, Loss: 0.36719149\n",
      "Train Iteration 34880, Loss: 0.54878410\n",
      "Train Iteration 34912, Loss: 0.48390569\n",
      "Train Iteration 34944, Loss: 0.43304325\n",
      "Train Iteration 34976, Loss: 0.45702337\n",
      "Val Iteration 6880, Loss: 0.45638398\n",
      "Val Iteration 6912, Loss: 0.51744383\n",
      "Val Iteration 6944, Loss: 0.55704289\n",
      "Val Iteration 6976, Loss: 0.44878058\n",
      "\n",
      "Epoch: 56\n",
      "Train Iteration 35008, Loss: 0.45206562\n",
      "Train Iteration 35040, Loss: 0.51251189\n",
      "Train Iteration 35072, Loss: 0.56338621\n",
      "Train Iteration 35104, Loss: 0.36278740\n",
      "Train Iteration 35136, Loss: 0.68632274\n",
      "Train Iteration 35168, Loss: 0.47861912\n",
      "Train Iteration 35200, Loss: 0.40629150\n",
      "Train Iteration 35232, Loss: 0.44730545\n",
      "Train Iteration 35264, Loss: 0.59311271\n",
      "Train Iteration 35296, Loss: 0.58299170\n",
      "Train Iteration 35328, Loss: 0.30025026\n",
      "Train Iteration 35360, Loss: 0.47723748\n",
      "Train Iteration 35392, Loss: 0.62366945\n",
      "Train Iteration 35424, Loss: 0.54319916\n",
      "Train Iteration 35456, Loss: 0.42535176\n",
      "Train Iteration 35488, Loss: 0.45449141\n",
      "Train Iteration 35520, Loss: 0.46725016\n",
      "Train Iteration 35552, Loss: 0.46270603\n",
      "Train Iteration 35584, Loss: 0.39495321\n",
      "Train Iteration 35616, Loss: 0.52955285\n",
      "Val Iteration 7008, Loss: 0.61416172\n",
      "Val Iteration 7040, Loss: 0.41590980\n",
      "Val Iteration 7072, Loss: 0.38506665\n",
      "Val Iteration 7104, Loss: 0.55491525\n",
      "\n",
      "Epoch: 57\n",
      "Train Iteration 35648, Loss: 0.43961038\n",
      "Train Iteration 35680, Loss: 0.43827744\n",
      "Train Iteration 35712, Loss: 0.52674685\n",
      "Train Iteration 35744, Loss: 0.43907763\n",
      "Train Iteration 35776, Loss: 0.36408957\n",
      "Train Iteration 35808, Loss: 0.49491853\n",
      "Train Iteration 35840, Loss: 0.52356352\n",
      "Train Iteration 35872, Loss: 0.52945110\n",
      "Train Iteration 35904, Loss: 0.48530970\n",
      "Train Iteration 35936, Loss: 0.46464743\n",
      "Train Iteration 35968, Loss: 0.42789446\n",
      "Train Iteration 36000, Loss: 0.63095210\n",
      "Train Iteration 36032, Loss: 0.54442575\n",
      "Train Iteration 36064, Loss: 0.37420503\n",
      "Train Iteration 36096, Loss: 0.61345885\n",
      "Train Iteration 36128, Loss: 0.47689846\n",
      "Train Iteration 36160, Loss: 0.42831582\n",
      "Train Iteration 36192, Loss: 0.48543860\n",
      "Train Iteration 36224, Loss: 0.40518229\n",
      "Val Iteration 7136, Loss: 0.37078625\n",
      "Val Iteration 7168, Loss: 0.70177143\n",
      "Val Iteration 7200, Loss: 0.50193968\n",
      "Val Iteration 7232, Loss: 0.53036853\n",
      "\n",
      "Epoch: 58\n",
      "Train Iteration 36256, Loss: 0.53657509\n",
      "Train Iteration 36288, Loss: 0.58543708\n",
      "Train Iteration 36320, Loss: 0.64816315\n",
      "Train Iteration 36352, Loss: 0.44877347\n",
      "Train Iteration 36384, Loss: 0.56229248\n",
      "Train Iteration 36416, Loss: 0.50881027\n",
      "Train Iteration 36448, Loss: 0.57875036\n",
      "Train Iteration 36480, Loss: 0.40630128\n",
      "Train Iteration 36512, Loss: 0.49101426\n",
      "Train Iteration 36544, Loss: 0.59078337\n",
      "Train Iteration 36576, Loss: 0.43631057\n",
      "Train Iteration 36608, Loss: 0.51854858\n",
      "Train Iteration 36640, Loss: 0.41049696\n",
      "Train Iteration 36672, Loss: 0.39773186\n",
      "Train Iteration 36704, Loss: 0.56448557\n",
      "Train Iteration 36736, Loss: 0.50453439\n",
      "Train Iteration 36768, Loss: 0.52007586\n",
      "Train Iteration 36800, Loss: 0.43252761\n",
      "Train Iteration 36832, Loss: 0.49398516\n",
      "Train Iteration 36864, Loss: 0.41375910\n",
      "Val Iteration 7264, Loss: 0.49051673\n",
      "Val Iteration 7296, Loss: 0.33976210\n",
      "Val Iteration 7328, Loss: 0.53346396\n",
      "Val Iteration 7360, Loss: 0.44464760\n",
      "\n",
      "Epoch: 59\n",
      "Train Iteration 36896, Loss: 0.59401806\n",
      "Train Iteration 36928, Loss: 0.53717420\n",
      "Train Iteration 36960, Loss: 0.72919153\n",
      "Train Iteration 36992, Loss: 0.46086975\n",
      "Train Iteration 37024, Loss: 0.53644793\n",
      "Train Iteration 37056, Loss: 0.66912779\n",
      "Train Iteration 37088, Loss: 0.64875383\n",
      "Train Iteration 37120, Loss: 0.47413230\n",
      "Train Iteration 37152, Loss: 0.62010987\n",
      "Train Iteration 37184, Loss: 0.70386910\n",
      "Train Iteration 37216, Loss: 0.54930934\n",
      "Train Iteration 37248, Loss: 0.58898196\n",
      "Train Iteration 37280, Loss: 0.37700309\n",
      "Train Iteration 37312, Loss: 0.55978533\n",
      "Train Iteration 37344, Loss: 0.47956617\n",
      "Train Iteration 37376, Loss: 0.56606093\n",
      "Train Iteration 37408, Loss: 0.40734178\n",
      "Train Iteration 37440, Loss: 0.40531158\n",
      "Train Iteration 37472, Loss: 0.72105179\n",
      "Val Iteration 7392, Loss: 0.53832801\n",
      "Val Iteration 7424, Loss: 0.48950121\n",
      "Val Iteration 7456, Loss: 0.44142705\n",
      "Val Iteration 7488, Loss: 0.45054822\n",
      "\n",
      "Epoch: 60\n",
      "Train Iteration 37504, Loss: 0.49684520\n",
      "Train Iteration 37536, Loss: 0.36838777\n",
      "Train Iteration 37568, Loss: 0.42320592\n",
      "Train Iteration 37600, Loss: 0.54172675\n",
      "Train Iteration 37632, Loss: 0.57280334\n",
      "Train Iteration 37664, Loss: 0.38816594\n",
      "Train Iteration 37696, Loss: 0.64918527\n",
      "Train Iteration 37728, Loss: 0.60402792\n",
      "Train Iteration 37760, Loss: 0.36406026\n",
      "Train Iteration 37792, Loss: 0.53902932\n",
      "Train Iteration 37824, Loss: 0.47898610\n",
      "Train Iteration 37856, Loss: 0.42131908\n",
      "Train Iteration 37888, Loss: 0.42570847\n",
      "Train Iteration 37920, Loss: 0.62384856\n",
      "Train Iteration 37952, Loss: 0.45955646\n",
      "Train Iteration 37984, Loss: 0.57104039\n",
      "Train Iteration 38016, Loss: 0.53594186\n",
      "Train Iteration 38048, Loss: 0.68031813\n",
      "Train Iteration 38080, Loss: 0.48590803\n",
      "Train Iteration 38112, Loss: 0.45969782\n",
      "Val Iteration 7520, Loss: 0.64245528\n",
      "Val Iteration 7552, Loss: 0.51293778\n",
      "Val Iteration 7584, Loss: 0.57414136\n",
      "Val Iteration 7616, Loss: 0.47405605\n",
      "\n",
      "Epoch: 61\n",
      "Train Iteration 38144, Loss: 0.61845001\n",
      "Train Iteration 38176, Loss: 0.56443395\n",
      "Train Iteration 38208, Loss: 0.48570293\n",
      "Train Iteration 38240, Loss: 0.59643201\n",
      "Train Iteration 38272, Loss: 0.41580197\n",
      "Train Iteration 38304, Loss: 0.49072262\n",
      "Train Iteration 38336, Loss: 0.37164297\n",
      "Train Iteration 38368, Loss: 0.47155715\n",
      "Train Iteration 38400, Loss: 0.63565563\n",
      "Train Iteration 38432, Loss: 0.51668217\n",
      "Train Iteration 38464, Loss: 0.52501694\n",
      "Train Iteration 38496, Loss: 0.45500717\n",
      "Train Iteration 38528, Loss: 0.58351724\n",
      "Train Iteration 38560, Loss: 0.50977808\n",
      "Train Iteration 38592, Loss: 0.62238640\n",
      "Train Iteration 38624, Loss: 0.61226984\n",
      "Train Iteration 38656, Loss: 0.44073919\n",
      "Train Iteration 38688, Loss: 0.51826603\n",
      "Train Iteration 38720, Loss: 0.45307027\n",
      "Val Iteration 7648, Loss: 0.48698038\n",
      "Val Iteration 7680, Loss: 0.49219088\n",
      "Val Iteration 7712, Loss: 0.60420523\n",
      "Val Iteration 7744, Loss: 0.42516290\n",
      "\n",
      "Epoch: 62\n",
      "Train Iteration 38752, Loss: 0.41206096\n",
      "Train Iteration 38784, Loss: 0.56693978\n",
      "Train Iteration 38816, Loss: 0.53324597\n",
      "Train Iteration 38848, Loss: 0.61992122\n",
      "Train Iteration 38880, Loss: 0.73374272\n",
      "Train Iteration 38912, Loss: 0.62783662\n",
      "Train Iteration 38944, Loss: 0.48512588\n",
      "Train Iteration 38976, Loss: 0.53213108\n",
      "Train Iteration 39008, Loss: 0.83005065\n",
      "Train Iteration 39040, Loss: 0.44670077\n",
      "Train Iteration 39072, Loss: 0.43954352\n",
      "Train Iteration 39104, Loss: 0.51142979\n",
      "Train Iteration 39136, Loss: 0.74276506\n",
      "Train Iteration 39168, Loss: 0.42985065\n",
      "Train Iteration 39200, Loss: 0.61899872\n",
      "Train Iteration 39232, Loss: 0.47991764\n",
      "Train Iteration 39264, Loss: 0.55660858\n",
      "Train Iteration 39296, Loss: 0.32604449\n",
      "Train Iteration 39328, Loss: 0.39052908\n",
      "Train Iteration 39360, Loss: 0.51522942\n",
      "Val Iteration 7776, Loss: 0.41153258\n",
      "Val Iteration 7808, Loss: 0.54572856\n",
      "Val Iteration 7840, Loss: 0.51137600\n",
      "Val Iteration 7872, Loss: 0.51904992\n",
      "\n",
      "Epoch: 63\n",
      "Train Iteration 39392, Loss: 0.57231897\n",
      "Train Iteration 39424, Loss: 0.49604951\n",
      "Train Iteration 39456, Loss: 0.51283963\n",
      "Train Iteration 39488, Loss: 0.64675081\n",
      "Train Iteration 39520, Loss: 0.53313105\n",
      "Train Iteration 39552, Loss: 0.56924967\n",
      "Train Iteration 39584, Loss: 0.61205355\n",
      "Train Iteration 39616, Loss: 0.44191473\n",
      "Train Iteration 39648, Loss: 0.51311136\n",
      "Train Iteration 39680, Loss: 0.56582569\n",
      "Train Iteration 39712, Loss: 0.67347216\n",
      "Train Iteration 39744, Loss: 0.54402713\n",
      "Train Iteration 39776, Loss: 0.46498243\n",
      "Train Iteration 39808, Loss: 0.45597474\n",
      "Train Iteration 39840, Loss: 0.49799821\n",
      "Train Iteration 39872, Loss: 0.53193949\n",
      "Train Iteration 39904, Loss: 0.59026732\n",
      "Train Iteration 39936, Loss: 0.63467222\n",
      "Train Iteration 39968, Loss: 0.73281253\n",
      "Train Iteration 40000, Loss: 0.53208491\n",
      "Val Iteration 7904, Loss: 0.45342919\n",
      "Val Iteration 7936, Loss: 0.46226998\n",
      "Val Iteration 7968, Loss: 0.51820850\n",
      "Val Iteration 8000, Loss: 0.29170282\n",
      "\n",
      "Epoch: 64\n",
      "Train Iteration 40032, Loss: 0.41538682\n",
      "Train Iteration 40064, Loss: 0.46115151\n",
      "Train Iteration 40096, Loss: 0.37145728\n",
      "Train Iteration 40128, Loss: 0.47659620\n",
      "Train Iteration 40160, Loss: 0.39598878\n",
      "Train Iteration 40192, Loss: 0.65742359\n",
      "Train Iteration 40224, Loss: 0.36220253\n",
      "Train Iteration 40256, Loss: 0.56448929\n",
      "Train Iteration 40288, Loss: 0.41830146\n",
      "Train Iteration 40320, Loss: 0.39525198\n",
      "Train Iteration 40352, Loss: 0.50878858\n",
      "Train Iteration 40384, Loss: 0.53492990\n",
      "Train Iteration 40416, Loss: 0.61364167\n",
      "Train Iteration 40448, Loss: 0.44345759\n",
      "Train Iteration 40480, Loss: 0.45281686\n",
      "Train Iteration 40512, Loss: 0.37239391\n",
      "Train Iteration 40544, Loss: 0.52360884\n",
      "Train Iteration 40576, Loss: 0.47757176\n",
      "Train Iteration 40608, Loss: 0.50928569\n",
      "Val Iteration 8032, Loss: 0.40289076\n",
      "Val Iteration 8064, Loss: 0.64808410\n",
      "Val Iteration 8096, Loss: 0.43547213\n",
      "\n",
      "Epoch: 65\n",
      "Train Iteration 40640, Loss: 0.70682326\n",
      "Train Iteration 40672, Loss: 0.41633244\n",
      "Train Iteration 40704, Loss: 0.56564626\n",
      "Train Iteration 40736, Loss: 0.50190479\n",
      "Train Iteration 40768, Loss: 0.62448108\n",
      "Train Iteration 40800, Loss: 0.41749344\n",
      "Train Iteration 40832, Loss: 0.65426033\n",
      "Train Iteration 40864, Loss: 0.34785197\n",
      "Train Iteration 40896, Loss: 0.65559070\n",
      "Train Iteration 40928, Loss: 0.50312145\n",
      "Train Iteration 40960, Loss: 0.31828417\n",
      "Train Iteration 40992, Loss: 0.43515841\n",
      "Train Iteration 41024, Loss: 0.26100551\n",
      "Train Iteration 41056, Loss: 0.53815162\n",
      "Train Iteration 41088, Loss: 0.53981227\n",
      "Train Iteration 41120, Loss: 0.54857034\n",
      "Train Iteration 41152, Loss: 0.47256784\n",
      "Train Iteration 41184, Loss: 0.41735922\n",
      "Train Iteration 41216, Loss: 0.64693537\n",
      "Train Iteration 41248, Loss: 0.49711881\n",
      "Val Iteration 8128, Loss: 0.55705951\n",
      "Val Iteration 8160, Loss: 0.50589570\n",
      "Val Iteration 8192, Loss: 0.40288775\n",
      "Val Iteration 8224, Loss: 0.54678205\n",
      "\n",
      "Epoch: 66\n",
      "Train Iteration 41280, Loss: 0.44991385\n",
      "Train Iteration 41312, Loss: 0.43302980\n",
      "Train Iteration 41344, Loss: 0.74222197\n",
      "Train Iteration 41376, Loss: 0.52421794\n",
      "Train Iteration 41408, Loss: 0.55510218\n",
      "Train Iteration 41440, Loss: 0.44028742\n",
      "Train Iteration 41472, Loss: 0.49012944\n",
      "Train Iteration 41504, Loss: 0.67607134\n",
      "Train Iteration 41536, Loss: 0.35932130\n",
      "Train Iteration 41568, Loss: 0.47075937\n",
      "Train Iteration 41600, Loss: 0.42242120\n",
      "Train Iteration 41632, Loss: 0.55368237\n",
      "Train Iteration 41664, Loss: 0.38993111\n",
      "Train Iteration 41696, Loss: 0.69966086\n",
      "Train Iteration 41728, Loss: 0.47911042\n",
      "Train Iteration 41760, Loss: 0.47360396\n",
      "Train Iteration 41792, Loss: 0.57087056\n",
      "Train Iteration 41824, Loss: 0.46150539\n",
      "Train Iteration 41856, Loss: 0.60041588\n",
      "Val Iteration 8256, Loss: 0.43760895\n",
      "Val Iteration 8288, Loss: 0.52362318\n",
      "Val Iteration 8320, Loss: 0.46927190\n",
      "Val Iteration 8352, Loss: 0.43465709\n",
      "\n",
      "Epoch: 67\n",
      "Train Iteration 41888, Loss: 0.45255436\n",
      "Train Iteration 41920, Loss: 0.64838917\n",
      "Train Iteration 41952, Loss: 0.40789637\n",
      "Train Iteration 41984, Loss: 0.50768005\n",
      "Train Iteration 42016, Loss: 0.56162440\n",
      "Train Iteration 42048, Loss: 0.50069008\n",
      "Train Iteration 42080, Loss: 0.53672078\n",
      "Train Iteration 42112, Loss: 0.49182853\n",
      "Train Iteration 42144, Loss: 0.47096301\n",
      "Train Iteration 42176, Loss: 0.73641172\n",
      "Train Iteration 42208, Loss: 0.50274557\n",
      "Train Iteration 42240, Loss: 0.49791303\n",
      "Train Iteration 42272, Loss: 0.47632729\n",
      "Train Iteration 42304, Loss: 0.39379664\n",
      "Train Iteration 42336, Loss: 0.48183860\n",
      "Train Iteration 42368, Loss: 0.54421227\n",
      "Train Iteration 42400, Loss: 0.60879366\n",
      "Train Iteration 42432, Loss: 0.50841049\n",
      "Train Iteration 42464, Loss: 0.34107890\n",
      "Train Iteration 42496, Loss: 0.50891381\n",
      "Val Iteration 8384, Loss: 0.36178458\n",
      "Val Iteration 8416, Loss: 0.44274409\n",
      "Val Iteration 8448, Loss: 0.58922953\n",
      "Val Iteration 8480, Loss: 0.44321960\n",
      "\n",
      "Epoch: 68\n",
      "Train Iteration 42528, Loss: 0.48457431\n",
      "Train Iteration 42560, Loss: 0.48324644\n",
      "Train Iteration 42592, Loss: 0.63748926\n",
      "Train Iteration 42624, Loss: 0.66563122\n",
      "Train Iteration 42656, Loss: 0.48975575\n",
      "Train Iteration 42688, Loss: 0.45080529\n",
      "Train Iteration 42720, Loss: 0.63690466\n",
      "Train Iteration 42752, Loss: 0.48579992\n",
      "Train Iteration 42784, Loss: 0.51915617\n",
      "Train Iteration 42816, Loss: 0.51749873\n",
      "Train Iteration 42848, Loss: 0.46973232\n",
      "Train Iteration 42880, Loss: 0.52782088\n",
      "Train Iteration 42912, Loss: 0.63461952\n",
      "Train Iteration 42944, Loss: 0.55001884\n",
      "Train Iteration 42976, Loss: 0.48183160\n",
      "Train Iteration 43008, Loss: 0.50485736\n",
      "Train Iteration 43040, Loss: 0.55631111\n",
      "Train Iteration 43072, Loss: 0.62033191\n",
      "Train Iteration 43104, Loss: 0.45375468\n",
      "Val Iteration 8512, Loss: 0.53423766\n",
      "Val Iteration 8544, Loss: 0.52269285\n",
      "Val Iteration 8576, Loss: 0.51677539\n",
      "Val Iteration 8608, Loss: 0.45374144\n",
      "\n",
      "Epoch: 69\n",
      "Train Iteration 43136, Loss: 0.53824510\n",
      "Train Iteration 43168, Loss: 0.37157070\n",
      "Train Iteration 43200, Loss: 0.47211187\n",
      "Train Iteration 43232, Loss: 0.56697719\n",
      "Train Iteration 43264, Loss: 0.59387283\n",
      "Train Iteration 43296, Loss: 0.67886432\n",
      "Train Iteration 43328, Loss: 0.53854439\n",
      "Train Iteration 43360, Loss: 0.39011270\n",
      "Train Iteration 43392, Loss: 0.43460597\n",
      "Train Iteration 43424, Loss: 0.45641244\n",
      "Train Iteration 43456, Loss: 0.57942310\n",
      "Train Iteration 43488, Loss: 0.50843154\n",
      "Train Iteration 43520, Loss: 0.49515438\n",
      "Train Iteration 43552, Loss: 0.44796111\n",
      "Train Iteration 43584, Loss: 0.49364672\n",
      "Train Iteration 43616, Loss: 0.44369357\n",
      "Train Iteration 43648, Loss: 0.40192118\n",
      "Train Iteration 43680, Loss: 0.48208027\n",
      "Train Iteration 43712, Loss: 0.49572683\n",
      "Train Iteration 43744, Loss: 0.52846872\n",
      "Val Iteration 8640, Loss: 0.41063368\n",
      "Val Iteration 8672, Loss: 0.47412668\n",
      "Val Iteration 8704, Loss: 0.51470480\n",
      "Val Iteration 8736, Loss: 0.55541200\n",
      "\n",
      "Epoch: 70\n",
      "Train Iteration 43776, Loss: 0.51950628\n",
      "Train Iteration 43808, Loss: 0.53030798\n",
      "Train Iteration 43840, Loss: 0.50125236\n",
      "Train Iteration 43872, Loss: 0.52960067\n",
      "Train Iteration 43904, Loss: 0.84993477\n",
      "Train Iteration 43936, Loss: 0.49207301\n",
      "Train Iteration 43968, Loss: 0.44907621\n",
      "Train Iteration 44000, Loss: 0.49217679\n",
      "Train Iteration 44032, Loss: 0.50233468\n",
      "Train Iteration 44064, Loss: 0.55421355\n",
      "Train Iteration 44096, Loss: 0.48052990\n",
      "Train Iteration 44128, Loss: 0.48917353\n",
      "Train Iteration 44160, Loss: 0.40872399\n",
      "Train Iteration 44192, Loss: 0.49398312\n",
      "Train Iteration 44224, Loss: 0.51263496\n",
      "Train Iteration 44256, Loss: 0.49494026\n",
      "Train Iteration 44288, Loss: 0.63320365\n",
      "Train Iteration 44320, Loss: 0.48062046\n",
      "Train Iteration 44352, Loss: 0.59876693\n",
      "Val Iteration 8768, Loss: 0.56633834\n",
      "Val Iteration 8800, Loss: 0.52071316\n",
      "Val Iteration 8832, Loss: 0.54345406\n",
      "Val Iteration 8864, Loss: 0.42978556\n",
      "\n",
      "Epoch: 71\n",
      "Train Iteration 44384, Loss: 0.73009690\n",
      "Train Iteration 44416, Loss: 0.52502665\n",
      "Train Iteration 44448, Loss: 0.38401872\n",
      "Train Iteration 44480, Loss: 0.50928654\n",
      "Train Iteration 44512, Loss: 0.43598376\n",
      "Train Iteration 44544, Loss: 0.54020782\n",
      "Train Iteration 44576, Loss: 0.42999988\n",
      "Train Iteration 44608, Loss: 0.44043434\n",
      "Train Iteration 44640, Loss: 0.51897634\n",
      "Train Iteration 44672, Loss: 0.41504456\n",
      "Train Iteration 44704, Loss: 0.54170962\n",
      "Train Iteration 44736, Loss: 0.56678375\n",
      "Train Iteration 44768, Loss: 0.45675014\n",
      "Train Iteration 44800, Loss: 0.43527437\n",
      "Train Iteration 44832, Loss: 0.40983595\n",
      "Train Iteration 44864, Loss: 0.50181324\n",
      "Train Iteration 44896, Loss: 0.51461881\n",
      "Train Iteration 44928, Loss: 0.37331222\n",
      "Train Iteration 44960, Loss: 0.40527919\n",
      "Train Iteration 44992, Loss: 0.52689207\n",
      "Val Iteration 8896, Loss: 0.51315138\n",
      "Val Iteration 8928, Loss: 0.44933524\n",
      "Val Iteration 8960, Loss: 0.53012362\n",
      "Val Iteration 8992, Loss: 0.39303702\n",
      "\n",
      "Epoch: 72\n",
      "Train Iteration 45024, Loss: 0.53477667\n",
      "Train Iteration 45056, Loss: 0.50557275\n",
      "Train Iteration 45088, Loss: 0.71467247\n",
      "Train Iteration 45120, Loss: 0.42311007\n",
      "Train Iteration 45152, Loss: 0.48779083\n",
      "Train Iteration 45184, Loss: 0.52661420\n",
      "Train Iteration 45216, Loss: 0.57365162\n",
      "Train Iteration 45248, Loss: 0.73807057\n",
      "Train Iteration 45280, Loss: 0.40606767\n",
      "Train Iteration 45312, Loss: 0.45581074\n",
      "Train Iteration 45344, Loss: 0.46767172\n",
      "Train Iteration 45376, Loss: 0.57297700\n",
      "Train Iteration 45408, Loss: 0.41604443\n",
      "Train Iteration 45440, Loss: 0.47962096\n",
      "Train Iteration 45472, Loss: 0.59659972\n",
      "Train Iteration 45504, Loss: 0.45693749\n",
      "Train Iteration 45536, Loss: 0.47787904\n",
      "Train Iteration 45568, Loss: 0.55619111\n",
      "Train Iteration 45600, Loss: 0.65936154\n",
      "Val Iteration 9024, Loss: 0.56193581\n",
      "Val Iteration 9056, Loss: 0.44503504\n",
      "Val Iteration 9088, Loss: 0.44916112\n",
      "Val Iteration 9120, Loss: 0.51087825\n",
      "\n",
      "Epoch: 73\n",
      "Train Iteration 45632, Loss: 0.37254273\n",
      "Train Iteration 45664, Loss: 0.37223029\n",
      "Train Iteration 45696, Loss: 0.55443142\n",
      "Train Iteration 45728, Loss: 0.35649469\n",
      "Train Iteration 45760, Loss: 0.61878453\n",
      "Train Iteration 45792, Loss: 0.55823602\n",
      "Train Iteration 45824, Loss: 0.46734896\n",
      "Train Iteration 45856, Loss: 0.49330304\n",
      "Train Iteration 45888, Loss: 0.49919326\n",
      "Train Iteration 45920, Loss: 0.75792513\n",
      "Train Iteration 45952, Loss: 0.57315160\n",
      "Train Iteration 45984, Loss: 0.46225585\n",
      "Train Iteration 46016, Loss: 0.58909734\n",
      "Train Iteration 46048, Loss: 0.50395540\n",
      "Train Iteration 46080, Loss: 0.49973530\n",
      "Train Iteration 46112, Loss: 0.32833081\n",
      "Train Iteration 46144, Loss: 0.47660442\n",
      "Train Iteration 46176, Loss: 0.59773883\n",
      "Train Iteration 46208, Loss: 0.43825564\n",
      "Train Iteration 46240, Loss: 0.48773435\n",
      "Val Iteration 9152, Loss: 0.39416049\n",
      "Val Iteration 9184, Loss: 0.53197825\n",
      "Val Iteration 9216, Loss: 0.51408176\n",
      "Val Iteration 9248, Loss: 0.50603499\n",
      "\n",
      "Epoch: 74\n",
      "Train Iteration 46272, Loss: 0.61914296\n",
      "Train Iteration 46304, Loss: 0.61196317\n",
      "Train Iteration 46336, Loss: 0.53445791\n",
      "Train Iteration 46368, Loss: 0.43708040\n",
      "Train Iteration 46400, Loss: 0.51360973\n",
      "Train Iteration 46432, Loss: 0.58751455\n",
      "Train Iteration 46464, Loss: 0.50839861\n",
      "Train Iteration 46496, Loss: 0.43299274\n",
      "Train Iteration 46528, Loss: 0.56542750\n",
      "Train Iteration 46560, Loss: 0.58728752\n",
      "Train Iteration 46592, Loss: 0.50816634\n",
      "Train Iteration 46624, Loss: 0.50050588\n",
      "Train Iteration 46656, Loss: 0.63069024\n",
      "Train Iteration 46688, Loss: 0.31514618\n",
      "Train Iteration 46720, Loss: 0.33228764\n",
      "Train Iteration 46752, Loss: 0.59566109\n",
      "Train Iteration 46784, Loss: 0.47437028\n",
      "Train Iteration 46816, Loss: 0.37614516\n",
      "Train Iteration 46848, Loss: 0.49210733\n",
      "Val Iteration 9280, Loss: 0.53601010\n",
      "Val Iteration 9312, Loss: 0.42194857\n",
      "Val Iteration 9344, Loss: 0.51097714\n",
      "\n",
      "Epoch: 75\n",
      "Train Iteration 46880, Loss: 0.47844099\n",
      "Train Iteration 46912, Loss: 0.45059385\n",
      "Train Iteration 46944, Loss: 0.61566880\n",
      "Train Iteration 46976, Loss: 0.60896351\n",
      "Train Iteration 47008, Loss: 0.46731084\n",
      "Train Iteration 47040, Loss: 0.44010434\n",
      "Train Iteration 47072, Loss: 0.73398476\n",
      "Train Iteration 47104, Loss: 0.51186028\n",
      "Train Iteration 47136, Loss: 0.56805193\n",
      "Train Iteration 47168, Loss: 0.57731212\n",
      "Train Iteration 47200, Loss: 0.59528191\n",
      "Train Iteration 47232, Loss: 0.43469597\n",
      "Train Iteration 47264, Loss: 0.41504850\n",
      "Train Iteration 47296, Loss: 0.46618741\n",
      "Train Iteration 47328, Loss: 0.48893047\n",
      "Train Iteration 47360, Loss: 0.53238027\n",
      "Train Iteration 47392, Loss: 0.53355537\n",
      "Train Iteration 47424, Loss: 0.55397796\n",
      "Train Iteration 47456, Loss: 0.56863421\n",
      "Train Iteration 47488, Loss: 0.63898918\n",
      "Val Iteration 9376, Loss: 0.45301960\n",
      "Val Iteration 9408, Loss: 0.60917249\n",
      "Val Iteration 9440, Loss: 0.49587606\n",
      "Val Iteration 9472, Loss: 0.63701148\n",
      "\n",
      "Epoch: 76\n",
      "Train Iteration 47520, Loss: 0.56117836\n",
      "Train Iteration 47552, Loss: 0.57874237\n",
      "Train Iteration 47584, Loss: 0.53538743\n",
      "Train Iteration 47616, Loss: 0.49714691\n",
      "Train Iteration 47648, Loss: 0.54293283\n",
      "Train Iteration 47680, Loss: 0.55154612\n",
      "Train Iteration 47712, Loss: 0.47939802\n",
      "Train Iteration 47744, Loss: 0.54084230\n",
      "Train Iteration 47776, Loss: 0.44591561\n",
      "Train Iteration 47808, Loss: 0.46733311\n",
      "Train Iteration 47840, Loss: 0.65966682\n",
      "Train Iteration 47872, Loss: 0.41178162\n",
      "Train Iteration 47904, Loss: 0.49330110\n",
      "Train Iteration 47936, Loss: 0.41781821\n",
      "Train Iteration 47968, Loss: 0.61103512\n",
      "Train Iteration 48000, Loss: 0.56422249\n",
      "Train Iteration 48032, Loss: 0.59425233\n",
      "Train Iteration 48064, Loss: 0.55001222\n",
      "Train Iteration 48096, Loss: 0.59515347\n",
      "Val Iteration 9504, Loss: 0.38017128\n",
      "Val Iteration 9536, Loss: 0.40543548\n",
      "Val Iteration 9568, Loss: 0.54980758\n",
      "Val Iteration 9600, Loss: 0.48435049\n",
      "\n",
      "Epoch: 77\n",
      "Train Iteration 48128, Loss: 0.34699498\n",
      "Train Iteration 48160, Loss: 0.56847209\n",
      "Train Iteration 48192, Loss: 0.71202867\n",
      "Train Iteration 48224, Loss: 0.51019593\n",
      "Train Iteration 48256, Loss: 0.59231533\n",
      "Train Iteration 48288, Loss: 0.58701606\n",
      "Train Iteration 48320, Loss: 0.46133202\n",
      "Train Iteration 48352, Loss: 0.45723281\n",
      "Train Iteration 48384, Loss: 0.45960629\n",
      "Train Iteration 48416, Loss: 0.54162338\n",
      "Train Iteration 48448, Loss: 0.51567190\n",
      "Train Iteration 48480, Loss: 0.56465368\n",
      "Train Iteration 48512, Loss: 0.48172922\n",
      "Train Iteration 48544, Loss: 0.59381706\n",
      "Train Iteration 48576, Loss: 0.55411616\n",
      "Train Iteration 48608, Loss: 0.43520230\n",
      "Train Iteration 48640, Loss: 0.42318859\n",
      "Train Iteration 48672, Loss: 0.53114942\n",
      "Train Iteration 48704, Loss: 0.70182657\n",
      "Train Iteration 48736, Loss: 0.53687740\n",
      "Val Iteration 9632, Loss: 0.47251929\n",
      "Val Iteration 9664, Loss: 0.39816814\n",
      "Val Iteration 9696, Loss: 0.52219608\n",
      "Val Iteration 9728, Loss: 0.45110777\n",
      "\n",
      "Epoch: 78\n",
      "Train Iteration 48768, Loss: 0.58364156\n",
      "Train Iteration 48800, Loss: 0.46266116\n",
      "Train Iteration 48832, Loss: 0.48846087\n",
      "Train Iteration 48864, Loss: 0.49229258\n",
      "Train Iteration 48896, Loss: 0.37912366\n",
      "Train Iteration 48928, Loss: 0.61998771\n",
      "Train Iteration 48960, Loss: 0.62929778\n",
      "Train Iteration 48992, Loss: 0.45256725\n",
      "Train Iteration 49024, Loss: 0.46780395\n",
      "Train Iteration 49056, Loss: 0.38699220\n",
      "Train Iteration 49088, Loss: 0.45028345\n",
      "Train Iteration 49120, Loss: 0.48917838\n",
      "Train Iteration 49152, Loss: 0.56821060\n",
      "Train Iteration 49184, Loss: 0.62390682\n",
      "Train Iteration 49216, Loss: 0.45977480\n",
      "Train Iteration 49248, Loss: 0.69398480\n",
      "Train Iteration 49280, Loss: 0.55497719\n",
      "Train Iteration 49312, Loss: 0.47651666\n",
      "Train Iteration 49344, Loss: 0.40251255\n",
      "Val Iteration 9760, Loss: 0.36227197\n",
      "Val Iteration 9792, Loss: 0.71576220\n",
      "Val Iteration 9824, Loss: 0.58516415\n",
      "Val Iteration 9856, Loss: 0.47422931\n",
      "\n",
      "Epoch: 79\n",
      "Train Iteration 49376, Loss: 0.48075793\n",
      "Train Iteration 49408, Loss: 0.57585893\n",
      "Train Iteration 49440, Loss: 0.47789076\n",
      "Train Iteration 49472, Loss: 0.60955746\n",
      "Train Iteration 49504, Loss: 0.48597454\n",
      "Train Iteration 49536, Loss: 0.52158744\n",
      "Train Iteration 49568, Loss: 0.37603805\n",
      "Train Iteration 49600, Loss: 0.36871753\n",
      "Train Iteration 49632, Loss: 0.63011906\n",
      "Train Iteration 49664, Loss: 0.70778524\n",
      "Train Iteration 49696, Loss: 0.50476268\n",
      "Train Iteration 49728, Loss: 0.34238805\n",
      "Train Iteration 49760, Loss: 0.51947186\n",
      "Train Iteration 49792, Loss: 0.39429260\n",
      "Train Iteration 49824, Loss: 0.48174460\n",
      "Train Iteration 49856, Loss: 0.53846798\n",
      "Train Iteration 49888, Loss: 0.61328846\n",
      "Train Iteration 49920, Loss: 0.54215964\n",
      "Train Iteration 49952, Loss: 0.61765592\n",
      "Train Iteration 49984, Loss: 0.58086098\n",
      "Val Iteration 9888, Loss: 0.49868026\n",
      "Val Iteration 9920, Loss: 0.50443603\n",
      "Val Iteration 9952, Loss: 0.30632286\n",
      "Val Iteration 9984, Loss: 0.52771813\n",
      "\n",
      "Epoch: 80\n",
      "Train Iteration 50016, Loss: 0.45428136\n",
      "Train Iteration 50048, Loss: 0.43264017\n",
      "Train Iteration 50080, Loss: 0.66706242\n",
      "Train Iteration 50112, Loss: 0.44984860\n",
      "Train Iteration 50144, Loss: 0.44563713\n",
      "Train Iteration 50176, Loss: 0.51914485\n",
      "Train Iteration 50208, Loss: 0.46105609\n",
      "Train Iteration 50240, Loss: 0.49253395\n",
      "Train Iteration 50272, Loss: 0.45586255\n",
      "Train Iteration 50304, Loss: 0.51118979\n",
      "Train Iteration 50336, Loss: 0.46348414\n",
      "Train Iteration 50368, Loss: 0.36119301\n",
      "Train Iteration 50400, Loss: 0.67414394\n",
      "Train Iteration 50432, Loss: 0.61951351\n",
      "Train Iteration 50464, Loss: 0.56827993\n",
      "Train Iteration 50496, Loss: 0.71722508\n",
      "Train Iteration 50528, Loss: 0.49461617\n",
      "Train Iteration 50560, Loss: 0.54929807\n",
      "Train Iteration 50592, Loss: 0.73445198\n",
      "Train Iteration 50624, Loss: 0.47354021\n",
      "Val Iteration 10016, Loss: 0.46933327\n",
      "Val Iteration 10048, Loss: 0.43274530\n",
      "Val Iteration 10080, Loss: 0.51300529\n",
      "Val Iteration 10112, Loss: 0.46499663\n",
      "\n",
      "Epoch: 81\n",
      "Train Iteration 50656, Loss: 0.48447561\n",
      "Train Iteration 50688, Loss: 0.51608440\n",
      "Train Iteration 50720, Loss: 0.59771696\n",
      "Train Iteration 50752, Loss: 0.34595458\n",
      "Train Iteration 50784, Loss: 0.65552964\n",
      "Train Iteration 50816, Loss: 0.48231544\n",
      "Train Iteration 50848, Loss: 0.62603882\n",
      "Train Iteration 50880, Loss: 0.49132670\n",
      "Train Iteration 50912, Loss: 0.60010156\n",
      "Train Iteration 50944, Loss: 0.60217010\n",
      "Train Iteration 50976, Loss: 0.49856927\n",
      "Train Iteration 51008, Loss: 0.45997362\n",
      "Train Iteration 51040, Loss: 0.38864857\n",
      "Train Iteration 51072, Loss: 0.61786301\n",
      "Train Iteration 51104, Loss: 0.54295465\n",
      "Train Iteration 51136, Loss: 0.38168514\n",
      "Train Iteration 51168, Loss: 0.57318908\n",
      "Train Iteration 51200, Loss: 0.55758850\n",
      "Train Iteration 51232, Loss: 0.53470625\n",
      "Val Iteration 10144, Loss: 0.42374955\n",
      "Val Iteration 10176, Loss: 0.46888256\n",
      "Val Iteration 10208, Loss: 0.58582654\n",
      "Val Iteration 10240, Loss: 0.58549576\n",
      "\n",
      "Epoch: 82\n",
      "Train Iteration 51264, Loss: 0.55487427\n",
      "Train Iteration 51296, Loss: 0.51870448\n",
      "Train Iteration 51328, Loss: 0.54171145\n",
      "Train Iteration 51360, Loss: 0.51697671\n",
      "Train Iteration 51392, Loss: 0.48544706\n",
      "Train Iteration 51424, Loss: 0.53022117\n",
      "Train Iteration 51456, Loss: 0.44661876\n",
      "Train Iteration 51488, Loss: 0.42475150\n",
      "Train Iteration 51520, Loss: 0.51342031\n",
      "Train Iteration 51552, Loss: 0.40306562\n",
      "Train Iteration 51584, Loss: 0.46249473\n",
      "Train Iteration 51616, Loss: 0.61509859\n",
      "Train Iteration 51648, Loss: 0.53168196\n",
      "Train Iteration 51680, Loss: 0.66907064\n",
      "Train Iteration 51712, Loss: 0.48760629\n",
      "Train Iteration 51744, Loss: 0.44603272\n",
      "Train Iteration 51776, Loss: 0.47328236\n",
      "Train Iteration 51808, Loss: 0.49877897\n",
      "Train Iteration 51840, Loss: 0.47899485\n",
      "Train Iteration 51872, Loss: 0.50440750\n",
      "Val Iteration 10272, Loss: 0.54971982\n",
      "Val Iteration 10304, Loss: 0.50571815\n",
      "Val Iteration 10336, Loss: 0.45154946\n",
      "Val Iteration 10368, Loss: 0.44602488\n",
      "\n",
      "Epoch: 83\n",
      "Train Iteration 51904, Loss: 0.48740589\n",
      "Train Iteration 51936, Loss: 0.50706492\n",
      "Train Iteration 51968, Loss: 0.60831156\n",
      "Train Iteration 52000, Loss: 0.58454168\n",
      "Train Iteration 52032, Loss: 0.51580037\n",
      "Train Iteration 52064, Loss: 0.56949496\n",
      "Train Iteration 52096, Loss: 0.72816384\n",
      "Train Iteration 52128, Loss: 0.43767169\n",
      "Train Iteration 52160, Loss: 0.43388149\n",
      "Train Iteration 52192, Loss: 0.54762358\n",
      "Train Iteration 52224, Loss: 0.47474668\n",
      "Train Iteration 52256, Loss: 0.48408607\n",
      "Train Iteration 52288, Loss: 0.54623038\n",
      "Train Iteration 52320, Loss: 0.51098439\n",
      "Train Iteration 52352, Loss: 0.52629630\n",
      "Train Iteration 52384, Loss: 0.43920116\n",
      "Train Iteration 52416, Loss: 0.44725328\n",
      "Train Iteration 52448, Loss: 0.44544811\n",
      "Train Iteration 52480, Loss: 0.51113462\n",
      "Val Iteration 10400, Loss: 0.52172911\n",
      "Val Iteration 10432, Loss: 0.41974726\n",
      "Val Iteration 10464, Loss: 0.43120386\n",
      "Val Iteration 10496, Loss: 0.50335574\n",
      "\n",
      "Epoch: 84\n",
      "Train Iteration 52512, Loss: 0.54538153\n",
      "Train Iteration 52544, Loss: 0.55277196\n",
      "Train Iteration 52576, Loss: 0.37371282\n",
      "Train Iteration 52608, Loss: 0.37195575\n",
      "Train Iteration 52640, Loss: 0.35815396\n",
      "Train Iteration 52672, Loss: 0.55221057\n",
      "Train Iteration 52704, Loss: 0.46148540\n",
      "Train Iteration 52736, Loss: 0.67511070\n",
      "Train Iteration 52768, Loss: 0.52285114\n",
      "Train Iteration 52800, Loss: 0.52177006\n",
      "Train Iteration 52832, Loss: 0.59895804\n",
      "Train Iteration 52864, Loss: 0.56011115\n",
      "Train Iteration 52896, Loss: 0.55933564\n",
      "Train Iteration 52928, Loss: 0.49002750\n",
      "Train Iteration 52960, Loss: 0.53901118\n",
      "Train Iteration 52992, Loss: 0.59203375\n",
      "Train Iteration 53024, Loss: 0.46146125\n",
      "Train Iteration 53056, Loss: 0.64452964\n",
      "Train Iteration 53088, Loss: 0.52011651\n",
      "Train Iteration 53120, Loss: 0.47035201\n",
      "Val Iteration 10528, Loss: 0.44889633\n",
      "Val Iteration 10560, Loss: 0.40557922\n",
      "Val Iteration 10592, Loss: 0.40383469\n",
      "Val Iteration 10624, Loss: 0.59976655\n",
      "\n",
      "Epoch: 85\n",
      "Train Iteration 53152, Loss: 0.45507799\n",
      "Train Iteration 53184, Loss: 0.58842073\n",
      "Train Iteration 53216, Loss: 0.45594191\n",
      "Train Iteration 53248, Loss: 0.58269452\n",
      "Train Iteration 53280, Loss: 0.50946851\n",
      "Train Iteration 53312, Loss: 0.59135603\n",
      "Train Iteration 53344, Loss: 0.58258531\n",
      "Train Iteration 53376, Loss: 0.67912325\n",
      "Train Iteration 53408, Loss: 0.42161192\n",
      "Train Iteration 53440, Loss: 0.60821563\n",
      "Train Iteration 53472, Loss: 0.52705382\n",
      "Train Iteration 53504, Loss: 0.48075597\n",
      "Train Iteration 53536, Loss: 0.53980260\n",
      "Train Iteration 53568, Loss: 0.40428089\n",
      "Train Iteration 53600, Loss: 0.34395676\n",
      "Train Iteration 53632, Loss: 0.60979872\n",
      "Train Iteration 53664, Loss: 0.49730179\n",
      "Train Iteration 53696, Loss: 0.72363639\n",
      "Train Iteration 53728, Loss: 0.49816077\n",
      "Val Iteration 10656, Loss: 0.40158362\n",
      "Val Iteration 10688, Loss: 0.59898131\n",
      "Val Iteration 10720, Loss: 0.58138195\n",
      "\n",
      "Epoch: 86\n",
      "Train Iteration 53760, Loss: 0.48121062\n",
      "Train Iteration 53792, Loss: 0.49850821\n",
      "Train Iteration 53824, Loss: 0.50610575\n",
      "Train Iteration 53856, Loss: 0.48506869\n",
      "Train Iteration 53888, Loss: 0.53153613\n",
      "Train Iteration 53920, Loss: 0.41870643\n",
      "Train Iteration 53952, Loss: 0.42185559\n",
      "Train Iteration 53984, Loss: 0.49188610\n",
      "Train Iteration 54016, Loss: 0.66392801\n",
      "Train Iteration 54048, Loss: 0.81277552\n",
      "Train Iteration 54080, Loss: 0.57609468\n",
      "Train Iteration 54112, Loss: 0.41501501\n",
      "Train Iteration 54144, Loss: 0.70010460\n",
      "Train Iteration 54176, Loss: 0.39010049\n",
      "Train Iteration 54208, Loss: 0.40549275\n",
      "Train Iteration 54240, Loss: 0.47942854\n",
      "Train Iteration 54272, Loss: 0.57803968\n",
      "Train Iteration 54304, Loss: 0.48007654\n",
      "Train Iteration 54336, Loss: 0.53570323\n",
      "Train Iteration 54368, Loss: 0.59510024\n",
      "Val Iteration 10752, Loss: 0.43902107\n",
      "Val Iteration 10784, Loss: 0.51406271\n",
      "Val Iteration 10816, Loss: 0.39874628\n",
      "Val Iteration 10848, Loss: 0.45613340\n",
      "\n",
      "Epoch: 87\n",
      "Train Iteration 54400, Loss: 0.53789344\n",
      "Train Iteration 54432, Loss: 0.57174949\n",
      "Train Iteration 54464, Loss: 0.53033548\n",
      "Train Iteration 54496, Loss: 0.38535514\n",
      "Train Iteration 54528, Loss: 0.51306330\n",
      "Train Iteration 54560, Loss: 0.42705104\n",
      "Train Iteration 54592, Loss: 0.62164697\n",
      "Train Iteration 54624, Loss: 0.37078627\n",
      "Train Iteration 54656, Loss: 0.43498447\n",
      "Train Iteration 54688, Loss: 0.58072948\n",
      "Train Iteration 54720, Loss: 0.63779785\n",
      "Train Iteration 54752, Loss: 0.48327417\n",
      "Train Iteration 54784, Loss: 0.44776718\n",
      "Train Iteration 54816, Loss: 0.65493417\n",
      "Train Iteration 54848, Loss: 0.79150287\n",
      "Train Iteration 54880, Loss: 0.54134815\n",
      "Train Iteration 54912, Loss: 0.45949835\n",
      "Train Iteration 54944, Loss: 0.60155827\n",
      "Train Iteration 54976, Loss: 0.67744240\n",
      "Val Iteration 10880, Loss: 0.46307151\n",
      "Val Iteration 10912, Loss: 0.51367944\n",
      "Val Iteration 10944, Loss: 0.55344892\n",
      "Val Iteration 10976, Loss: 0.44719704\n",
      "\n",
      "Epoch: 88\n",
      "Train Iteration 55008, Loss: 0.49171391\n",
      "Train Iteration 55040, Loss: 0.53454383\n",
      "Train Iteration 55072, Loss: 0.53931308\n",
      "Train Iteration 55104, Loss: 0.52220046\n",
      "Train Iteration 55136, Loss: 0.44724500\n",
      "Train Iteration 55168, Loss: 0.40755175\n",
      "Train Iteration 55200, Loss: 0.40840731\n",
      "Train Iteration 55232, Loss: 0.40902178\n",
      "Train Iteration 55264, Loss: 0.50174051\n",
      "Train Iteration 55296, Loss: 0.39789207\n",
      "Train Iteration 55328, Loss: 0.37474908\n",
      "Train Iteration 55360, Loss: 0.59562754\n",
      "Train Iteration 55392, Loss: 0.51268718\n",
      "Train Iteration 55424, Loss: 0.61796410\n",
      "Train Iteration 55456, Loss: 0.73317388\n",
      "Train Iteration 55488, Loss: 0.41492858\n",
      "Train Iteration 55520, Loss: 0.58525490\n",
      "Train Iteration 55552, Loss: 0.46343618\n",
      "Train Iteration 55584, Loss: 0.55347103\n",
      "Train Iteration 55616, Loss: 0.64091901\n",
      "Val Iteration 11008, Loss: 0.60416462\n",
      "Val Iteration 11040, Loss: 0.40869078\n",
      "Val Iteration 11072, Loss: 0.38525507\n",
      "Val Iteration 11104, Loss: 0.55105404\n",
      "\n",
      "Epoch: 89\n",
      "Train Iteration 55648, Loss: 0.31208115\n",
      "Train Iteration 55680, Loss: 0.53650495\n",
      "Train Iteration 55712, Loss: 0.39765013\n",
      "Train Iteration 55744, Loss: 0.35974856\n",
      "Train Iteration 55776, Loss: 0.37180097\n",
      "Train Iteration 55808, Loss: 0.57328704\n",
      "Train Iteration 55840, Loss: 0.58499272\n",
      "Train Iteration 55872, Loss: 0.53081606\n",
      "Train Iteration 55904, Loss: 0.44679681\n",
      "Train Iteration 55936, Loss: 0.65759904\n",
      "Train Iteration 55968, Loss: 0.42827438\n",
      "Train Iteration 56000, Loss: 0.54066468\n",
      "Train Iteration 56032, Loss: 0.51916440\n",
      "Train Iteration 56064, Loss: 0.49269873\n",
      "Train Iteration 56096, Loss: 0.47293193\n",
      "Train Iteration 56128, Loss: 0.47384186\n",
      "Train Iteration 56160, Loss: 0.43224333\n",
      "Train Iteration 56192, Loss: 0.37039716\n",
      "Train Iteration 56224, Loss: 0.56117600\n",
      "Val Iteration 11136, Loss: 0.37927147\n",
      "Val Iteration 11168, Loss: 0.68237370\n",
      "Val Iteration 11200, Loss: 0.49402554\n",
      "Val Iteration 11232, Loss: 0.52595654\n",
      "\n",
      "Epoch: 90\n",
      "Train Iteration 56256, Loss: 0.54337224\n",
      "Train Iteration 56288, Loss: 0.41119403\n",
      "Train Iteration 56320, Loss: 0.55105162\n",
      "Train Iteration 56352, Loss: 0.59499080\n",
      "Train Iteration 56384, Loss: 0.57203128\n",
      "Train Iteration 56416, Loss: 0.48737234\n",
      "Train Iteration 56448, Loss: 0.60176009\n",
      "Train Iteration 56480, Loss: 0.52453620\n",
      "Train Iteration 56512, Loss: 0.41129720\n",
      "Train Iteration 56544, Loss: 0.55188023\n",
      "Train Iteration 56576, Loss: 0.61076048\n",
      "Train Iteration 56608, Loss: 0.66038406\n",
      "Train Iteration 56640, Loss: 0.59323958\n",
      "Train Iteration 56672, Loss: 0.71345646\n",
      "Train Iteration 56704, Loss: 0.45207920\n",
      "Train Iteration 56736, Loss: 0.58926839\n",
      "Train Iteration 56768, Loss: 0.54707088\n",
      "Train Iteration 56800, Loss: 0.60182110\n",
      "Train Iteration 56832, Loss: 0.54766492\n",
      "Train Iteration 56864, Loss: 0.34990710\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#put your code for model implementation and training here\n",
    "############################################################\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.special import erf\n",
    "\n",
    "\n",
    "# ---------- Helper fn ------------\n",
    "def patchify_1d(x, p1):\n",
    "    \"\"\"\n",
    "    L: sequence length (42 for UNSW-NB15 dataset)\n",
    "    x: np.ndarray of shape (B, L, C)\n",
    "    p1: patch size (must divide L)\n",
    "    returns: np.ndarray of shape (B, L//p1, C*p1)\n",
    "    \"\"\"\n",
    "    B, L, C = x.shape\n",
    "    assert L % p1 == 0, f\"Sequence length {L} not divisible by patch size {p1}\"\n",
    "    n = L // p1\n",
    "\n",
    "    x_patches = x.reshape(B, n, p1, C)\n",
    "    return x_patches.reshape(B, n, p1 * C)\n",
    "\n",
    "\n",
    "# ---------- Init weight & bias with Kaiming uniform distribution ----------\n",
    "def calculate_gain(nonlinearity, param=None):\n",
    "    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L72\n",
    "    if nonlinearity in ['linear', 'conv1d', 'conv2d', 'conv3d',\n",
    "                        'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'sigmoid']:\n",
    "        return 1.0\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        negative_slope = 0.01 if param is None else param\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n",
    "\n",
    "\n",
    "def _calculate_correct_fan(tensor, mode):\n",
    "    # Inspired from _calculate_fan_in_and_fan_out: https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L345\n",
    "    # Note that pytorch uses _calculate_correct_fan as API for _calculate_fan_in_and_fan_out so I merely remove the API\n",
    "    if tensor.ndim < 2:\n",
    "        raise ValueError(\"Fan in and fan out cannot be computed for tensor with fewer than 2 dimensions\")\n",
    "    if mode == \"fan_in\":\n",
    "        num_input_fmaps = tensor.shape[1]\n",
    "        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n",
    "        return num_input_fmaps * receptive_field_size\n",
    "    elif mode == \"fan_out\":\n",
    "        num_output_fmaps = tensor.shape[0]\n",
    "        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n",
    "        return num_output_fmaps * receptive_field_size\n",
    "    else:\n",
    "        raise ValueError(\"mode must be either 'fan_in' or 'fan_out'\")\n",
    "\n",
    "\n",
    "def kaiming_uniform_(tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"):\n",
    "    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L456\n",
    "    fan = _calculate_correct_fan(tensor, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    bound = math.sqrt(3.0) * std\n",
    "    tensor[:] = np.random.uniform(-bound, bound, tensor.shape).astype(np.float32)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# ---------- Layers ----------\n",
    "class Linear:\n",
    "    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py#L50\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = np.empty((out_features, in_features), dtype=np.float32)\n",
    "        self.bias = np.empty((out_features,), dtype=np.float32) if bias else None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        kaiming_uniform_(self.weight, a=math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "        if self.bias is not None:\n",
    "            fan_in = _calculate_correct_fan(self.weight, \"fan_in\")\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            self.bias[:] = np.random.uniform(-bound, bound, self.bias.shape).astype(np.float32)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input (batch_size, N, in_features)\n",
    "\n",
    "            Returns np.ndarray (batch_size, N, out_features)\n",
    "        \"\"\"\n",
    "\n",
    "        #output = np.einsum('bni,oi->bno', input, self.weight)\n",
    "        #if self.bias is not None:\n",
    "        #    output += self.bias.reshape(1, 1, -1)\n",
    "        #return output\n",
    "\n",
    "        # Original implementation # Observed no difference between einsum and original implementation\n",
    "        return np.matmul(np.ascontiguousarray(input), self.weight.T) + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "    def backward(self, grad_output, input):\n",
    "        \"\"\"\n",
    "            grad_output (np.ndarray): Gradient w.r.t. output with shape (B, N, out_features)\n",
    "            input (np.ndarray): Original input with shape (B, N, in_features)\n",
    "\n",
    "            Returns:\n",
    "                grad_input (np.ndarray): Gradient w.r.t. input with shape (B, N, in_features)\n",
    "                grad_weight (np.ndarray): Gradient w.r.t. weight with shape (out_features, in_features)\n",
    "                grad_bias (np.ndarray or None): Gradient w.r.t. bias with shape (out_features,)\n",
    "        \"\"\"\n",
    "        grad_input = np.matmul(grad_output, self.weight)\n",
    "        if input.ndim == 2:\n",
    "            self.grad_weight = np.einsum('bi,bj->ij', grad_output, input)\n",
    "            self.grad_bias = np.sum(grad_output, axis=0) if self.bias is not None else None\n",
    "        elif input.ndim == 3:\n",
    "            self.grad_weight = np.einsum('bnk,bnl->kl', grad_output, input)\n",
    "            self.grad_bias = np.sum(grad_output, axis=(0, 1))\n",
    "        # store self.grad_weight and self.grad_bias for optimiser.step later on\n",
    "        return grad_input, self.grad_weight, self.grad_bias\n",
    "\n",
    "\n",
    "eps = 1e-5\n",
    "class LayerNorm:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.weight = np.ones((dim,), dtype=np.float32)\n",
    "        self.bias = np.zeros((dim,), dtype=np.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, embed_dim)\n",
    "        Returns:\n",
    "            out: layer-normalized activations, shape (B, T, embed_dim)\n",
    "            cache: tuple of (x, self.w, mean, rstd) for backward pass.\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        mean = np.sum(x, axis=-1, keepdims=True) / C           # (B, T, 1)\n",
    "        xshift = x - mean\n",
    "        var = np.sum(xshift ** 2, axis=-1, keepdims=True) / C  # (B, T, 1)\n",
    "        rstd = (var + eps) ** (-0.5)                           # (B, T, 1)\n",
    "        norm = xshift * rstd\n",
    "        out = norm * self.weight + self.bias\n",
    "        cache = (x, self.weight, mean, rstd)\n",
    "        return out, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"\n",
    "        dout: upstream gradients, shape (B, N, embed_dim)\n",
    "        cache: tuple of (x, w, mean, rstd) from forward pass.\n",
    "        Returns:\n",
    "            dx: gradient with respect to x, shape (B, T, embed_dim)\n",
    "            dw: gradient with respect to self.w, shape (embed_dim,)\n",
    "            db: gradient with respect to self.b, shape (embed_dim,)\n",
    "        \"\"\"\n",
    "        x, w, mean, rstd = cache\n",
    "        norm = (x - mean) * rstd\n",
    "        db = np.sum(dout, axis=(0, 1))\n",
    "        dw = np.sum(dout * norm, axis=(0, 1))\n",
    "        dnorm = dout * w\n",
    "        dnorm_mean = np.mean(dnorm, axis=-1, keepdims=True)\n",
    "        dx = dnorm - dnorm_mean - norm * np.mean(dnorm * norm, axis=-1, keepdims=True)\n",
    "        dx *= rstd\n",
    "        self.grad_weight = dw\n",
    "        self.grad_bias = db\n",
    "        return dx, dw, db\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    # Inspired from https://gist.github.com/nbertagnolli/35eb960d08c566523b4da599f6099b41\n",
    "    # and https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L35\n",
    "    def __init__(self, p=0.5):\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"p must be a probability in [0, 1].\")\n",
    "        self.p = p\n",
    "        self.training = True\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of any shape.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Output array with dropout applied.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Only generate a new mask if one hasn't been set externally.\n",
    "            if self.mask is None or self.mask.shape != x.shape:\n",
    "                self.mask = (np.random.rand(*x.shape) >= self.p).astype(x.dtype)\n",
    "            return (x * self.mask) / (1 - self.p)\n",
    "        else:\n",
    "            self.mask = None\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.training:\n",
    "            if self.mask is None:\n",
    "                raise ValueError(\"Must run forward pass before backward pass in training mode.\")\n",
    "            return (grad_output * self.mask) / (1 - self.p)\n",
    "        else:\n",
    "            return grad_output\n",
    "\n",
    "\n",
    "class Reduce_np:\n",
    "    # Practically the same as AveragePooling layer along specified axis\n",
    "    def __init__(self, axis, reduction='mean'):\n",
    "        self.axis = axis\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input = x.copy()\n",
    "        if self.reduction == 'mean':\n",
    "            return np.mean(x, axis=self.axis)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        shape = self.last_input.shape                         # (B, N, dim)\n",
    "        scale = shape[self.axis]                              # axis = N in MLP_mixer\n",
    "        grad_input = grad_output / scale\n",
    "        grad_input = grad_input.reshape(grad_input.shape[0], 1, grad_input.shape[1])  # Reshape to (B, 1, dim) to broadcast along N dim\n",
    "        return np.broadcast_to(grad_input, shape)             # broadcasts to (B, N, dim)\n",
    "\n",
    "\n",
    "# ---------- Activation Function ----------\n",
    "class GELU:\n",
    "    # Inspired from https://github.com/oniani/ai/blob/main/activation/gelu.py#L4\n",
    "    def __init__(self):\n",
    "        self.ctx = {}\n",
    "\n",
    "    def forward(self, input):\n",
    "        cdf = 0.5 * (1 + erf(input / np.sqrt(2)))\n",
    "        self.ctx['input'] = input.copy()\n",
    "        self.ctx['cdf'] = cdf.copy()\n",
    "        return input * cdf\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input = self.ctx['input']\n",
    "        cdf = self.ctx['cdf']\n",
    "        pdf_val = 1 / np.sqrt(2 * np.pi) * np.exp(-0.5 * np.power(input, 2))\n",
    "        grad_local = cdf + input * pdf_val\n",
    "\n",
    "        return grad_output * grad_local\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.ctx = {}\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = np.tanh(input)\n",
    "        self.ctx['output'] = output.copy()\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        output = self.ctx['output']\n",
    "        grad_input = grad_output * (1 - output**2)\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# ---------- Blocks ----------\n",
    "class PreNormResidual:\n",
    "    # Inspired from https://github.com/lucidrains/mlp-mixer-pytorch/blob/main/mlp_mixer_pytorch/mlp_mixer_pytorch.py#L7\n",
    "    \"\"\"\n",
    "    PreNorm + residual connection with stochastic depth (DropPath).\n",
    "    Each forward pass, with probability drop_prob the residual branch is skipped.\n",
    "    When kept, its output is scaled by 1/(1 - drop_prob) to preserve expectation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, fn, drop_prob=0.0):\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        self.drop_prob = drop_prob\n",
    "        self.training = True\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LayerNorm\n",
    "        norm_x, norm_cache = self.norm.forward(x)\n",
    "        # Apply function\n",
    "        f_out = self.fn.forward(norm_x)\n",
    "        # Decide whether to drop the branch\n",
    "        if self.training and np.random.rand() < self.drop_prob:\n",
    "            # dropped: skip branch\n",
    "            out = x\n",
    "            keep = False\n",
    "        else:\n",
    "            # kept: scale to preserve expected value\n",
    "            out = x + f_out / (1 - self.drop_prob)\n",
    "            keep = True\n",
    "        # store caches for backward, including f_out for skip-case shape\n",
    "        self.cache = {\n",
    "            'x': x,\n",
    "            'norm_cache': norm_cache,\n",
    "            'f_out': f_out,\n",
    "            'fn_cache': getattr(self.fn, 'cache', None),\n",
    "            'keep': keep\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        keep = self.cache['keep']\n",
    "        x = self.cache['x']\n",
    "        norm_cache = self.cache['norm_cache']\n",
    "\n",
    "        if not keep:\n",
    "            # branch was dropped: set gradients in norm and fn to zero\n",
    "            # zero gradient flowing into residual branch\n",
    "            f_out = self.cache['f_out']\n",
    "            # backprop zeros through fn to produce zero grads on its parameters\n",
    "            d_zero = np.zeros_like(f_out)\n",
    "            d_fn, f_params_grad = self.fn.backward(d_zero)\n",
    "            # backprop zeros through norm\n",
    "            # norm.backward expects (dout, cache) -> (dx, dw, db)\n",
    "            d_norm, norm_dw, norm_db = self.norm.backward(np.zeros_like(d_fn), norm_cache)\n",
    "            # gradient flows only through the skip connection\n",
    "            d_x = d_out\n",
    "        else:\n",
    "            # branch kept: backprop through scaled residual\n",
    "            # merge grad for fn\n",
    "            d_res = d_out / (1 - self.drop_prob)\n",
    "            # backward through fn\n",
    "            d_f, f_params_grad = self.fn.backward(d_res)\n",
    "            # backward through norm\n",
    "            d_norm, norm_dw, norm_db = self.norm.backward(d_f, norm_cache)\n",
    "            # total gradient to x\n",
    "            d_x = d_norm + d_out\n",
    "        # store norm grads for optimiser\n",
    "        self.norm.grad_weight = norm_dw\n",
    "        self.norm.grad_bias = norm_db\n",
    "        return d_x, norm_dw, norm_db, f_params_grad\n",
    "\n",
    "\n",
    "# TokenMixing operates along N dimension\n",
    "class TokenMixing:\n",
    "    def __init__(self, num_tokens, expansion_factor, dropout_p):\n",
    "        inner_dim = int(num_tokens * expansion_factor)\n",
    "        self.linear1 = Linear(num_tokens, inner_dim)\n",
    "        self.activation_fn = GELU()\n",
    "        # self.gelu = GELU()\n",
    "        self.dropout1 = Dropout(dropout_p)\n",
    "        self.linear2 = Linear(inner_dim, num_tokens)\n",
    "        self.dropout2 = Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x_t = x transposed\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        x_t = np.transpose(x, (0, 2, 1))\n",
    "        cache['x_t'] = x_t.copy()\n",
    "        out_l1 = self.linear1.forward(x_t)\n",
    "        cache['out_l1'] = out_l1.copy()\n",
    "        #out_gelu = self.gelu.forward(out_l1)\n",
    "        out_activation_fn = self.activation_fn.forward(out_l1)\n",
    "        # cache['out_gelu'] = out_gelu.copy()\n",
    "        out_drop1 = self.dropout1.forward(out_activation_fn)\n",
    "        cache['out_drop1'] = out_drop1.copy()\n",
    "        out_l2 = self.linear2.forward(out_drop1)\n",
    "        cache['out_l2'] = out_l2.copy()\n",
    "        out_drop2 = self.dropout2.forward(out_l2)\n",
    "        cache['out_drop2'] = out_drop2.copy()\n",
    "        out = np.transpose(out_drop2, (0, 2, 1))\n",
    "        self.cache = cache\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        cache = self.cache\n",
    "        d_drop2 = np.transpose(d_out, (0, 2, 1))\n",
    "        d_l2 = self.dropout2.backward(d_drop2)\n",
    "        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_l2, cache['out_drop1'])\n",
    "        # d_gelu = self.dropout1.backward(d_drop1)\n",
    "        d_activation_fn = self.dropout1.backward(d_drop1)\n",
    "        # d_l1 = self.gelu.backward(d_gelu)\n",
    "        d_x_t, grad_w1, grad_b1 = self.linear1.backward(self.activation_fn.backward(d_activation_fn), cache['x_t'])\n",
    "        d_x = np.transpose(d_x_t, (0, 2, 1))\n",
    "        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n",
    "\n",
    "        # Reset cache to prevent memory leak\n",
    "        cache = {}\n",
    "        return d_x, self.params_grad\n",
    "\n",
    "\n",
    "# ChannelMixing operates along embedding dimension\n",
    "class ChannelMixing:\n",
    "    def __init__(self, dim, expansion_factor, dropout_p):\n",
    "        inner_dim = int(dim * expansion_factor)\n",
    "        self.linear1 = Linear(dim, inner_dim)\n",
    "        # self.gelu = GELU()\n",
    "        self.activation_fn = GELU()\n",
    "        self.dropout1 = Dropout(dropout_p)\n",
    "        self.linear2 = Linear(inner_dim, dim)\n",
    "        self.dropout2 = Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cache = {}\n",
    "        cache['x'] = x.copy()\n",
    "        out_l1 = self.linear1.forward(x)\n",
    "        cache['out_l1'] = out_l1.copy()\n",
    "        # out_gelu = self.gelu.forward(out_l1)\n",
    "        out_activation_fn = self.activation_fn.forward(out_l1)\n",
    "        # cache['out_gelu'] = out_gelu.copy()\n",
    "        out_drop1 = self.dropout1.forward(out_activation_fn)\n",
    "        cache['out_drop1'] = out_drop1.copy()\n",
    "        out_l2 = self.linear2.forward(out_drop1)\n",
    "        cache['out_l2'] = out_l2.copy()\n",
    "        out_drop2 = self.dropout2.forward(out_l2)\n",
    "        #cache['out_drop2'] = out_drop2.copy()\n",
    "        self.cache = cache\n",
    "        return out_drop2\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        cache = self.cache\n",
    "        d_drop2 = self.dropout2.backward(d_out)\n",
    "        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_drop2, cache['out_drop1'])\n",
    "        d_activation_fn = self.dropout1.backward(d_drop1)\n",
    "        # d_gelu = self.dropout1.backward(d_drop1)\n",
    "        d_l1, grad_w1, grad_b1 = self.linear1.backward(self.activation_fn.backward(d_activation_fn), cache['x'])\n",
    "        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n",
    "\n",
    "        # Reset cache to prevent memory leak\n",
    "        cache = {}\n",
    "        return d_l1, self.params_grad\n",
    "\n",
    "\n",
    "# ---------- Model ----------\n",
    "class MLPMixer:\n",
    "    def __init__(self, seq_len, channels, dim, depth, num_classes, patch_size, \n",
    "                 expansion_factor=4, expansion_factor_token=0.5, dropout_p=0., stochastic_depth_p=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_tokens (int): Number of tokens (N).\n",
    "            channels (int): Input channel (1 for UNSW-NB15 dataset)\n",
    "            dim (int): Mixer embedding dimension.\n",
    "            depth (int): Number of mixer layers.\n",
    "            num_classes (int): Number of classes for classification.\n",
    "            expansion_factor (float): Expansion factor for token mixing.\n",
    "            expansion_factor_token (float): Expansion factor for channel mixing.\n",
    "            dropout_p (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        assert seq_len % patch_size == 0\n",
    "        self.num_tokens = seq_len // patch_size\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_p = dropout_p\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = Linear(channels*patch_size, dim)\n",
    "        self.mixer_layers = []\n",
    "        for i in range(depth):\n",
    "            drop_prob = stochastic_depth_p * (i / max(depth - 1, 1))\n",
    "            token_mixing = PreNormResidual(dim, fn=TokenMixing(self.num_tokens, expansion_factor, dropout_p), drop_prob=drop_prob)\n",
    "            channel_mixing = PreNormResidual(dim, fn=ChannelMixing(dim, expansion_factor_token, dropout_p), drop_prob=drop_prob)\n",
    "            self.mixer_layers.append((token_mixing, channel_mixing))\n",
    "        self.layer_norm = LayerNorm(dim)\n",
    "        self.reduce = Reduce_np(axis=1, reduction='mean')\n",
    "        self.classifier = Linear(dim, num_classes)\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = patchify_1d(x, self.patch_size)\n",
    "        for tm, cm in self.mixer_layers:\n",
    "            tm.training = cm.training = getattr(x, 'training', True)\n",
    "        if self.proj is not None:\n",
    "            self.cache['proj_in'] = x.copy()\n",
    "            x = self.proj.forward(x)\n",
    "        else:\n",
    "            self.cache['proj_in'] = None\n",
    "        self.cache['mixer'] = []\n",
    "\n",
    "        for token_mixing, channel_mixing in self.mixer_layers:\n",
    "            mixer_cache = {}\n",
    "            mixer_cache['in'] = x.copy()\n",
    "            t_out = token_mixing.forward(x)\n",
    "            mixer_cache['token_keep'] = token_mixing.cache['keep']\n",
    "            mixer_cache['token_cache'] = token_mixing.cache\n",
    "            c_out = channel_mixing.forward(t_out)\n",
    "            mixer_cache['channel_keep'] = channel_mixing.cache['keep']\n",
    "            mixer_cache['channel_cache'] = channel_mixing.cache\n",
    "            self.cache['mixer'].append(mixer_cache)\n",
    "            x = c_out\n",
    "\n",
    "        self.cache['ln_in'] = x.copy()\n",
    "        x_ln, ln_cache = self.layer_norm.forward(x)\n",
    "        self.cache['ln_cache'] = ln_cache\n",
    "        self.cache['reduce_in'] = x_ln.copy()\n",
    "        x_red = self.reduce.forward(x_ln)\n",
    "        self.cache['clf_in'] = x_red.copy()\n",
    "        x_cls = self.classifier.forward(x_red)\n",
    "        self.cache['output'] = x_cls.copy()\n",
    "        return x_cls\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        d_clf, _ , _ = self.classifier.backward(grad_output, self.cache['clf_in'])\n",
    "        d_reduce = self.reduce.backward(d_clf)\n",
    "        d_ln, _ , _ = self.layer_norm.backward(d_reduce, self.cache['ln_cache'])\n",
    "        grad = d_ln\n",
    "\n",
    "        for (token_mixing, channel_mixing), mixer_cache in zip(self.mixer_layers[::-1],\n",
    "                                                               self.cache['mixer'][::-1]):\n",
    "            grad, _, _, _ = channel_mixing.backward(grad)\n",
    "            grad, _, _, _ = token_mixing.backward(grad)\n",
    "\n",
    "        if self.proj is not None:\n",
    "            grad, _ , _ = self.proj.backward(grad, self.cache['proj_in'])\n",
    "\n",
    "        return grad\n",
    "\n",
    "\n",
    "# ---------- Loss ----------\n",
    "class BCEWithLogits_np:\n",
    "    # Inspired from https://github.com/pytorch/pytorch/blob/c64e006fc399d528bb812ae589789d0365f3daf4/aten/src/ATen/native/Loss.cpp#L214-L259\n",
    "    # binary cross-entropy loss in a more numerically stable way using log-sum-exp when integrating sigmoid ops under this class\n",
    "    def __init__(self, weight=None, pos_weight=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weight (np.ndarray or None): Optional weight tensor, broadcastable to input.\n",
    "            pos_weight (np.ndarray or None): Optional weight for positive examples.\n",
    "            reduction (str): 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        self.weight = weight\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (np.ndarray): logits (B, 2)\n",
    "            target (np.ndarray): labels (B, 2)\n",
    "        Returns:\n",
    "            loss (float or np.ndarray): reduced loss value if reduction is 'mean' or 'sum'; elementwise loss if reduction is 'none'.\n",
    "        \"\"\"\n",
    "        max_val = np.maximum(-input, 0)\n",
    "\n",
    "        if self.pos_weight is not None:\n",
    "            log_weight = 1 + (self.pos_weight - 1) * target\n",
    "            lse = np.log(np.exp(-max_val) + np.exp(-input - max_val))\n",
    "            loss = (1 - target) * input + log_weight * (lse + max_val)\n",
    "        else:\n",
    "            loss = (1 - target) * input + max_val + np.log(np.exp(-max_val) + np.exp(-input - max_val))\n",
    "\n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight\n",
    "\n",
    "        self.cache['input'] = input.copy()\n",
    "        self.cache['target'] = target.copy()\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "\n",
    "    def backward(self, grad_output=1.0):\n",
    "        input = self.cache['input']\n",
    "        target = self.cache['target']\n",
    "\n",
    "        # Compute sigmoid(input) in a stable way.\n",
    "        sig = 1 / (1 + np.exp(-input))\n",
    "        if self.pos_weight is not None:\n",
    "            grad_input = (sig * (self.pos_weight * target + (1 - target)) - self.pos_weight * target)\n",
    "        else:\n",
    "            grad_input = sig - target\n",
    "\n",
    "        if self.weight is not None:\n",
    "            grad_input = grad_input * self.weight\n",
    "\n",
    "        # If reduction is mean, scale the gradient by 1/numel.\n",
    "        if self.reduction == 'mean':\n",
    "            grad_input = grad_input * grad_output / input.size\n",
    "        else:\n",
    "            grad_input = grad_input * grad_output\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# ---------- Optimiser ----------\n",
    "class ADAM_np:\n",
    "    # Inspired from https://gist.github.com/aerinkim/dfe3da1000e67aced1c7d9279351cb88\n",
    "    \"\"\"\n",
    "        Adam optimizer using NumPy, operating on a list of parameter 5-tuples:\n",
    "            (p, param_attr, mod, grad, grad_attr)\n",
    "        where:\n",
    "        - p is the parameter array (e.g. mod.weight or mod.bias)\n",
    "        - param_attr is \"weight\" or \"bias\"\n",
    "        - mod is the module that holds this parameter\n",
    "        - grad is the current gradient stored in mod (may be None if not computed yet)\n",
    "        - grad_attr is the name of the attribute where the gradient is stored (e.g. \"grad_weight\")\n",
    "    \"\"\"\n",
    "    def __init__(self, model, lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n",
    "        self.param_tuples = self.get_pointers_to_param_and_grad(model)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Create a state for each parameter\n",
    "        self.state = {}\n",
    "        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n",
    "            self.state[id(p)] = {\n",
    "                'step': 0,\n",
    "                'exp_avg': np.zeros_like(p),\n",
    "                'exp_avg_sq': np.zeros_like(p)\n",
    "            }\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update p in each tuple in self.param_tuples provided grad\n",
    "        Also update state\n",
    "        \"\"\"\n",
    "        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n",
    "            # Retrieve the gradient dynamically\n",
    "            grad = getattr(mod, grad_attr)\n",
    "            state = self.state[id(p)]\n",
    "            state['step'] += 1\n",
    "\n",
    "            # Apply weight decay (L2 regularization)\n",
    "            if self.weight_decay != 0:\n",
    "                grad = grad + self.weight_decay * p\n",
    "\n",
    "            # Update exponential moving averages.\n",
    "            state['exp_avg'] = self.beta1 * state['exp_avg'] + (1 - self.beta1) * grad\n",
    "            state['exp_avg_sq'] = self.beta2 * state['exp_avg_sq'] + (1 - self.beta2) * (grad * grad)\n",
    "            denom = np.sqrt(state['exp_avg_sq']) + self.eps\n",
    "\n",
    "            # Bias corrections.\n",
    "            bias_correction1 = 1 / (1 - self.beta1 ** state['step'])\n",
    "            bias_correction2 = 1 / (1 - self.beta2 ** state['step'])\n",
    "            adapted_lr = self.lr * bias_correction1 / math.sqrt(bias_correction2)\n",
    "\n",
    "            # Update parameter in place.\n",
    "            p[:] = p - adapted_lr * (state['exp_avg'] / denom)\n",
    "\n",
    "    def copy_updated_params(self):\n",
    "        \"\"\"\n",
    "        Explicitly copy the updated parameter array p back to the module's parameters in place\n",
    "        because I don't trust in-place parameters updating from optimiser.step() and it turnt out to be a good call\n",
    "        \"\"\"\n",
    "        for (p, param_attr, mod, grad, grad_attr) in self.param_tuples:\n",
    "            if param_attr == \"weight\" and hasattr(mod, \"weight\"):\n",
    "                mod.weight[...] = np.copy(p)\n",
    "            elif param_attr == \"bias\" and hasattr(mod, \"bias\"):\n",
    "                mod.bias[...] = np.copy(p)\n",
    "\n",
    "        # Optimiser.zero_grad()\n",
    "        self.param_tuples = {}\n",
    "\n",
    "    def get_pointers_to_param_and_grad(self, module, visited=None):\n",
    "        \"\"\"\n",
    "        Recursively traverses the module hierarchy and returns this tuple in the exact order:\n",
    "        (p, param_attr, module, grad, grad_attr)\n",
    "        - p is np.float32 parameter, either weight or bias of linear layer or layernorm,\n",
    "        - param_attr is a string that tells where to put p in optimiser.step equation\n",
    "        - module is reference pointer to the module that holds this parameter\n",
    "        - grad is np.float32 parameter, either grad_weight or grad_bias of linear layer or layernorm\n",
    "        - grad_attr is a string that tells where to put grad in optimiser.step equation\n",
    "\n",
    "        Skips keys that are irrelevant or duplicative (like params_grad) during recursive search\n",
    "        \"\"\"\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        tuples = []\n",
    "        if id(module) in visited:\n",
    "            return tuples\n",
    "        visited.add(id(module))\n",
    "\n",
    "        # --- Get reference pointers to the weight and grad_weight, or bias and grad_bias of a linear layer\n",
    "        if hasattr(module, 'weight') and isinstance(module.weight, np.ndarray):\n",
    "            tuples.append((getattr(module, 'weight', None),\n",
    "                        \"weight\",\n",
    "                        module,\n",
    "                        getattr(module, 'grad_weight', None),\n",
    "                        \"grad_weight\")\n",
    "                        )\n",
    "        if hasattr(module, 'bias') and module.bias is not None and isinstance(module.bias, np.ndarray):\n",
    "            tuples.append((getattr(module, 'bias', None),\n",
    "                        \"bias\",\n",
    "                        module,\n",
    "                        getattr(module, 'grad_bias', None),\n",
    "                        \"grad_bias\")\n",
    "                        )\n",
    "\n",
    "        # ---  Get reference pointers to the weight and grad_weight, or bias and grad_bias of a layernorm\n",
    "        if hasattr(module, 'norm'):\n",
    "            norm_mod = module.norm\n",
    "            if hasattr(norm_mod, 'weight') and isinstance(norm_mod.weight, np.ndarray):\n",
    "                tuples.append((getattr(norm_mod, 'weight', None),\n",
    "                            \"weight\",\n",
    "                            norm_mod,\n",
    "                            getattr(norm_mod, 'grad_weight', None),\n",
    "                            \"grad_weight\")\n",
    "                            )\n",
    "            if hasattr(norm_mod, 'bias') and norm_mod.bias is not None and isinstance(norm_mod.bias, np.ndarray):\n",
    "                tuples.append((getattr(norm_mod, 'bias', None),\n",
    "                            \"bias\",\n",
    "                            norm_mod,\n",
    "                            getattr(norm_mod, 'grad_bias', None),\n",
    "                            \"grad_bias\")\n",
    "                            )\n",
    "\n",
    "        # --- Recursively explore submodules stored as attributes.\n",
    "        for key, value in module.__dict__.items():\n",
    "            if key in {'cache', 'ctx', 'params_grad'}:\n",
    "                continue\n",
    "            if hasattr(value, 'forward') and callable(value.forward) and value is not module:\n",
    "                tuples.extend(self.get_pointers_to_param_and_grad(value, visited))\n",
    "            elif isinstance(value, (list, tuple)):\n",
    "                for item in value:\n",
    "                    if hasattr(item, 'forward') and callable(item.forward):\n",
    "                        tuples.extend(self.get_pointers_to_param_and_grad(item, visited))\n",
    "                    elif isinstance(item, (list, tuple)):\n",
    "                        for subitem in item:\n",
    "                            if hasattr(subitem, 'forward') and callable(subitem.forward):\n",
    "                                tuples.extend(self.get_pointers_to_param_and_grad(subitem, visited))\n",
    "\n",
    "        self.param_tuples = tuples\n",
    "        return tuples\n",
    "\n",
    "    def refresh_grad_per_backprop(self, model):\n",
    "        self.param_tuples = self.get_pointers_to_param_and_grad(model)\n",
    "\n",
    "\n",
    "class CosineAnnealingLR_np:\n",
    "    # Inspired from linear warmup: https://github.com/Tony-Y/pytorch_warmup\n",
    "    # and CosineAnnealingLR from pytorch: https://github.com/pytorch/pytorch/blob/v2.6.0/torch/optim/lr_scheduler.py#L1046\n",
    "    def __init__(self,\n",
    "                optimiser,\n",
    "                T_max,\n",
    "                eta_min=0.0,\n",
    "                warmup_epochs=0,\n",
    "                warmup_start_lr=0.0,\n",
    "                last_epoch=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer:       instance of ADAM_np (must have .lr and .initial_lr kept)\n",
    "            T_max (int):     number of cosine-decay epochs\n",
    "            eta_min (float): floor LR after decay\n",
    "            warmup_epochs(int): # of epochs to ramp up\n",
    "            warmup_start_lr(float): LR at epoch=0 of warmup\n",
    "            last_epoch (int): index of last epoch (so first .step() yields epoch=0)\n",
    "        \"\"\"\n",
    "        self.optimiser        = optimiser\n",
    "        self.T_max            = T_max\n",
    "        self.eta_min          = eta_min\n",
    "        self.warmup_epochs    = warmup_epochs\n",
    "        self.warmup_start_lr  = warmup_start_lr\n",
    "        self.initial_lr       = optimiser.lr\n",
    "        self.last_epoch       = last_epoch\n",
    "\n",
    "        if self.last_epoch >= 0:\n",
    "            self.step()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        for each call, update ADAM_np as follows:\n",
    "            lr = eta_min + 0.5*(initial_lr - eta_min)*(1 + cos(pi * epoch / T_max))\n",
    "            if epoch > T_max, lr == eta_min.\n",
    "        \"\"\"\n",
    "        self.last_epoch += 1\n",
    "\n",
    "        # 1) Linear warmup phase\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            alpha = (self.last_epoch + 1) / float(self.warmup_epochs)\n",
    "            new_lr = self.warmup_start_lr + alpha * (self.initial_lr - self.warmup_start_lr)\n",
    "\n",
    "        # 2) Cosine decay phase\n",
    "        elif self.last_epoch <= self.warmup_epochs + self.T_max:\n",
    "            # offset epoch for the cosine schedule\n",
    "            epoch_in_cosine = self.last_epoch - self.warmup_epochs\n",
    "            cos_term = math.cos(math.pi * epoch_in_cosine / self.T_max)\n",
    "            new_lr = self.eta_min + 0.5 * (self.initial_lr - self.eta_min) * (1 + cos_term)\n",
    "\n",
    "        # 3) eta_min phase\n",
    "        else:\n",
    "            new_lr = self.eta_min\n",
    "\n",
    "        self.optimiser.lr = new_lr\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmRestarts_np:\n",
    "    # Inspired from https://github.com/pytorch/pytorch/blob/v2.6.0/torch/optim/lr_scheduler.py#L1739\n",
    "    # I try this to escape local saddle point in loss landscape with WarmRestart\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        optimizer: any object with attributes:\n",
    "            - lr: current learning rate\n",
    "            - initial_lr: initial (max) learning rate\n",
    "        T_0 (int): number of iterations for the first cycle\n",
    "        T_mult (int): multiplicative factor to increase cycle length after each restart\n",
    "        eta_min (float): minimum learning rate\n",
    "        last_epoch (int or float): index of last iteration (allows fractional epochs)\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0.0, last_epoch=-1, warmup_epochs=0, warmup_start_lr=0.0):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0\")\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1\")\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.T_i = T_0\n",
    "        self.T_cur = last_epoch\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        # Warmup epoch\n",
    "        self.warmup_epochs  = warmup_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        # figure out which iteration we're on\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        if epoch < 0:\n",
    "            raise ValueError(\"Expected non-negative epoch\")\n",
    "\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        # 1) Warmup phase\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # linear ramp from warmup_start_lr → base_lr\n",
    "            alpha = (epoch + 1) / float(self.warmup_epochs)\n",
    "            lr = self.warmup_start_lr + alpha * (self.base_lr - self.warmup_start_lr)\n",
    "            return lr\n",
    "\n",
    "        # 2) Cosine‑restarts phase\n",
    "        # shift epoch index so cycle logic starts at 0\n",
    "        e = epoch - self.warmup_epochs\n",
    "\n",
    "        # determine which restart‐cycle we're in\n",
    "        if e >= self.T_0:\n",
    "            if self.T_mult == 1:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = e % self.T_0\n",
    "            else:\n",
    "                # solve for cycle index n\n",
    "                n = int(\n",
    "                    math.log(\n",
    "                        (e / self.T_0) * (self.T_mult - 1) + 1,\n",
    "                        self.T_mult\n",
    "                    )\n",
    "                )\n",
    "                self.T_i = self.T_0 * (self.T_mult ** n)\n",
    "                self.T_cur = e - (self.T_0 * (self.T_mult**n - 1)) / (self.T_mult - 1)\n",
    "        else:\n",
    "            # still in first cosine cycle\n",
    "            self.T_i = self.T_0\n",
    "            self.T_cur = e\n",
    "\n",
    "        # compute cosine lr\n",
    "        cos_val = math.cos(math.pi * self.T_cur / self.T_i)\n",
    "        lr = self.eta_min + 0.5 * (self.base_lr - self.eta_min) * (1 + cos_val)\n",
    "        self.optimizer.lr = lr\n",
    "    \n",
    "\n",
    "def clip_grad_global_norm(param_tuples, max_norm=1.0, eps=1e-6):\n",
    "    \"\"\"\n",
    "    This is gradient clipping. It scales down grad a bit like how weight decay scales down grad\n",
    "    \"\"\"\n",
    "    # Compute squared norm\n",
    "    total_norm_sq = 0.0\n",
    "    for _, _, _, grad, _ in param_tuples:\n",
    "        if grad is not None:\n",
    "            total_norm_sq += np.sum(grad * grad)\n",
    "    total_norm = math.sqrt(total_norm_sq)\n",
    "\n",
    "    clip_coef = max_norm / (total_norm + eps)\n",
    "\n",
    "    # scale all gradients by squared norm gradient_max_norm\n",
    "    if clip_coef < 1.0:\n",
    "        for i, (p, param_attr, mod, grad, grad_attr) in enumerate(param_tuples):\n",
    "            if grad is not None:\n",
    "                new_grad = grad * clip_coef\n",
    "                param_tuples[i] = (p, param_attr, mod, new_grad, grad_attr)\n",
    "\n",
    "\n",
    "# ----------Model.zero_grad() ----------\n",
    "def clear_auxiliary(module):\n",
    "    \"\"\"\n",
    "    Recursively clears auxiliary attributes from the module and its submodules.\n",
    "    This includes attributes with keys 'cache', 'ctx', 'params_grad', and 'mask'.\n",
    "\n",
    "    For each key:\n",
    "      - If its value is a dict, it is set to {}.\n",
    "      - Otherwise, it is set to None.\n",
    "    \"\"\"\n",
    "    aux_keys = ('cache', 'ctx', 'params_grad', 'mask')\n",
    "    # Clear any auxiliary attribute on this module.\n",
    "    for key in aux_keys:\n",
    "        if key in module.__dict__:\n",
    "            current = module.__dict__[key]\n",
    "            if isinstance(current, dict):\n",
    "                setattr(module, key, {})\n",
    "            else:\n",
    "                setattr(module, key, None)\n",
    "\n",
    "    # Now recursively traverse submodules.\n",
    "    for key, value in module.__dict__.items():\n",
    "        # Skip aux keys already handled.\n",
    "        if key in aux_keys:\n",
    "            continue\n",
    "\n",
    "        # If the attribute is a list or tuple, iterate through its items.\n",
    "        if isinstance(value, (list, tuple)):\n",
    "            for item in value:\n",
    "                if isinstance(item, tuple):\n",
    "                    for subitem in item:\n",
    "                        if hasattr(subitem, 'forward') and callable(subitem.forward):\n",
    "                            clear_auxiliary(subitem)\n",
    "                elif hasattr(value, 'forward') and callable(value.forward):\n",
    "                    # Unlikely: this checks if the entire list itself has a forward().\n",
    "                    clear_auxiliary(value)\n",
    "                elif hasattr(item, 'forward') and callable(item.forward):\n",
    "                    clear_auxiliary(item)\n",
    "        # If the attribute is a submodule (has callable forward), recurse.\n",
    "        elif hasattr(value, 'forward') and callable(value.forward) and value is not module:\n",
    "            clear_auxiliary(value)\n",
    "\n",
    "\n",
    "# ---------- After training ----------\n",
    "def save_model_and_optimiser(model, optimiser, filename=\"model_optim_state.npy\"):\n",
    "    \"\"\"\n",
    "    Save model parameters and optimizer state to a .npy file.\n",
    "    The final dictionary has the form:\n",
    "\n",
    "        {\n",
    "           \"model_state\": { key: parameter_array, ... },\n",
    "           \"optimizer_state\": optimizer.state\n",
    "        }\n",
    "\n",
    "    Args:\n",
    "        model: Your MLPMixer instance.\n",
    "        optimizer: Your ADAM_np instance.\n",
    "        filename: The file name to save the state (default: \"model_optim_state.npy\").\n",
    "    \"\"\"\n",
    "    param_tuples = optimiser.get_pointers_to_param_and_grad(model)\n",
    "\n",
    "    model_state = {}\n",
    "    # Create a unique key for each parameter.\n",
    "    for (p, param_attr, mod, grad, grad_attr) in param_tuples:\n",
    "        key = f\"{mod.__class__.__name__}_{param_attr}_{id(p)}\"\n",
    "        model_state[key] = p.copy()\n",
    "\n",
    "    optimiser_state = optimiser.state.copy()\n",
    "\n",
    "    state = {\n",
    "        \"model_state\": model_state,\n",
    "        \"optimizer_state\": optimiser_state\n",
    "    }\n",
    "\n",
    "    # Save to a .npy file (when loading, use allow_pickle=True).\n",
    "    np.save(filename, state)\n",
    "    print(f\"Model and optimizer state saved to {filename}\")\n",
    "\n",
    "\n",
    "# ---------- Hyper-params ----------\n",
    "embed_dim = 198\n",
    "patch_size = 3\n",
    "seq_len = nb15_dataset.train_set.shape[1] - 1\n",
    "channels=1 if nb15_dataset.train_set.ndim == 2 else nb15_dataset.train_set.shape[-1]\n",
    "depth=10\n",
    "num_classes=1\n",
    "dropout_p=0.3\n",
    "stochastic_depth_p=0.3\n",
    "\n",
    "#accum_steps = 64\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "lr = 0.1\n",
    "weight_decay=0\n",
    "warmup_epochs=2\n",
    "gradient_clip_max_norm=1.0\n",
    "T_max = num_epochs*0.15\n",
    "\n",
    "\n",
    "# ---------- Instantiate model and trainer components ----------\n",
    "model = MLPMixer(seq_len=seq_len, channels=channels, dim=embed_dim, depth=depth, num_classes=num_classes, patch_size=patch_size, dropout_p=dropout_p, stochastic_depth_p=stochastic_depth_p)\n",
    "loss_fn_np = BCEWithLogits_np(reduction='mean')\n",
    "optimiser_np = ADAM_np(model, lr=lr, weight_decay=weight_decay)\n",
    "scheduler_np = CosineAnnealingLR_np(optimiser_np, T_max=T_max, eta_min=1e-6, warmup_epochs=warmup_epochs, warmup_start_lr=0.0)\n",
    "# CosineAnnealingWarmRestarts_np sucks. It's better to just add noise in parameter updating\n",
    "# scheduler_np = CosineAnnealingWarmRestarts_np(optimiser_np, T_0=warmup_epochs, T_mult=2, eta_min=1e-8, last_epoch=num_epochs, warmup_epochs=warmup_epochs, warmup_start_lr=0)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "# import pdb\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler_np.step()\n",
    "    # scheduler_np.step(epoch)\n",
    "    np.random.shuffle(nb15_dataset.train_set)\n",
    "    batches = [nb15_dataset.train_set[i:i+batch_size] for i in range(0, nb15_dataset.train_set.shape[0], batch_size)]\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    total_train_loss_per_epoch = 0.0\n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        labels = batch[:, -1].reshape(-1, 1)\n",
    "        features = np.expand_dims(batch[:, :-1], axis=-1)\n",
    "        logits = model.forward(features)\n",
    "\n",
    "        batch_train_loss = loss_fn_np.forward(logits, labels)\n",
    "\n",
    "        grad_logits = loss_fn_np.backward()\n",
    "        # grad_logits /= accum_steps\n",
    "        _ = model.backward(grad_logits)\n",
    "\n",
    "        #before_step_mixer_linear = np.copy(model.mixer_layers[0][0].fn.linear1.weight)\n",
    "        optimiser_np.refresh_grad_per_backprop(model)\n",
    "        clip_grad_global_norm(optimiser_np.param_tuples, max_norm=gradient_clip_max_norm)\n",
    "        #print(optimiser_np.param_tuples)\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        #if ((batch_idx + 1) % accum_steps == 0) or (batch_idx + 1 == len(batches)):\n",
    "        optimiser_np.step()\n",
    "        #after_step_mixer_linear = model.mixer_layers[0][0].fn.linear1.weight\n",
    "        #print(\"Max abs diff between before and after stepping (should be a small difference):\", np.max(np.abs(before_step_mixer_linear - after_step_mixer_linear)))\n",
    "        #pdb.set_trace()\n",
    "        optimiser_np.copy_updated_params()\n",
    "        #after_step_and_copy_mixer_linear = model.mixer_layers[0][0].fn.linear1.weight\n",
    "        #print(\"Max abs diff between before and after update and copy (should be 0):\", np.max(np.abs(after_step_and_copy_mixer_linear - after_step_mixer_linear)))\n",
    "        #pdb.set_trace()\n",
    "        clear_auxiliary(model)\n",
    "\n",
    "        total_train_loss_per_epoch += batch_train_loss\n",
    "\n",
    "        train_iter += 1\n",
    "        if train_iter % batch_size == 0:\n",
    "            print(f\"Train Iteration {train_iter}, Loss: {batch_train_loss:.8f}\")\n",
    "    avg_train_loss_per_epoch = total_train_loss_per_epoch / len(batches)\n",
    "    train_losses.append(avg_train_loss_per_epoch)\n",
    "\n",
    "    total_val_loss_per_epoch = 0.0\n",
    "    val_batches = [nb15_dataset.val_set[i:i+batch_size] for i in range(0, nb15_dataset.val_set.shape[0], batch_size)]\n",
    "    for val_batch in val_batches:\n",
    "        labels = val_batch[:, -1].reshape(-1, 1)\n",
    "        features = np.expand_dims(val_batch[:, :-1], axis=-1)\n",
    "        logits = model.forward(features)\n",
    "\n",
    "        batch_val_loss = loss_fn_np.forward(logits, labels)\n",
    "\n",
    "        total_val_loss_per_epoch += batch_val_loss\n",
    "\n",
    "        val_iter += 1\n",
    "        if val_iter % batch_size == 0:\n",
    "            print(f\"Val Iteration {val_iter}, Loss: {batch_val_loss:.8f}\")\n",
    "    avg_val_loss_per_epoch = total_val_loss_per_epoch / len(val_batches)\n",
    "    val_losses.append(avg_val_loss_per_epoch)\n",
    "\n",
    "print(\"Average loss:\", np.mean(train_losses))\n",
    "save_model_and_optimiser(model, optimiser_np, filename=f\"mlpmixer{embed_dim}_patch{patch_size}_embeddim{embed_dim}_depth{depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1745181644145,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "AkvF5DIjpq5t"
   },
   "outputs": [],
   "source": [
    "# print(optimiser_np.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1745181644162,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "4UacOPuFe6QF"
   },
   "outputs": [],
   "source": [
    "# print(optimiser_np.param_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1745181683553,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "SX5nvX0WwJX_"
   },
   "outputs": [],
   "source": [
    "clear_auxiliary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745181686621,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "cv6DppIDRwI8",
    "outputId": "7c6a321b-0a3e-4cc4-af9e-cf4077961d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ4bBu-PQ3-g"
   },
   "source": [
    "# ***3. Model Performance Evaluation (Task 3)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8442,
     "status": "aborted",
     "timestamp": 1745181644167,
     "user": {
      "displayName": "Nam Tran",
      "userId": "15778016803607933273"
     },
     "user_tz": -60
    },
    "id": "filj8Tb2Bav9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.6744\n",
      "Precision: 0.9767\n",
      "Recall: 0.5292\n",
      "F1 Score: 0.6864\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#put your code for model performance evaluation here\n",
    "def load_model_and_optimiser(model, optimiser, filename=\"model_optim_state.npy\"):\n",
    "    \"\"\"\n",
    "    Load pretrained model parameters and optimizer state from a file.\n",
    "\n",
    "    This function loads the state dictionary saved using 'save_model_and_optimiser', updates\n",
    "    the model's parameters (in place) and sets the optimizer's state accordingly.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance whose parameters should be updated.\n",
    "        optimiser: The instance of ADAM_np managing the model parameters.\n",
    "        filename: The filename of the saved state (default is \"model_optim_state.npy\").\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If a parameter key from the model is not found in the saved state.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Load the saved state dictionary with allow_pickle=True\n",
    "    state = np.load(filename, allow_pickle=True).item()\n",
    "    saved_model_state = state[\"model_state\"]\n",
    "    saved_optimiser_state = state[\"optimizer_state\"]\n",
    "\n",
    "    # Refresh the mapping (the same order as used when saving is assumed)\n",
    "    param_tuples = optimiser.get_pointers_to_param_and_grad(model)\n",
    "\n",
    "    # Update each parameter in the model using the saved state\n",
    "    for (p, param_attr, mod, grad, grad_attr) in param_tuples:\n",
    "        # Recreate the key used during saving\n",
    "        key = f\"{mod.__class__.__name__}_{param_attr}_{id(p)}\"\n",
    "        if key in saved_model_state:\n",
    "            # In-place update of the parameter array\n",
    "            p[...] = saved_model_state[key]\n",
    "        else:\n",
    "            # If key not found, it's likely there's a mismatch between model architectures\n",
    "            raise KeyError(f\"Parameter key '{key}' not found in saved state. \"\n",
    "                           \"Ensure that the model architecture and the optimizer pointers \"\n",
    "                           \"match those used when saving.\")\n",
    "\n",
    "    # Update the optimizer state (this assumes the optimizer's internal state structure remains consistent)\n",
    "    optimiser.state = saved_optimiser_state.copy()\n",
    "\n",
    "    print(f\"Loaded model and optimizer state from '{filename}'\")\n",
    "\n",
    "\n",
    "# load_model_and_optimiser(model, optimiser_np)\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for val_batch in val_batches:\n",
    "    labels = val_batch[:, -1].reshape(-1, 1)\n",
    "    features = np.expand_dims(val_batch[:, :-1], axis=-1)\n",
    "    logits = model.forward(features)\n",
    "\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    predictions = (probabilities >= 0.5).astype(int)\n",
    "\n",
    "    all_labels.append(labels)\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "TP = np.sum((all_predictions == 1) & (all_labels == 1))\n",
    "FP = np.sum((all_predictions == 1) & (all_labels == 0))\n",
    "TN = np.sum((all_predictions == 0) & (all_labels == 0))\n",
    "FN = np.sum((all_predictions == 0) & (all_labels == 1))\n",
    "accuracy_1 = np.mean(all_predictions == all_labels)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "F1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "# Print the results.\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {F1:.4f}\")\n",
    "# print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "############################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming train_losses and val_losses are lists of loss values per epoch.\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "test_features = np.expand_dims(nb15_dataset.test_set, axis=-1)\n",
    "test_logits = model.forward(test_features)\n",
    "test_probabilities = 1 / (1 + np.exp(-test_logits))\n",
    "test_predictions = (test_probabilities >= 0.5).astype(int)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iCUL48FBwlQ"
   },
   "source": [
    "# ***4. Performance Evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hda1q3KOBM_f"
   },
   "source": [
    "**Please report the classification accuracy your model achieved on data samples in the testing set 1 below (in percentage)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePqjGPnHmYWv"
   },
   "source": [
    "95.92%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpMWHu9bCFSo"
   },
   "source": [
    "**Please provide the predicted class labels of the data samples in the testing set 1 below (0 or 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8B_MxvCnf3"
   },
   "source": [
    "| **Sample ID** |**Predicted Label** |\n",
    "| --- | --- |\n",
    "| 1 | 1 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n",
    "| 5 | 0  |\n",
    "| 6 | 0  |\n",
    "| 7 | 0  |\n",
    "| 8 | 0  |\n",
    "| 9 | 0  |\n",
    "| 10 |0   |\n",
    "| 11 |0   |\n",
    "| 12 |0   |\n",
    "| 13 |1   |\n",
    "| 14 |1   |\n",
    "| 15 |0   |\n",
    "| 16 |0   |\n",
    "| 17 |1   |\n",
    "| 18 |1   |\n",
    "| 19 |0   |\n",
    "| 20 |0   |\n",
    "| 21 |0   |\n",
    "| 22 |1   |\n",
    "| 23 |0   |\n",
    "| 24 |0   |\n",
    "| 25 |1   |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLPMixer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
