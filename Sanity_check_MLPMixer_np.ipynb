{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkh5H+haxLTvKn/60xqLh2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Speed and precision benchmark between polars vs pandas in reading csv"],"metadata":{"id":"WtO2_RLelhzN"}},{"cell_type":"code","source":["############################################################\n","#put your code for reading the csv data here\n","############################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/\n","\n","import pandas as pd\n","import polars as pl\n","\n","import torch.utils.benchmark as benchmark\n","\n","def pandas_read_csv(csv_train, csv_val, csv_test):\n","    \"\"\"\n","    Args:\n","        csv_train (str): name of train set csv file\n","        csv_val (str): name of val set csv file\n","        csv_test (str): name of test set csv file\n","\n","    Output:\n","        read train, val, test sets in DataFrame format\n","    \"\"\"\n","    df_train = pd.read_csv(csv_train)\n","    df_val = pd.read_csv(csv_val)\n","    df_test = pd.read_csv(csv_test)\n","\n","    # uncomment this to verify floating value difference\n","    #df_train.to_csv(\"df_train.csv\", index=False)\n","    #df_val.to_csv(\"df_val.csv\", index=False)\n","    #df_test.to_csv(\"df_test.csv\", index=False)\n","\n","    # uncomment return when not doing speed benchmark\n","    return df_train, df_val, df_test\n","\n","def polar_read_csv(csv_train, csv_val, csv_test):\n","    \"\"\"\n","    Args:\n","        csv_train (str): name of train set csv file\n","        csv_val (str): name of val set csv file\n","        csv_test (str): name of test set csv file\n","\n","    Output:\n","        read train, val, test sets in DataFrame format\n","    \"\"\"\n","\n","    df_train = pl.read_csv(csv_train)\n","    df_val = pl.read_csv(csv_val)\n","    df_test = pl.read_csv(csv_test)\n","\n","    # uncomment this to verify floating value difference\n","    #polar_train.write_csv(\"polar_train.csv\")\n","    #polar_val.write_csv(\"polar_val.csv\")\n","    #polar_test.write_csv(\"polar_test.csv\")\n","\n","    # uncomment return when not doing speed benchmark\n","    return df_train, df_val, df_test\n","\n","pandas_read_csv(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")\n","polar_read_csv(\"UNSWNB15_training_coursework.csv\", \"UNSWNB15_testing1_coursework.csv\", \"UNSWNB15_testing2_coursework_no_label.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOCH1SOPlEy9","executionInfo":{"status":"ok","timestamp":1744292424058,"user_tz":-60,"elapsed":33416,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"93e90894-206c-4211-8e45-02d457b4c940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]},{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]},{"output_type":"execute_result","data":{"text/plain":["(shape: (20_000, 44)\n"," ┌───────┬──────────┬───────┬─────────┬───┬────────────┬────────────┬─────────────────┬───────┐\n"," │ id    ┆ dur      ┆ proto ┆ service ┆ … ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_ports ┆ label │\n"," │ ---   ┆ ---      ┆ ---   ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---             ┆ ---   │\n"," │ i64   ┆ f64      ┆ str   ┆ str     ┆   ┆ i64        ┆ i64        ┆ i64             ┆ i64   │\n"," ╞═══════╪══════════╪═══════╪═════════╪═══╪════════════╪════════════╪═════════════════╪═══════╡\n"," │ 1     ┆ 0.000003 ┆ unas  ┆ -       ┆ … ┆ 8          ┆ 11         ┆ 0               ┆ 1     │\n"," │ 2     ┆ 0.885807 ┆ tcp   ┆ ftp     ┆ … ┆ 5          ┆ 1          ┆ 0               ┆ 0     │\n"," │ 3     ┆ 0.538781 ┆ tcp   ┆ http    ┆ … ┆ 2          ┆ 6          ┆ 0               ┆ 0     │\n"," │ 4     ┆ 0.000008 ┆ udp   ┆ dns     ┆ … ┆ 27         ┆ 34         ┆ 0               ┆ 1     │\n"," │ 5     ┆ 0.448734 ┆ tcp   ┆ ftp     ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 1     │\n"," │ …     ┆ …        ┆ …     ┆ …       ┆ … ┆ …          ┆ …          ┆ …               ┆ …     │\n"," │ 19996 ┆ 0.000008 ┆ sctp  ┆ -       ┆ … ┆ 2          ┆ 5          ┆ 0               ┆ 1     │\n"," │ 19997 ┆ 0.789808 ┆ tcp   ┆ http    ┆ … ┆ 2          ┆ 1          ┆ 0               ┆ 1     │\n"," │ 19998 ┆ 0.539269 ┆ tcp   ┆ http    ┆ … ┆ 6          ┆ 2          ┆ 0               ┆ 1     │\n"," │ 19999 ┆ 0.931055 ┆ tcp   ┆ -       ┆ … ┆ 2          ┆ 7          ┆ 0               ┆ 0     │\n"," │ 20000 ┆ 2.947155 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 0     │\n"," └───────┴──────────┴───────┴─────────┴───┴────────────┴────────────┴─────────────────┴───────┘,\n"," shape: (4_000, 44)\n"," ┌──────┬──────────┬───────┬─────────┬───┬────────────┬────────────┬─────────────────┬───────┐\n"," │ id   ┆ dur      ┆ proto ┆ service ┆ … ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_ports ┆ label │\n"," │ ---  ┆ ---      ┆ ---   ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---             ┆ ---   │\n"," │ i64  ┆ f64      ┆ str   ┆ str     ┆   ┆ i64        ┆ i64        ┆ i64             ┆ i64   │\n"," ╞══════╪══════════╪═══════╪═════════╪═══╪════════════╪════════════╪═════════════════╪═══════╡\n"," │ 1    ┆ 0.000009 ┆ sep   ┆ -       ┆ … ┆ 1          ┆ 6          ┆ 0               ┆ 1     │\n"," │ 2    ┆ 0.965595 ┆ tcp   ┆ ftp     ┆ … ┆ 2          ┆ 1          ┆ 0               ┆ 0     │\n"," │ 3    ┆ 0.000008 ┆ udp   ┆ dns     ┆ … ┆ 22         ┆ 35         ┆ 0               ┆ 1     │\n"," │ 4    ┆ 0.012175 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 8          ┆ 0               ┆ 0     │\n"," │ 5    ┆ 0.000004 ┆ udp   ┆ dns     ┆ … ┆ 26         ┆ 26         ┆ 0               ┆ 1     │\n"," │ …    ┆ …        ┆ …     ┆ …       ┆ … ┆ …          ┆ …          ┆ …               ┆ …     │\n"," │ 3996 ┆ 0.689171 ┆ tcp   ┆ -       ┆ … ┆ 1          ┆ 6          ┆ 0               ┆ 0     │\n"," │ 3997 ┆ 1.091111 ┆ tcp   ┆ pop3    ┆ … ┆ 1          ┆ 1          ┆ 0               ┆ 1     │\n"," │ 3998 ┆ 0.017941 ┆ tcp   ┆ -       ┆ … ┆ 10         ┆ 8          ┆ 0               ┆ 0     │\n"," │ 3999 ┆ 0.000008 ┆ unas  ┆ -       ┆ … ┆ 4          ┆ 6          ┆ 0               ┆ 1     │\n"," │ 4000 ┆ 0.00095  ┆ udp   ┆ dns     ┆ … ┆ 2          ┆ 2          ┆ 0               ┆ 0     │\n"," └──────┴──────────┴───────┴─────────┴───┴────────────┴────────────┴─────────────────┴───────┘,\n"," shape: (25, 43)\n"," ┌─────┬──────────┬───────────┬─────────┬───┬──────────────┬────────────┬────────────┬──────────────┐\n"," │ id  ┆ dur      ┆ proto     ┆ service ┆ … ┆ ct_flw_http_ ┆ ct_src_ltm ┆ ct_srv_dst ┆ is_sm_ips_po │\n"," │ --- ┆ ---      ┆ ---       ┆ ---     ┆   ┆ mthd         ┆ ---        ┆ ---        ┆ rts          │\n"," │ i64 ┆ f64      ┆ str       ┆ str     ┆   ┆ ---          ┆ i64        ┆ i64        ┆ ---          │\n"," │     ┆          ┆           ┆         ┆   ┆ i64          ┆            ┆            ┆ i64          │\n"," ╞═════╪══════════╪═══════════╪═════════╪═══╪══════════════╪════════════╪════════════╪══════════════╡\n"," │ 1   ┆ 0.000003 ┆ udp       ┆ -       ┆ … ┆ 0            ┆ 1          ┆ 3          ┆ 0            │\n"," │ 2   ┆ 0.015833 ┆ tcp       ┆ -       ┆ … ┆ 0            ┆ 2          ┆ 6          ┆ 0            │\n"," │ 3   ┆ 0.000003 ┆ merit-inp ┆ -       ┆ … ┆ 0            ┆ 2          ┆ 4          ┆ 0            │\n"," │ 4   ┆ 0.000003 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 32         ┆ 33         ┆ 0            │\n"," │ 5   ┆ 0.00106  ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 3          ┆ 3          ┆ 0            │\n"," │ …   ┆ …        ┆ …         ┆ …       ┆ … ┆ …            ┆ …          ┆ …          ┆ …            │\n"," │ 21  ┆ 0.916548 ┆ tcp       ┆ ftp     ┆ … ┆ 0            ┆ 2          ┆ 1          ┆ 0            │\n"," │ 22  ┆ 0.000009 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 12         ┆ 20         ┆ 0            │\n"," │ 23  ┆ 1.005819 ┆ tcp       ┆ http    ┆ … ┆ 1            ┆ 3          ┆ 3          ┆ 0            │\n"," │ 24  ┆ 0.590464 ┆ tcp       ┆ ftp     ┆ … ┆ 0            ┆ 2          ┆ 2          ┆ 0            │\n"," │ 25  ┆ 0.000008 ┆ udp       ┆ dns     ┆ … ┆ 0            ┆ 27         ┆ 39         ┆ 0            │\n"," └─────┴──────────┴───────────┴─────────┴───┴──────────────┴────────────┴────────────┴──────────────┘)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# uncommment this for speed benchmark\n","t0 = benchmark.Timer(\n","    stmt='pandas_read_csv(csv_train, csv_val, csv_test)',\n","    setup='from __main__ import pandas_read_csv',\n","    globals={'csv_train': \"UNSWNB15_training_coursework.csv\", \"csv_val\": \"UNSWNB15_testing1_coursework.csv\", \"csv_test\": \"UNSWNB15_testing2_coursework_no_label.csv\"})\n","\n","t1 = benchmark.Timer(\n","    stmt='polar_read_csv(csv_train, csv_val, csv_test)',\n","    setup='from __main__ import polar_read_csv',\n","    globals={'csv_train': \"UNSWNB15_training_coursework.csv\", \"csv_val\": \"UNSWNB15_testing1_coursework.csv\", \"csv_test\": \"UNSWNB15_testing2_coursework_no_label.csv\"})\n","\n","print(t0.timeit(1000))\n","print(t1.timeit(1000))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCrtAgpo6kYe","executionInfo":{"status":"ok","timestamp":1744191803640,"user_tz":-60,"elapsed":235829,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"e4b523d8-e0a3-46bb-cccd-57476af5fb90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.benchmark.utils.common.Measurement object at 0x7e7615cd8690>\n","pandas_read_csv(csv_train, csv_val, csv_test)\n","setup: from __main__ import pandas_read_csv\n","  173.19 ms\n","  1 measurement, 1000 runs , 1 thread\n","<torch.utils.benchmark.utils.common.Measurement object at 0x7e750cb518d0>\n","polar_read_csv(csv_train, csv_val, csv_test)\n","setup: from __main__ import polar_read_csv\n","  60.18 ms\n","  1 measurement, 1000 runs , 1 thread\n"]}]},{"cell_type":"markdown","source":["### Precision benchmark for Data-Processing"],"metadata":{"id":"ipZN9oLalr4o"}},{"cell_type":"code","source":["import numpy as np\n","\n","a = np.float32(1)\n","b = np.float32(2)\n","c1 = np.float32(266666656)\n","c2 = np.float32(266661656)\n","print(f\"{b/c1:.30f}\")\n","print(f\"{b/c2:.30f}\")\n","print(f\"{a/c1:.30f}\")\n","print(f\"{a/c2:.30f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qD-jyzXKlyAv","executionInfo":{"status":"ok","timestamp":1744118490474,"user_tz":-60,"elapsed":13,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"95a19632-089c-4bfc-b9ec-b5ecbe423e78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.000000007500000620552782493178\n","0.000000007500140952743095112965\n","0.000000003750000310276391246589\n","0.000000003750070476371547556482\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import polars as pl\n","import json\n","\n","def encode_category_attribute(csv_train, csv_val, csv_test, attribute_list, dict_output_file=\"train_category.json\"):\n","    df_train, df_val, df_test = polar_read_csv(csv_train, csv_val, csv_test)\n","\n","    attribute_dict = {}\n","    for attribute in attribute_list:\n","        if attribute in df_train.columns:\n","            unique_vals = (df_train.select(attribute).drop_nulls().unique().to_series().to_list())\n","            attribute_dict[attribute] = {val: idx for idx, val in enumerate(unique_vals)}\n","        else:\n","            print(f\"Warning: '{attribute}' not found in training CSV.\")\n","\n","    def apply_encoding(df, attribute_dict):\n","        for attribute, val_to_idx in attribute_dict.items():\n","            if attribute in df.columns:\n","                df = df.with_columns(pl.col(attribute).replace(val_to_idx).alias(attribute))\n","        return df\n","    df_train_encoded = apply_encoding(df_train, attribute_dict)\n","    df_val_encoded = apply_encoding(df_val, attribute_dict)\n","    df_test_encoded = apply_encoding(df_test, attribute_dict)\n","\n","    # save encoded csv and .json to verify difference between using pandas vs polars to encode csv file\n","    with open(dict_output_file, 'w') as f:\n","        json.dump(attribute_dict, f, indent=4)\n","    #df_train_encoded.write_csv(\"polar_train_encoded.csv\")\n","    #df_val_encoded.write_csv(\"polar_val_encoded.csv\")\n","    #df_test_encoded.write_csv(\"polar_test_encoded.csv\")\n","\n","    return df_train_encoded, df_val_encoded, df_test_encoded\n","\n","\n","def get_mean_std_after_rescaled(x: np.ndarray):\n","    scale = np.max(x)\n","    x_rescaled = x/scale\n","    mean = np.mean(x_rescaled, axis=0, keepdims=True)\n","    std = np.std(x_rescaled, axis=0, keepdims=True)\n","    return mean, std\n","\n","def fused_rescale_normalise(x: np.ndarray, mean, std):\n","    scaling_factor = np.max(x)\n","    return (x - mean*scaling_factor) / (std*scaling_factor)\n","\n","def rescale_then_normalise(x: np.ndarray):\n","    scale = np.max(x)\n","    x_rescaled = x/scale\n","    mean = np.mean(x_rescaled, axis=0)\n","    std = np.std(x_rescaled, axis=0)\n","    return (x_rescaled - mean) / std\n","\n","train_df_encoded, val_df_encoded, test_df_encoded = encode_category_attribute(\"UNSWNB15_training_coursework.csv\",\n","                                                                              \"UNSWNB15_testing1_coursework.csv\",\n","                                                                              \"UNSWNB15_testing2_coursework_no_label.csv\",\n","                                                                               [\"proto\", \"service\", \"state\"])\n","val_df_encoded = val_df_encoded.filter(pl.col(\"state\") != \"RST\")\n","\n","train_arr = train_df_encoded.to_numpy().astype(np.float32)\n","val_arr = val_df_encoded.to_numpy().astype(np.float32)\n","test_arr = test_df_encoded.to_numpy().astype(np.float32)\n","\n","train_features = train_arr[:, 1:-1]\n","val_features = val_arr[:, 1:-1]\n","test_features = test_arr[:, 1:]\n","\n","mean, std = get_mean_std_after_rescaled(train_features)\n","train_set = np.concatenate([fused_rescale_normalise(train_features, mean, std), train_arr[:, -1].reshape(-1, 1)], axis=1)\n","val_set = np.concatenate([fused_rescale_normalise(val_features, mean, std), val_arr[:, -1].reshape(-1, 1)], axis=1)\n","test_set = fused_rescale_normalise(test_features, mean, std)\n","\n","separate_train_set = np.concatenate([rescale_then_normalise(train_features), train_arr[:, -1].reshape(-1, 1)], axis=1)\n","print(\"Max absolute difference between fused scale-normalise and scale-then-normalise\", np.max(np.abs(train_set - separate_train_set)))\n","\n","print(train_set.shape)\n","num_features = train_set.shape[1]\n","column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n","\n","df = pl.DataFrame(train_set)\n","df.write_csv(\"normalised_train_features.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsHW8YyHlyin","executionInfo":{"status":"ok","timestamp":1744292428887,"user_tz":-60,"elapsed":1293,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"4031f911-62bd-4712-9687-2fa6f389019a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max absolute difference between fused scale-normalise and scale-then-normalise 3.8146973e-06\n","(20000, 43)\n"]}]},{"cell_type":"code","source":["batch_size = 16\n","batches = [train_set[i:i+batch_size] for i in range(0, train_set.shape[0], batch_size)]\n","batch_0 = batches[0]\n","labels_0 = batch_0[:, -1].reshape(-1, 1)\n","features_0 = np.expand_dims(batch_0[:, :-1], axis=-1)\n","print(features_0.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIOP3WCo662z","executionInfo":{"status":"ok","timestamp":1744292442917,"user_tz":-60,"elapsed":11,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"8860fac5-fdd4-46e0-fdaf-a233799bf505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(16, 42, 1)\n"]}]},{"cell_type":"markdown","source":["# Precision benchmark for model implementation"],"metadata":{"id":"D9OdOPVSnGe9"}},{"cell_type":"markdown","source":["### Linear layer"],"metadata":{"id":"AF1hXYn3jvKt"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import math\n","\n","# ---------- Kaiming uniform distribution to init weight & bias of Linear layer ----------\n","def calculate_gain(nonlinearity, param=None):\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L72\n","    if nonlinearity in ['linear', 'conv1d', 'conv2d', 'conv3d',\n","                        'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'sigmoid']:\n","        return 1.0\n","    elif nonlinearity == 'tanh':\n","        return 5.0 / 3\n","    elif nonlinearity == 'relu':\n","        return math.sqrt(2.0)\n","    elif nonlinearity == 'leaky_relu':\n","        negative_slope = 0.01 if param is None else param\n","        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n","    else:\n","        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n","\n","\n","def _calculate_correct_fan(tensor, mode):\n","    # Inspired from _calculate_fan_in_and_fan_out: https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L345\n","    # Note that pytorch uses _calculate_correct_fan as API for _calculate_fan_in_and_fan_out so I merely remove the API\n","    if tensor.ndim < 2:\n","        raise ValueError(\"Fan in and fan out cannot be computed for tensor with fewer than 2 dimensions\")\n","    if mode == \"fan_in\":\n","        num_input_fmaps = tensor.shape[1]\n","        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n","        return num_input_fmaps * receptive_field_size\n","    elif mode == \"fan_out\":\n","        num_output_fmaps = tensor.shape[0]\n","        receptive_field_size = np.prod(tensor.shape[2:]) if tensor.ndim > 2 else 1\n","        return num_output_fmaps * receptive_field_size\n","    else:\n","        raise ValueError(\"mode must be either 'fan_in' or 'fan_out'\")\n","\n","\n","def kaiming_uniform_(tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"):\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py#L456\n","    fan = _calculate_correct_fan(tensor, mode)\n","    gain = calculate_gain(nonlinearity, a)\n","    std = gain / math.sqrt(fan)\n","    bound = math.sqrt(3.0) * std\n","    tensor[:] = np.random.uniform(-bound, bound, tensor.shape).astype(np.float32)\n","    return tensor\n","\n","\n","class Linear:\n","    # Inspired from https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py#L50\n","    def __init__(self, in_features, out_features, bias=True):\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = np.empty((out_features, in_features), dtype=np.float32)\n","        self.bias = np.empty((out_features,), dtype=np.float32) if bias else None\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        kaiming_uniform_(self.weight, a=math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n","        if self.bias is not None:\n","            fan_in = _calculate_correct_fan(self.weight, \"fan_in\")\n","            bound = 1 / math.sqrt(fan_in)\n","            self.bias[:] = np.random.uniform(-bound, bound, self.bias.shape).astype(np.float32)\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Args:\n","            input (np.ndarray): shape (batch_size, N, in_features)\n","\n","        Returns:\n","            np.ndarray: shape (batch_size, N, out_features)\n","        \"\"\"\n","\n","        #output = np.einsum('bni,oi->bno', input, self.weight)\n","        #if self.bias is not None:\n","        #    output += self.bias.reshape(1, 1, -1)\n","        #return output\n","\n","        # Original implementation # Observed no difference between einsum and original implementation\n","        return np.matmul(np.ascontiguousarray(input), self.weight.T) + (self.bias if self.bias is not None else 0)\n","\n","    def backward(self, grad_output, input):\n","        \"\"\"\n","        Args:\n","            grad_output (np.ndarray): Gradient w.r.t. output with shape (B, N, out_features)\n","            input (np.ndarray): Original input with shape (B, N, in_features)\n","\n","        Returns:\n","            grad_input (np.ndarray): Gradient w.r.t. input with shape (B, N, in_features)\n","            grad_weight (np.ndarray): Gradient w.r.t. weight with shape (out_features, in_features)\n","            grad_bias (np.ndarray or None): Gradient w.r.t. bias with shape (out_features,)\n","\n","        I find floating difference between torch.nn.Linear and this code is smaller when I use Einsum implementation of grad_weight\n","        \"\"\"\n","        grad_input = np.matmul(grad_output, self.weight)\n","\n","        # Matmul implementation of grad_weight:\n","        #input_reshaped = input.reshape(B * N, self.in_features)\n","        #grad_output_reshaped = grad_output.reshape(B * N, self.out_features)\n","        #grad_weight = np.matmul(grad_output_reshaped.T, input_reshaped)\n","\n","        # Einsum implementation of grad_weight:\n","        if input.ndim == 2:\n","            self.grad_weight = np.einsum('bi,bj->ij', grad_output, input)\n","            self.grad_bias = np.sum(grad_output, axis=0) if self.bias is not None else None\n","        elif input.ndim == 3:\n","            self.grad_weight = np.einsum('bnk,bnl->kl', grad_output, input)\n","            self.grad_bias = np.sum(grad_output, axis=(0, 1))\n","\n","        return grad_input, self.grad_weight, self.grad_bias\n","\n","# ---------- Comparison with torch.nn.Linear ----------\n","B, N, C = features_0.shape\n","x_torch = torch.tensor(features_0, dtype=torch.float32, requires_grad=True)\n","embed_dim = 128\n","\n","custom_linear = Linear(C, embed_dim, bias=True)\n","\n","torch_linear = nn.Linear(C, embed_dim, bias=True)\n","with torch.no_grad():\n","    torch_linear.weight.copy_(torch.tensor(custom_linear.weight))\n","    if custom_linear.bias is not None:\n","        torch_linear.bias.copy_(torch.tensor(custom_linear.bias))\n","\n","# ---------- Forward Pass Comparison ----------\n","output_np = custom_linear.forward(features_0)\n","output_torch = torch_linear(x_torch)\n","print(\"Forward pass comparison max abs difference:\", np.abs(output_np - output_torch.detach().numpy()).max())\n","\n","# ---------- Backward Pass Comparison ----------\n","# Dummy gradient in numpy\n","np.random.seed(1000)\n","grad_np = np.random.randn(B, N, embed_dim).astype(np.float32)\n","grad_torch = torch.tensor(grad_np, dtype=torch.float32)\n","\n","# Manual backward pass in NumPy\n","grad_input_np, grad_weight_np, grad_bias_np = custom_linear.backward(grad_np, features_0)\n","\n","# Autograd in PyTorch\n","output_torch.backward(grad_torch)\n","grad_input_torch = x_torch.grad.detach()\n","grad_weight_torch = torch_linear.weight.grad.detach()\n","grad_bias_torch = torch_linear.bias.grad.detach()\n","\n","print(\"\\nBackward pass comparison:\")\n","print(\"Grad output max abs difference:\", np.abs(grad_np - grad_torch.numpy()).max())\n","print(\"Grad input max abs difference:\", np.abs(grad_input_np - grad_input_torch.numpy()).max())\n","print(\"Grad weight max abs difference:\", np.abs(grad_weight_np - grad_weight_torch.numpy()).max())\n","print(\"Grad bias max abs difference:\", np.abs(grad_bias_np - grad_bias_torch.numpy()).max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTCr-YHZ70Js","executionInfo":{"status":"ok","timestamp":1744292715793,"user_tz":-60,"elapsed":5139,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"cbecd859-ad1d-4603-8bc8-aabebb1a8a02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Forward pass comparison max abs difference: 0.0\n","\n","Backward pass comparison:\n","Grad output max abs difference: 0.0\n","Grad input max abs difference: 2.861023e-06\n","Grad weight max abs difference: 1.5258789e-05\n","Grad bias max abs difference: 3.8146973e-05\n"]}]},{"cell_type":"markdown","source":["### GELU"],"metadata":{"id":"9C2y2MCejyCM"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from scipy.special import erf\n","\n","\n","class GELU:\n","    # Inspired from https://github.com/oniani/ai/blob/main/activation/gelu.py#L4\n","    def __init__(self):\n","        self.ctx = {}\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Args:\n","            input (B, N, embed_dim)\n","\n","        Returns\n","            GELU output of (B, N, embed_dim)\n","        \"\"\"\n","        cdf = 0.5 * (1 + erf(input / np.sqrt(2)))\n","        self.ctx['input'] = input.copy()\n","        self.ctx['cdf'] = cdf.copy()\n","        return input * cdf\n","\n","    def backward(self, grad_output):\n","        \"\"\"\n","        Returns:\n","            Grad loss w.r.t to GELU input\n","        \"\"\"\n","        input = self.ctx['input']\n","        cdf = self.ctx['cdf']\n","        pdf_val = 1 / np.sqrt(2 * np.pi) * np.exp(-0.5 * np.power(input, 2))\n","        grad_local = cdf + input * pdf_val\n","\n","        # Reset ctx to prevent memory leak\n","        # self.ctx = {}\n","        return grad_output * grad_local\n","\n","\n","if __name__ == \"__main__\":\n","    np.random.seed(1000)\n","    embed_dim = 192\n","\n","    # Compare forward pass\n","    x_np = np.random.randn(batch_size, N, embed_dim).astype(np.float32)\n","    gelu = GELU()\n","    y_np = gelu.forward(x_np)\n","\n","    x_torch = torch.tensor(x_np, requires_grad=True)\n","    y_torch = F.gelu(x_torch)\n","    print(\"GELU max abs difference in forward pass:\", np.abs(y_np - y_torch.detach().numpy()).max())\n","\n","    # Compare backward pass:\n","    grad_np = np.random.randn(batch_size, N, embed_dim).astype(np.float32)\n","    grad_input_np = gelu.backward(grad_np)\n","\n","    grad_torch = torch.tensor(grad_np, dtype=torch.float32)\n","    y_torch.backward(grad_torch)\n","    grad_input_torch = x_torch.grad.detach()\n","\n","    # Compare the backward gradients (maximum absolute difference)\n","    print(\"GELU max abs difference in backward pass:\", np.abs(grad_input_np - grad_input_torch.numpy()).max())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uz-V7KvDWVIe","executionInfo":{"status":"ok","timestamp":1744292743043,"user_tz":-60,"elapsed":300,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"3ef5abb0-2313-4f7a-b5bc-9fe83528d652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GELU max abs difference in forward pass: 3.541185931155155e-07\n","GELU max abs difference in backward pass: 9.027864429356214e-07\n"]}]},{"cell_type":"markdown","source":["### Linear -> GELU"],"metadata":{"id":"_4K8b23ReI4T"}},{"cell_type":"code","source":["class CustomModel:\n","    def __init__(self, in_features, out_features, bias=True):\n","        self.linear = Linear(in_features, out_features, bias=bias)\n","        self.gelu = GELU()\n","\n","    def forward(self, input):\n","        self.linear_out = self.linear.forward(input)\n","        self.out = self.gelu.forward(self.linear_out)\n","        return self.out\n","\n","    def backward(self, grad_output, input):\n","        grad_after_gelu = self.gelu.backward(grad_output)\n","        grad_input, grad_weight, grad_bias = self.linear.backward(grad_after_gelu, input)\n","        return grad_input, grad_weight, grad_bias\n","\n","# ---------- Main Comparison ----------\n","if __name__ == \"__main__\":\n","    embed_dim = 128\n","    x_torch = torch.tensor(features_0, dtype=torch.float32, requires_grad=True)\n","\n","    # ---------- Copy weight and bias from torch.nn.Linear -> numpy Linear ----------\n","    custom_model = CustomModel(C, embed_dim, bias=True)\n","    torch_linear = nn.Linear(C, embed_dim, bias=True)\n","    with torch.no_grad():\n","        torch_linear.weight.copy_(torch.tensor(custom_model.linear.weight))\n","        if custom_model.linear.bias is not None:\n","            torch_linear.bias.copy_(torch.tensor(custom_model.linear.bias))\n","\n","    # ---------- Forward Pass Comparison ----------\n","    out_np = custom_model.forward(features_0)\n","\n","    out_torch = F.gelu(torch_linear(x_torch))\n","\n","    print(\"Combined model forward pass max abs difference:\", np.abs(out_np - out_torch.detach().numpy()).max())\n","\n","    # ---------- Backward Pass Comparison ----------\n","    np.random.seed(1000)\n","    grad_dummy_np = np.random.randn(B, N, embed_dim).astype(np.float32)\n","    grad_dummy_torch = torch.tensor(grad_dummy_np, dtype=torch.float32)\n","\n","    # NumPy backward pass:\n","    grad_input_np, grad_weight_np, grad_bias_np = custom_model.backward(grad_dummy_np, features_0)\n","\n","    # PyTorch backward pass: compute gradients via autograd\n","    out_torch.backward(grad_dummy_torch)\n","    grad_input_torch = x_torch.grad.detach()\n","    grad_weight_torch = torch_linear.weight.grad.detach()\n","    grad_bias_torch = torch_linear.bias.grad.detach()\n","\n","    print(\"\\nCombined model backward pass comparisons:\")\n","    print(\"Grad input max abs difference:\", np.abs(grad_input_np - grad_input_torch.numpy()).max())\n","    print(\"Grad weight max abs difference:\", np.abs(grad_weight_np - grad_weight_torch.numpy()).max())\n","    print(\"Grad bias max abs difference:\", np.abs(grad_bias_np - grad_bias_torch.numpy()).max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbh3_PTOeH3v","executionInfo":{"status":"ok","timestamp":1744292797315,"user_tz":-60,"elapsed":18,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"dd46fac0-eab2-49a6-b88e-69cbe4d78baa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined model forward pass max abs difference: 3.5736101633432327e-07\n","\n","Combined model backward pass comparisons:\n","Grad input max abs difference: 1.986309933421637e-06\n","Grad weight max abs difference: 2.4437184062975348e-05\n","Grad bias max abs difference: 1.0478283925863252e-05\n"]}]},{"cell_type":"markdown","source":["### LayerNorm"],"metadata":{"id":"m00iBR6slGp5"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","eps = 1e-5\n","class LayerNorm:\n","    def __init__(self, dim):\n","        self.dim = dim\n","        self.weight = np.ones((dim,), dtype=np.float32)\n","        self.bias = np.zeros((dim,), dtype=np.float32)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: (B, N, embed_dim)\n","        Returns:\n","            out: layer-normalized activations, shape (B, T, embed_dim)\n","            cache: tuple of (x, self.w, mean, rstd) for backward pass.\n","        \"\"\"\n","        B, T, C = x.shape\n","        mean = np.sum(x, axis=-1, keepdims=True) / C           # (B, T, 1)\n","        xshift = x - mean\n","        var = np.sum(xshift ** 2, axis=-1, keepdims=True) / C  # (B, T, 1)\n","        rstd = (var + eps) ** (-0.5)                           # (B, T, 1)\n","        norm = xshift * rstd\n","        out = norm * self.weight + self.bias\n","        cache = (x, self.weight, mean, rstd)\n","        return out, cache\n","\n","    def backward(self, dout, cache):\n","        \"\"\"\n","        dout: upstream gradients, shape (B, N, embed_dim)\n","        cache: tuple of (x, w, mean, rstd) from forward pass.\n","        Returns:\n","            dx: gradient with respect to x, shape (B, T, embed_dim)\n","            dw: gradient with respect to self.w, shape (embed_dim,)\n","            db: gradient with respect to self.b, shape (embed_dim,)\n","        \"\"\"\n","        x, w, mean, rstd = cache\n","        norm = (x - mean) * rstd\n","        db = np.sum(dout, axis=(0, 1))\n","        dw = np.sum(dout * norm, axis=(0, 1))\n","        dnorm = dout * w\n","        dnorm_mean = np.mean(dnorm, axis=-1, keepdims=True)\n","        dx = dnorm - dnorm_mean - norm * np.mean(dnorm * norm, axis=-1, keepdims=True)\n","        dx *= rstd\n","        self.grad_weight = dw\n","        self.grad_bias = db\n","        return dx, dw, db\n","\n","if __name__ == \"__main__\":\n","    np.random.seed(42)\n","    shape = (128, 42, 192)\n","    x_np = np.random.randn(*shape).astype(np.float32)\n","    x_torch = torch.tensor(x_np, dtype=torch.float32, requires_grad=True)\n","\n","    # Generate random weights and biases of shape (768,)\n","    C = shape[2]\n","    w_np = np.random.randn(C).astype(np.float32)\n","    b_np = np.random.randn(C).astype(np.float32)\n","\n","    # Create PyTorch built-in LayerNorm and set its parameters.\n","    layernorm_torch = nn.LayerNorm(normalized_shape=C, eps=eps, elementwise_affine=True)\n","    layernorm_torch.weight.data = torch.tensor(w_np, dtype=torch.float32)\n","    layernorm_torch.bias.data = torch.tensor(b_np, dtype=torch.float32)\n","\n","    # ---------------- Forward Pass Comparison ----------------\n","    # Create our NumPy LayerNorm and set its weight and bias to match\n","    layernorm_np = LayerNorm(C)\n","    layernorm_np.weight = w_np.copy()\n","    layernorm_np.bias = b_np.copy()\n","    out_np, cache = layernorm_np.forward(x_np)\n","\n","    out_torch = layernorm_torch(x_torch)\n","    out_torch_np = out_torch.detach().numpy()\n","\n","    print(\"Forward Pass Comparison:\")\n","    print(\"Max abs difference:\", np.max(np.abs(out_np - out_torch_np)))\n","\n","    # ---------------- Backward Pass Comparison ----------------\n","    # Create random upstream gradients (dout) with the same shape\n","    dout_np = np.random.randn(*shape).astype(np.float32)\n","    dout_torch = torch.tensor(dout_np, dtype=torch.float32)\n","\n","    # Backward pass using the NumPy implementation\n","    dx_np, dw_np, db_np = layernorm_np.backward(dout_np, cache)\n","\n","    # Zero gradients before backward pass in PyTorch\n","    if x_torch.grad is not None:\n","        x_torch.grad.zero_()\n","    if layernorm_torch.weight.grad is not None:\n","        layernorm_torch.weight.grad.zero_()\n","    if layernorm_torch.bias.grad is not None:\n","        layernorm_torch.bias.grad.zero_()\n","\n","    # Backward pass in PyTorch: supply dout_torch as gradient\n","    out_torch.backward(dout_torch)\n","    dx_torch = x_torch.grad.detach().numpy()\n","    dw_torch = layernorm_torch.weight.grad.detach().numpy()\n","    db_torch = layernorm_torch.bias.grad.detach().numpy()\n","\n","    print(\"\\nBackward Pass Comparison:\")\n","    print(\"dx max abs difference:\", np.max(np.abs(dx_np - dx_torch)))\n","    print(\"dw max abs difference:\", np.max(np.abs(dw_np - dw_torch)))\n","    print(\"db max abs difference:\", np.max(np.abs(db_np - db_torch)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FAGZgZ1Ec3X","executionInfo":{"status":"ok","timestamp":1744292815314,"user_tz":-60,"elapsed":135,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"f44f6cdd-0548-453f-f64c-34d4b58d5291"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Forward Pass Comparison:\n","Max abs difference: 1.9073486e-06\n","\n","Backward Pass Comparison:\n","dx max abs difference: 2.861023e-06\n","dw max abs difference: 7.8201294e-05\n","db max abs difference: 0.0\n"]}]},{"cell_type":"markdown","source":["### DropOut\n","\n","torch.manual_seed(42) != numpy.manual_seed(42) so to fairly compare, I need to copy dropout mask from pytorch to numpy. torch.nn.Dropout doesn't return mask so I create a children class of torch.nn.Dropout that stores mask"],"metadata":{"id":"OAvyIZ6e2AFl"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","class Dropout:\n","    # Inspired from https://gist.github.com/nbertagnolli/35eb960d08c566523b4da599f6099b41\n","    # and https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/modules/dropout.py#L35\n","    def __init__(self, p=0.5):\n","        if p < 0 or p > 1:\n","            raise ValueError(\"p must be a probability in [0, 1].\")\n","        self.p = p\n","        self.training = True\n","        self.mask = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (np.ndarray): Input array of any shape.\n","\n","        Returns:\n","            np.ndarray: Output array with dropout applied.\n","        \"\"\"\n","        if self.training:\n","            # Only generate a new mask if one hasn't been set externally.\n","            if self.mask is None or self.mask.shape != x.shape:\n","                self.mask = (np.random.rand(*x.shape) >= self.p).astype(x.dtype)\n","            return (x * self.mask) / (1 - self.p)\n","        else:\n","            self.mask = None\n","            return x\n","\n","    def backward(self, grad_output):\n","        if self.training:\n","            if self.mask is None:\n","                raise ValueError(\"Must run forward pass before backward pass in training mode.\")\n","            return (grad_output * self.mask) / (1 - self.p)\n","        else:\n","            return grad_output\n","\n","\n","# ---------- Custom PyTorch Dropout that Stores the Mask ----------\n","class CustomTorchDropout(nn.Module):\n","    def __init__(self, p=0.5):\n","        super(CustomTorchDropout, self).__init__()\n","        if p < 0 or p > 1:\n","            raise ValueError(\"p must be in [0, 1].\")\n","        self.p = p\n","        self.training = True\n","        self.mask = None\n","\n","    def forward(self, x):\n","        if self.training:\n","            self.mask = (torch.rand_like(x) >= self.p).float()\n","            return (x * self.mask) / (1 - self.p)\n","        else:\n","            self.mask = None\n","            return x\n","\n","\n","# ---------- Instantiate Layers ----------\n","embed_dim = 192\n","expansion_factor = 4\n","p = 0.5\n","torch_dropout = CustomTorchDropout(p=p)\n","torch_dropout.training = True\n","\n","numpy_dropout = Dropout(p=p)\n","numpy_dropout.training = True\n","\n","# ---------- Copy mask ----------\n","np.random.seed(42)\n","x_np = np.random.randn(embed_dim, embed_dim*expansion_factor).astype(np.float32)\n","x_torch = torch.tensor(x_np, requires_grad=True)\n","\n","output_torch = torch_dropout(x_torch)\n","mask_from_torch = torch_dropout.mask.cpu().numpy().astype(np.float32)\n","numpy_dropout.mask = mask_from_torch.copy()\n","\n","# ---------- Forward Pass Comparison ----------\n","output_torch = torch_dropout(x_torch)\n","mask_from_torch = torch_dropout.mask.cpu().numpy()\n","numpy_dropout.mask = mask_from_torch.copy()\n","\n","output_np = numpy_dropout.forward(x_np)\n","print(\"Max absolute difference in forward pass:\", np.max(np.abs(output_np - output_torch.detach().cpu().numpy())))\n","\n","# ---------- Backward Pass Comparison ----------\n","grad_output_torch = torch.ones_like(x_torch)\n","output_torch.backward(grad_output_torch)\n","grad_input_torch = x_torch.grad.detach().cpu().numpy()\n","\n","grad_output_np = np.ones_like(x_np, dtype=np.float32)\n","grad_input_np = numpy_dropout.backward(grad_output_np)\n","\n","print(\"Max absolute difference in backward pass:\", np.max(np.abs(grad_input_np - grad_input_torch)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxI_ecli-ZPJ","executionInfo":{"status":"ok","timestamp":1744292818041,"user_tz":-60,"elapsed":51,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"c0ff4294-7c83-4a70-ea25-bcb6d45a53ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max absolute difference in forward pass: 0.0\n","Max absolute difference in backward pass: 0.0\n"]}]},{"cell_type":"markdown","source":["### Precision benchmark MLP-Mixer"],"metadata":{"id":"BYzkHZ0T3dvT"}},{"cell_type":"markdown","source":["numpy MLP-Mixer"],"metadata":{"id":"MxVIcJ5klQUl"}},{"cell_type":"code","source":["class Reduce_np:\n","    def __init__(self, axis, reduction='mean'):\n","        self.axis = axis\n","        self.reduction = reduction\n","\n","    def forward(self, x):\n","        self.last_input = x.copy()\n","        if self.reduction == 'mean':\n","            return np.mean(x, axis=self.axis)\n","        else:\n","            raise NotImplementedError()\n","\n","    def backward(self, grad_output):\n","        shape = self.last_input.shape                         # (B, N, dim)\n","        scale = shape[self.axis]                              # axis = N in MLP_mixer\n","        grad_input = grad_output / scale\n","        grad_input = grad_input.reshape(grad_input.shape[0], 1, grad_input.shape[1])  # Reshape to (B, 1, dim) to broadcast along N dim\n","        return np.broadcast_to(grad_input, shape)             # broadcasts to (B, N, dim)\n","\n","\n","class PreNormResidual:\n","    # Inspired from https://github.com/lucidrains/mlp-mixer-pytorch/blob/main/mlp_mixer_pytorch/mlp_mixer_pytorch.py#L7\n","    def __init__(self, dim, fn):\n","        self.norm = LayerNorm(dim)\n","        self.fn = fn\n","        # fn=TokenMixing/ChannelMixing\n","        # both return gradients w.r.t input, linear weight & bias, both have cache\n","    def forward(self, x):\n","        norm_x, norm_cache = self.norm.forward(x)\n","        f_out = self.fn.forward(norm_x)\n","        # Store caches for backward\n","        self.cache = {'x': x.copy(), 'norm_cache': norm_cache, 'f_cache': self.fn.cache}\n","        return f_out + x\n","\n","    def backward(self, d_out):\n","        # d_out (B, N, dim)\n","        d_f, f_params_grad = self.fn.backward(d_out)\n","        # Backprop through norm: use stored norm cache\n","        d_norm, norm_dw, norm_db = self.norm.backward(d_f, self.cache['norm_cache'])\n","        self.norm.grad_weight = norm_dw\n","        self.norm.grad_bias = norm_db\n","        # The gradient from x is then: d_norm (through norm and f) plus the identity branch d_out.\n","        d_x = d_norm + d_out\n","\n","        return d_x, norm_dw, norm_db, f_params_grad\n","\n","\n","# TokenMixing operates along N dimension\n","class TokenMixing:\n","    def __init__(self, num_tokens, expansion_factor, dropout_p):\n","        inner_dim = int(num_tokens * expansion_factor)\n","        self.linear1 = Linear(num_tokens, inner_dim)\n","        self.gelu = GELU()\n","        self.dropout1 = Dropout(dropout_p)\n","        self.linear2 = Linear(inner_dim, num_tokens)\n","        self.dropout2 = Dropout(dropout_p)\n","\n","    def forward(self, x):\n","        cache = {}\n","        cache['x'] = x.copy()\n","        x_t = np.transpose(x, (0, 2, 1))\n","        cache['x_t'] = x_t.copy()\n","        out_l1 = self.linear1.forward(x_t)\n","        cache['out_l1'] = out_l1.copy()\n","        out_gelu = self.gelu.forward(out_l1)\n","        cache['out_gelu'] = out_gelu.copy()\n","        out_drop1 = self.dropout1.forward(out_gelu)\n","        cache['out_drop1'] = out_drop1.copy()\n","        out_l2 = self.linear2.forward(out_drop1)\n","        cache['out_l2'] = out_l2.copy()\n","        out_drop2 = self.dropout2.forward(out_l2)\n","        cache['out_drop2'] = out_drop2.copy()\n","        out = np.transpose(out_drop2, (0, 2, 1))\n","        self.cache = cache\n","        return out\n","\n","    def backward(self, d_out):\n","        cache = self.cache\n","        d_drop2 = np.transpose(d_out, (0, 2, 1))\n","        d_l2 = self.dropout2.backward(d_drop2)\n","        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_l2, cache['out_drop1'])\n","        d_gelu = self.dropout1.backward(d_drop1)\n","        d_l1 = self.gelu.backward(d_gelu)\n","        d_x_t, grad_w1, grad_b1 = self.linear1.backward(d_l1, cache['x_t'])\n","        d_x = np.transpose(d_x_t, (0, 2, 1))\n","        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n","\n","        # Reset cache to prevent memory leak\n","        cache = {}\n","        return d_x, self.params_grad\n","\n","# ChannelMixing operates along embedding dimension\n","class ChannelMixing:\n","    def __init__(self, dim, expansion_factor, dropout_p):\n","        inner_dim = int(dim * expansion_factor)\n","        self.linear1 = Linear(dim, inner_dim)\n","        self.gelu = GELU()\n","        self.dropout1 = Dropout(dropout_p)\n","        self.linear2 = Linear(inner_dim, dim)\n","        self.dropout2 = Dropout(dropout_p)\n","\n","    def forward(self, x):\n","        cache = {}\n","        cache['x'] = x.copy()\n","        out_l1 = self.linear1.forward(x)\n","        cache['out_l1'] = out_l1.copy()\n","        out_gelu = self.gelu.forward(out_l1)\n","        cache['out_gelu'] = out_gelu.copy()\n","        out_drop1 = self.dropout1.forward(out_gelu)\n","        cache['out_drop1'] = out_drop1.copy()\n","        out_l2 = self.linear2.forward(out_drop1)\n","        cache['out_l2'] = out_l2.copy()\n","        out_drop2 = self.dropout2.forward(out_l2)\n","        cache['out_drop2'] = out_drop2.copy()\n","        self.cache = cache\n","        return out_drop2\n","\n","    def backward(self, d_out):\n","        cache = self.cache\n","        d_drop2 = self.dropout2.backward(d_out)\n","        d_drop1, grad_w2, grad_b2 = self.linear2.backward(d_drop2, cache['out_drop1'])\n","        d_gelu = self.dropout1.backward(d_drop1)\n","        d_l1, grad_w1, grad_b1 = self.linear1.backward(self.gelu.backward(d_gelu), cache['x'])\n","        self.params_grad = {'linear1': (grad_w1, grad_b1), 'linear2': (grad_w2, grad_b2)}\n","\n","        # Reset cache to prevent memory leak\n","        cache = {}\n","        return d_l1, self.params_grad\n","\n","# ---------- Model ----------\n","class MLPMixer:\n","    def __init__(self, num_tokens, input_dim, dim, depth, num_classes,\n","                 expansion_factor=4, expansion_factor_token=0.5, dropout_p=0.):\n","        \"\"\"\n","        Args:\n","            num_tokens (int): Number of tokens (N).\n","            input_dim (int): Input token dimension.\n","            dim (int): Mixer embedding dimension.\n","            depth (int): Number of mixer layers.\n","            num_classes (int): Number of classes for classification.\n","            expansion_factor (float): Expansion factor for token mixing.\n","            expansion_factor_token (float): Expansion factor for channel mixing.\n","            dropout_p (float): Dropout probability.\n","        \"\"\"\n","        self.num_tokens = num_tokens\n","        self.input_dim = input_dim\n","        self.dim = dim\n","        self.depth = depth\n","        self.num_classes = num_classes\n","        self.dropout_p = dropout_p\n","        self.proj = Linear(input_dim, dim) if input_dim != dim else None\n","        self.mixer_layers = []\n","        for _ in range(depth):\n","            token_mixing = PreNormResidual(dim, TokenMixing(num_tokens, expansion_factor, dropout_p))\n","            channel_mixing = PreNormResidual(dim, ChannelMixing(dim, expansion_factor_token, dropout_p))\n","            self.mixer_layers.append((token_mixing, channel_mixing))\n","        self.layer_norm = LayerNorm(dim)\n","        self.reduce = Reduce_np(axis=1, reduction='mean')\n","        self.classifier = Linear(dim, num_classes)\n","        self.cache = {}\n","\n","    def forward(self, x):\n","        if self.proj is not None:\n","            self.cache['proj_in'] = x.copy()\n","            x = self.proj.forward(x)\n","        else:\n","            self.cache['proj_in'] = None\n","        self.cache['mixer'] = []\n","\n","        for token_mixing, channel_mixing in self.mixer_layers:\n","            mixer_cache = {}\n","            mixer_cache['in'] = x.copy()\n","            t_out = token_mixing.forward(x)\n","            mixer_cache['token_cache'] = token_mixing.cache\n","            c_out = channel_mixing.forward(t_out)\n","            mixer_cache['channel_cache'] = channel_mixing.cache\n","            self.cache['mixer'].append(mixer_cache)\n","            x = c_out\n","\n","        self.cache['ln_in'] = x.copy()\n","        x_ln, ln_cache = self.layer_norm.forward(x)\n","        self.cache['ln_cache'] = ln_cache\n","        self.cache['reduce_in'] = x_ln.copy()\n","        x_red = self.reduce.forward(x_ln)\n","        self.cache['clf_in'] = x_red.copy()\n","        x_cls = self.classifier.forward(x_red)\n","        self.cache['output'] = x_cls.copy()\n","        return x_cls\n","\n","    def backward(self, grad_output):\n","        d_clf, _ , _ = self.classifier.backward(grad_output, self.cache['clf_in'])\n","        d_reduce = self.reduce.backward(d_clf)\n","        d_ln, _ , _ = self.layer_norm.backward(d_reduce, self.cache['ln_cache'])\n","        grad = d_ln\n","\n","        for (token_mixing, channel_mixing), mixer_cache in zip(self.mixer_layers[::-1],\n","                                                               self.cache['mixer'][::-1]):\n","            grad, _, _, _ = channel_mixing.backward(grad)\n","            grad, _, _, _ = token_mixing.backward(grad)\n","\n","        if self.proj is not None:\n","            grad, _ , _ = self.proj.backward(grad, self.cache['proj_in'])\n","\n","        return grad"],"metadata":{"id":"18tPwFK28bW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torch MLPMixer - from lucidrains"],"metadata":{"id":"FCFS_NhqlUkh"}},{"cell_type":"code","source":["from functools import partial\n","from einops.layers.torch import Rearrange, Reduce\n","\n","# Helper: When a non-tuple is provided, treat it as square dimensions.\n","pair = lambda x: x if isinstance(x, tuple) else (x, x)\n","\n","class PreNormResidual_torch(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.fn = fn\n","        self.norm = nn.LayerNorm(dim)\n","    def forward(self, x):\n","        return self.fn(self.norm(x)) + x\n","\n","def FeedForward_torch(dim, expansion_factor=4, dropout=0., dense=nn.Linear):\n","    inner_dim = int(dim * expansion_factor)\n","    return nn.Sequential(\n","        dense(dim, inner_dim),\n","        nn.GELU(),\n","        nn.Dropout(dropout),\n","        dense(inner_dim, dim),\n","        nn.Dropout(dropout)\n","    )\n","\n","def MLPMixer_torch(num_tokens, input_dim, dim, depth, num_classes,\n","             expansion_factor=4, expansion_factor_token=0.5, dropout=0.):\n","    # Inspired from https://github.com/lucidrains/mlp-mixer-pytorch/blob/main/mlp_mixer_pytorch/mlp_mixer_pytorch.py#L26\n","    # but modify for input shape (B, N, 1) that reflects NIDS dataset\n","    chan_first, chan_last = partial(nn.Conv1d, kernel_size=1), nn.Linear\n","\n","    return nn.Sequential(\n","        nn.Linear(input_dim, dim),\n","        *[nn.Sequential(\n","            PreNormResidual_torch(dim, FeedForward_torch(num_tokens, expansion_factor, dropout, dense=chan_first)),\n","            PreNormResidual_torch(dim, FeedForward_torch(dim, expansion_factor_token, dropout, dense=chan_last))\n","        ) for _ in range(depth)],\n","        nn.LayerNorm(dim),\n","        Reduce('b n c -> b c', reduction='mean'),\n","        nn.Linear(dim, num_classes)\n","    )"],"metadata":{"id":"KgtN6Y6U3jHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you set dropout > 0, the difference will be significant because torch.nn.Dropout initialise a different mask to numpy Dropout. As mention above, there's no way to copy mask\n","from torch.nn.Dropout so if you want to keep the maximum absolute difference small and using dropout, use CustomTorchDropout"],"metadata":{"id":"nbgAGLfeToDc"}},{"cell_type":"code","source":["# ----- Benchmark Setup -----\n","if __name__ == '__main__':\n","    num_tokens = 44\n","    input_dim = 1\n","    dim = 192\n","    depth = 8\n","    num_classes = 2\n","\n","    # Instantiate the models\n","    model_torch = MLPMixer_torch(num_tokens, input_dim, dim, depth, num_classes, dropout=0.)\n","    model = MLPMixer(num_tokens, input_dim, dim, depth, num_classes, dropout_p=0.)\n","\n","    x_torch = torch.tensor(batch_0, dtype=torch.float32, requires_grad=True)\n","\n","    # Copy weight and bias from torch.nn.Linear to numpy Linear in MLPMixer\n","    with torch.no_grad():\n","        # Copy projection parameters\n","        torch_proj = model_torch[0]\n","        if model.proj is not None:\n","            model.proj.weight[:] = torch_proj.weight.detach().cpu().numpy()\n","            model.proj.bias[:] = torch_proj.bias.detach().cpu().numpy()\n","\n","        # For each mixer block, visit token and channel mixing blocks\n","        for i in range(depth):\n","            block = model_torch[i+1]  # Each block is an nn.Sequential of length 2.\n","\n","            # Token Mixing Branch:\n","            torch_token_branch = block[0]\n","            feed_seq_token = torch_token_branch.fn\n","            torch_token_lin1 = feed_seq_token[0]   # nn.Conv1d (inner_dim, num_tokens, 1)\n","            torch_token_lin2 = feed_seq_token[3]   # nn.Conv1d (num_tokens, inner_dim, 1)\n","            np_token_branch = model.mixer_layers[i][0].fn\n","            np_token_branch.linear1.weight[:] = torch_token_lin1.weight.detach().cpu().numpy().squeeze(-1)\n","            np_token_branch.linear1.bias[:] = torch_token_lin1.bias.detach().cpu().numpy()\n","            np_token_branch.linear2.weight[:] = torch_token_lin2.weight.detach().cpu().numpy().squeeze(-1)\n","            np_token_branch.linear2.bias[:] = torch_token_lin2.bias.detach().cpu().numpy()\n","\n","            # Channel Mixing Branch:\n","            torch_channel_branch = block[1]\n","            feed_seq_channel = torch_channel_branch.fn\n","            torch_channel_lin1 = feed_seq_channel[0]   # nn.Linear(dim, inner_dim)\n","            torch_channel_lin2 = feed_seq_channel[3]   # nn.Linear(inner_dim, dim)\n","            np_channel_branch = model.mixer_layers[i][1].fn\n","            np_channel_branch.linear1.weight[:] = torch_channel_lin1.weight.detach().cpu().numpy()\n","            np_channel_branch.linear1.bias[:] = torch_channel_lin1.bias.detach().cpu().numpy()\n","            np_channel_branch.linear2.weight[:] = torch_channel_lin2.weight.detach().cpu().numpy()\n","            np_channel_branch.linear2.bias[:] = torch_channel_lin2.bias.detach().cpu().numpy()\n","\n","        # Copy classifier parameters.\n","        torch_classifier = model_torch[-1]\n","        model.classifier.weight[:] = torch_classifier.weight.detach().cpu().numpy()\n","        model.classifier.bias[:] = torch_classifier.bias.detach().cpu().numpy()\n","\n","    # --- Benchmark forward pass ---\n","    logits_np = model.forward(batch_0)\n","    print(\"logits shape\", logits_np.shape)\n","    logits_torch = model_torch(x_torch)\n","    print(\"Max abs difference in forward pass:\", np.max(np.abs(logits_np - logits_torch.detach().numpy())))\n","\n","    # --- Benchmark backward pass ---\n","    # Create a dummy grad for classifier output\n","    np.random.seed(42)\n","    grad_dummy_np = np.random.randn(batch_size, num_classes).astype(np.float32)\n","    grad_dummy_torch = torch.tensor(grad_dummy_np, dtype=torch.float32)\n","\n","    # NumPy backward pass: obtain gradient with respect to input.\n","    grad_input_np = model.backward(grad_dummy_np)\n","    # PyTorch backward pass:\n","    if x_torch.grad is not None:\n","        x_torch.grad.zero_()\n","    logits_torch.backward(grad_dummy_torch)\n","    grad_input_torch = x_torch.grad.detach().cpu().numpy()\n","\n","    bwd_diff = np.max(np.abs(grad_input_np - grad_input_torch))\n","    print(\"Max abs difference in backward pass:\", bwd_diff)"],"metadata":{"id":"UciI_ckxFUwO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744208929395,"user_tz":-60,"elapsed":6952,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"349ccf89-bff2-4f3f-8ed0-84d0eb1e6b1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["logits shape (64, 2)\n","Max abs difference in forward pass: 2.56540936383054e-07\n","Max abs difference in backward pass: 2.912513954178575e-07\n"]}]},{"cell_type":"markdown","source":["# Precision benchmark for loss, optimiser, optimiser.zero_grad()"],"metadata":{"id":"4IeIbeofh-xV"}},{"cell_type":"markdown","source":["### BCELoss with logits"],"metadata":{"id":"-lA0RiKcrWKH"}},{"cell_type":"code","source":["import math\n","\n","class BCEWithLogits_np:\n","    # Inspired from https://github.com/pytorch/pytorch/blob/c64e006fc399d528bb812ae589789d0365f3daf4/aten/src/ATen/native/Loss.cpp#L214-L259\n","    # binary cross-entropy loss with logits in a numerically stable way using log-sum-exp and backprop it\n","    def __init__(self, weight=None, pos_weight=None, reduction='mean'):\n","        \"\"\"\n","        Args:\n","            weight (np.ndarray or None): Optional weight tensor, broadcastable to input.\n","            pos_weight (np.ndarray or None): Optional weight for positive examples.\n","            reduction (str): 'mean' or 'sum'\n","        \"\"\"\n","        self.weight = weight\n","        self.pos_weight = pos_weight\n","        self.reduction = reduction\n","        self.cache = {}\n","\n","    def forward(self, input, target):\n","        \"\"\"\n","        Args:\n","            input (np.ndarray): logits (B, 2)\n","            target (np.ndarray): labels (B, 2)\n","        Returns:\n","            loss (float or np.ndarray): reduced loss value if reduction is 'mean' or 'sum'; elementwise loss if reduction is 'none'.\n","        \"\"\"\n","        max_val = np.maximum(-input, 0)\n","\n","        if self.pos_weight is not None:\n","            log_weight = 1 + (self.pos_weight - 1) * target\n","            lse = np.log(np.exp(-max_val) + np.exp(-input - max_val))\n","            loss = (1 - target) * input + log_weight * (lse + max_val)\n","        else:\n","            loss = (1 - target) * input + max_val + np.log(np.exp(-max_val) + np.exp(-input - max_val))\n","\n","        if self.weight is not None:\n","            loss = loss * self.weight\n","\n","        self.cache['input'] = input.copy()\n","        self.cache['target'] = target.copy()\n","\n","        if self.reduction == 'mean':\n","            return np.mean(loss)\n","        elif self.reduction == 'sum':\n","            return np.sum(loss)\n","\n","    def backward(self, grad_output=1.0):\n","        input = self.cache['input']\n","        target = self.cache['target']\n","\n","        # Compute sigmoid(input) in a stable way.\n","        sig = 1 / (1 + np.exp(-input))\n","        if self.pos_weight is not None:\n","            grad_input = (sig * (self.pos_weight * target + (1 - target)) - self.pos_weight * target)\n","        else:\n","            grad_input = sig - target\n","\n","        if self.weight is not None:\n","            grad_input = grad_input * self.weight\n","\n","        # If reduction is mean, scale the gradient by 1/numel.\n","        if self.reduction == 'mean':\n","            grad_input = grad_input * grad_output / input.size\n","        else:\n","            grad_input = grad_input * grad_output\n","\n","        return grad_input"],"metadata":{"id":"ztBWdr3zrYo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adam optimiser"],"metadata":{"id":"GIa9ll0xwTYu"}},{"cell_type":"code","source":["class ADAM_np:\n","    # Inspired from https://gist.github.com/aerinkim/dfe3da1000e67aced1c7d9279351cb88\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n","        \"\"\"\n","        Args:\n","            params (list of np.ndarray): List of parameters to optimize. Each parameter must have a field\n","                `grad` holding the gradient (numpy array of the same shape).\n","            lr (float): Learning rate.\n","            betas (tuple): Coefficients for exponential moving averages (beta1, beta2).\n","            eps (float): Term added to denominator for numerical stability.\n","            weight_decay (float): L2 penalty.\n","        \"\"\"\n","        self.param_tuples = params\n","        self.lr = lr\n","        self.beta1, self.beta2 = betas\n","        self.eps = eps\n","        self.weight_decay = weight_decay\n","\n","        # Create a state for each parameter.\n","        self.state = {}\n","        for (p, grad_attr, mod) in self.param_tuples:\n","            self.state[id(p)] = {\n","                'step': 0,\n","                'exp_avg': np.zeros_like(p),\n","                'exp_avg_sq': np.zeros_like(p)\n","            }\n","\n","    def step(self):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \"\"\"\n","        for (p, grad_attr, mod) in self.param_tuples:\n","            # Get gradient stored in module: e.g., mod.grad_weight or mod.grad_bias.\n","            grad = getattr(mod, grad_attr)\n","            state = self.state[id(p)]\n","            state['step'] += 1\n","\n","            # Apply weight decay if needed.\n","            if self.weight_decay != 0:\n","                grad = grad + self.weight_decay * p\n","\n","            # Update exponential moving averages.\n","            state['exp_avg'] = self.beta1 * state['exp_avg'] + (1 - self.beta1) * grad\n","            state['exp_avg_sq'] = self.beta2 * state['exp_avg_sq'] + (1 - self.beta2) * (grad * grad)\n","\n","            # Compute the denominator.\n","            denom = np.sqrt(state['exp_avg_sq']) + self.eps\n","\n","            # Bias corrections.\n","            bias_correction1 = 1 / (1 - self.beta1 ** state['step'])\n","            bias_correction2 = 1 / (1 - self.beta2 ** state['step'])\n","            adapted_lr = self.lr * bias_correction1 / math.sqrt(bias_correction2)\n","\n","            # Parameter update.\n","            p[:] = p - adapted_lr * (state['exp_avg'] / denom)"],"metadata":{"id":"Rkpkr2sYwjSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","x_torch = torch.tensor(batch_0, dtype=torch.float32, requires_grad=True)\n","B, N, C = batch_0.shape\n","num_classes = 2\n","\n","# Create dummy binary labels (for each sample, a vector of two values in {0,1})\n","target_np = np.random.randint(0, 2, size=(B, num_classes)).astype(np.float32)\n","target_torch = torch.tensor(target_np, dtype=torch.float32)\n","\n","model_torch = MLPMixer_torch(num_tokens, input_dim, dim, depth, num_classes, dropout=0.)\n","model = MLPMixer(num_tokens, input_dim, dim, depth, num_classes, dropout_p=0.)\n","\n","# Copy weight and bias from torch.nn.Linear to numpy Linear in MLPMixer\n","with torch.no_grad():\n","    # Copy projection parameters\n","    torch_proj = model_torch[0]\n","    if model.proj is not None:\n","        model.proj.weight[:] = torch_proj.weight.detach().cpu().numpy()\n","        model.proj.bias[:] = torch_proj.bias.detach().cpu().numpy()\n","\n","    # For each mixer block, visit token and channel mixing blocks\n","    with torch.no_grad():\n","        # Copy projection parameters\n","        torch_proj = model_torch[0]\n","        if model.proj is not None:\n","            model.proj.weight[:] = torch_proj.weight.detach().cpu().numpy()\n","            model.proj.bias[:] = torch_proj.bias.detach().cpu().numpy()\n","\n","        # For each mixer block, visit token and channel mixing blocks\n","        for i in range(depth):\n","            block = model_torch[i+1]  # Each block is an nn.Sequential of length 2.\n","\n","            # Token Mixing Branch:\n","            torch_token_branch = block[0]\n","            feed_seq_token = torch_token_branch.fn\n","            torch_token_lin1 = feed_seq_token[0]   # nn.Conv1d (inner_dim, num_tokens, 1)\n","            torch_token_lin2 = feed_seq_token[3]   # nn.Conv1d (num_tokens, inner_dim, 1)\n","            np_token_branch = model.mixer_layers[i][0].fn\n","            np_token_branch.linear1.weight[:] = torch_token_lin1.weight.detach().cpu().numpy().squeeze(-1)\n","            np_token_branch.linear1.bias[:] = torch_token_lin1.bias.detach().cpu().numpy()\n","            np_token_branch.linear2.weight[:] = torch_token_lin2.weight.detach().cpu().numpy().squeeze(-1)\n","            np_token_branch.linear2.bias[:] = torch_token_lin2.bias.detach().cpu().numpy()\n","\n","            # Channel Mixing Branch:\n","            torch_channel_branch = block[1]\n","            feed_seq_channel = torch_channel_branch.fn\n","            torch_channel_lin1 = feed_seq_channel[0]   # nn.Linear(dim, inner_dim)\n","            torch_channel_lin2 = feed_seq_channel[3]   # nn.Linear(inner_dim, dim)\n","            np_channel_branch = model.mixer_layers[i][1].fn\n","            np_channel_branch.linear1.weight[:] = torch_channel_lin1.weight.detach().cpu().numpy()\n","            np_channel_branch.linear1.bias[:] = torch_channel_lin1.bias.detach().cpu().numpy()\n","            np_channel_branch.linear2.weight[:] = torch_channel_lin2.weight.detach().cpu().numpy()\n","            np_channel_branch.linear2.bias[:] = torch_channel_lin2.bias.detach().cpu().numpy()\n","\n","        # Copy classifier parameters.\n","        torch_classifier = model_torch[-1]\n","        model.classifier.weight[:] = torch_classifier.weight.detach().cpu().numpy()\n","        model.classifier.bias[:] = torch_classifier.bias.detach().cpu().numpy()\n","\n","# --- Benchmark forward pass ---\n","logits_np = model.forward(batch_0)\n","logits_torch = model_torch(x_torch)"],"metadata":{"id":"L5JlsCR7Onxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Compute Loss ---\n","loss_fn_np = BCEWithLogits_np(reduction='mean')\n","loss_np = loss_fn_np.forward(logits_np, target_np)\n","\n","loss_fn_torch = nn.BCEWithLogitsLoss(reduction='mean')\n","loss_torch = loss_fn_torch(logits_torch, target_torch)\n","\n","print(\"Max abs difference in forward loss:\", np.abs(loss_np - loss_torch.item()))\n","\n","# --- Backward Pass ---\n","grad_logits_np = loss_fn_np.backward()\n","grad_input_np = model.backward(grad_logits_np)\n","\n","# --- Optimiser Step ---\n","def get_all_param_tuples(module):\n","    tuples = []\n","    # If the module has a weight attribute and it is a NumPy array, add it,\n","    if hasattr(module, 'weight') and isinstance(module.weight, np.ndarray):\n","        tuples.append((module.weight, 'grad_weight', module))\n","    if hasattr(module, 'bias') and module.bias is not None and isinstance(module.bias, np.ndarray):\n","        tuples.append((module.bias, 'grad_bias', module))\n","    # Also check for a submodule 'norm'\n","    if hasattr(module, 'norm'):\n","        norm_mod = module.norm\n","        if hasattr(norm_mod, 'weight') and isinstance(norm_mod.weight, np.ndarray):\n","            tuples.append((norm_mod.weight, 'grad_weight', norm_mod))\n","        if hasattr(norm_mod, 'bias') and norm_mod.bias is not None and isinstance(norm_mod.bias, np.ndarray):\n","            tuples.append((norm_mod.bias, 'grad_bias', norm_mod))\n","    # Recursively traverse the module's __dict__, but skip keys known to be caches.\n","    for key, value in module.__dict__.items():\n","        if hasattr(value, 'forward') and callable(value.forward) and value is not module:\n","            tuples.extend(get_all_param_tuples(value))\n","        elif isinstance(value, (list, tuple)):\n","            for item in value:\n","                if hasattr(item, 'forward') and callable(item.forward):\n","                    tuples.extend(get_all_param_tuples(item))\n","    return tuples\n","\n","numpy_params = get_all_param_tuples(model)\n","optimiser_np = ADAM_np(numpy_params, lr=0.06, weight_decay=0.1)\n","optimiser_np.step()\n","\n","optimizer_torch = torch.optim.Adam(model_torch.parameters(), lr=0.06, weight_decay=0.1)\n","optimizer_torch.zero_grad()\n","loss_torch.backward()\n","optimizer_torch.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"8BghtKkmPmxf","executionInfo":{"status":"error","timestamp":1744211532492,"user_tz":-60,"elapsed":64,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"8d5592f7-1d56-4df7-85d5-aa47c3f9c21f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max abs difference in forward loss: 5.7054438284964704e-08\n"]},{"output_type":"error","ename":"KeyError","evalue":"'clf_in'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-412-4745faec2e82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# --- Backward Pass ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgrad_logits_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgrad_input_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_logits_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# --- Optimiser Step ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-362-7b6ac180733f>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0md_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0md_reduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0md_ln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ln_cache'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'clf_in'"]}]},{"cell_type":"code","source":["# --- Compare classifier weight and bias after update ---\n","classifier_weight_np = model.classifier.weight.copy()\n","classifier_bias_np = model.classifier.bias.copy()\n","\n","classifier_weight_torch = model_torch[-1].weight.detach().cpu().numpy()\n","classifier_bias_torch = model_torch[-1].bias.detach().cpu().numpy()\n","\n","weight_diff = np.max(np.abs(classifier_weight_np - classifier_weight_torch))\n","bias_diff = np.max(np.abs(classifier_bias_np - classifier_bias_torch))\n","\n","print(\"Max abs difference in classifier weight after update:\", weight_diff)\n","print(\"Max abs difference in classifier bias after update:\", bias_diff)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJa5eEW20sgS","executionInfo":{"status":"ok","timestamp":1744209134241,"user_tz":-60,"elapsed":16,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"9487ee79-f9bc-46fb-ea1b-6167f98d45f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max abs difference in classifier weight after update: 0.00013391674\n","Max abs difference in classifier bias after update: 3.427267e-07\n"]}]},{"cell_type":"code","source":["np_layernorm_weight = model.layer_norm.weight.copy()\n","np_layernorm_bias   = model.layer_norm.bias.copy()\n","\n","torch_layernorm = None\n","for module in model_torch.modules():\n","    # This will return every submodule of your Torch model.\n","    if isinstance(module, nn.LayerNorm):\n","        torch_layernorm = module\n","        break\n","\n","if torch_layernorm is None:\n","    raise RuntimeError(\"Could not find a LayerNorm module in your Torch model.\")\n","\n","torch_layernorm_weight = torch_layernorm.weight.detach().cpu().numpy()\n","torch_layernorm_bias   = torch_layernorm.bias.detach().cpu().numpy()\n","\n","print(\"Max abs difference in LayerNorm weight:\", np.max(np.abs(np_layernorm_weight - torch_layernorm_weight)))\n","print(\"Max abs difference in LayerNorm bias:\", np.max(np.abs(np_layernorm_bias - torch_layernorm_bias)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaDvsQ6E0cNy","executionInfo":{"status":"ok","timestamp":1744209137363,"user_tz":-60,"elapsed":17,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"ae9a98ea-9f58-404a-e9ce-80d5d5fb30d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max abs difference in LayerNorm weight: 5.9604645e-08\n","Max abs difference in LayerNorm bias: 0.11999719\n"]}]},{"cell_type":"code","source":["np_proj_weight = model.proj.weight.copy()\n","np_proj_bias   = model.proj.bias.copy()\n","\n","torch_proj_weight = model_torch[0].weight.detach().cpu().numpy()\n","torch_proj_bias = model_torch[0].bias.detach().cpu().numpy()\n","\n","print(\"Max abs difference in projection weight:\", np.max(np.abs(np_proj_weight - torch_proj_weight)))\n","print(\"Max abs difference in projection bias:\", np.max(np.abs(np_proj_bias - torch_proj_bias)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CH6Qo2MQ41G7","executionInfo":{"status":"ok","timestamp":1744209141307,"user_tz":-60,"elapsed":10,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"a50808c1-f426-4c9d-ac88-702468127611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max abs difference in projection weight: 6.7166984e-06\n","Max abs difference in projection bias: 1.0665506e-05\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","a = np.array([np.array([1, 2, 3]),\n","              np.array([4, 5]),\n","              np.array([6, 7, 8, 9])])\n","print(\"Array a:\", a)\n","print(\"a.shape:\", a.shape)  # This will typically be (3,) because it is an object array.\n","\n","# Now, try to create an array of zeros with the same shape as a.\n","b = np.zeros_like(a)\n","\n","print(\"a:\", a)\n","print(\"dtype:\", a.dtype)\n","\n","# Attempt to create a zeros array with the same shape as a.\n","b = np.zeros_like(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"molsdOLgwV6r","executionInfo":{"status":"error","timestamp":1744205864041,"user_tz":-60,"elapsed":57,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"d83fb718-9dac-4a1e-a5c6-df5556a8768a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-330-6a90b678c3cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m a = np.array([np.array([1, 2, 3]), \n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               np.array([6, 7, 8, 9])])\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."]}]},{"cell_type":"code","source":["import numpy as np\n","\n","x = np.array([[1, 2], [3, 4]])\n","x[...] = 0\n","print(x)"],"metadata":{"id":"G4X4ehfm5eF1","executionInfo":{"status":"ok","timestamp":1744241572774,"user_tz":-60,"elapsed":15,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"1c22044a-c710-4501-f2f4-46cd8182817e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0]\n"," [0 0]]\n"]}]},{"cell_type":"markdown","source":["### Optimizer.zero_grad()"],"metadata":{"id":"FMNfajn-4Uak"}},{"cell_type":"code","source":["def clear_auxiliary(module):\n","    \"\"\"\n","    Recursively clears auxiliary attributes from the module and its submodules.\n","    This includes attributes with keys 'cache', 'ctx', 'params_grad', and 'mask'.\n","\n","    For each key:\n","      - If its value is a dict, it is set to {}.\n","      - Otherwise, it is set to None.\n","    \"\"\"\n","    aux_keys = ('cache', 'ctx', 'params_grad', 'mask')\n","    # Clear any auxiliary attribute on this module.\n","    for key in aux_keys:\n","        if key in module.__dict__:\n","            current = module.__dict__[key]\n","            if isinstance(current, dict):\n","                setattr(module, key, {})\n","            else:\n","                setattr(module, key, None)\n","\n","    # Now recursively traverse submodules.\n","    for key, value in module.__dict__.items():\n","        # Skip aux keys already handled.\n","        if key in aux_keys:\n","            continue\n","\n","        # If the attribute is a list or tuple, iterate through its items.\n","        if isinstance(value, (list, tuple)):\n","            for item in value:\n","                if isinstance(item, tuple):\n","                    for subitem in item:\n","                        if hasattr(subitem, 'forward') and callable(subitem.forward):\n","                            clear_auxiliary(subitem)\n","                elif hasattr(value, 'forward') and callable(value.forward):\n","                    # Unlikely: this checks if the entire list itself has a forward().\n","                    clear_auxiliary(value)\n","                elif hasattr(item, 'forward') and callable(item.forward):\n","                    clear_auxiliary(item)\n","        # If the attribute is a submodule (has callable forward), recurse.\n","        elif hasattr(value, 'forward') and callable(value.forward) and value is not module:\n","            clear_auxiliary(value)\n","\n","# Example usage:\n","# After an optimizer step or at the start/end of an iteration, call:\n","clear_auxiliary(model)"],"metadata":{"id":"kjJ0KrdDFCWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_dict(d, indent=0):\n","    prefix = \" \" * indent\n","    if not isinstance(d, dict):\n","        print(f\"{prefix}{d}\")\n","        return\n","    for key, value in d.items():\n","        if isinstance(value, np.ndarray):\n","            print(f\"{prefix}{key}: array of shape {value.shape}\")\n","        elif isinstance(value, dict):\n","            print(f\"{prefix}{key}: dict:\")\n","            print_dict(value, indent+4)\n","        elif isinstance(value, (int, float, str)):\n","            print(f\"{prefix}{key}: {value}\")\n","        else:\n","            # If it is a list, tuple, or other object, show the type\n","            print(f\"{prefix}{key}: {type(value).__name__}\")\n","\n","def print_model_details(module, indent=0):\n","    prefix = \" \" * indent\n","    # Print current module class name.\n","    print(f\"{prefix}{module.__class__.__name__} details:\")\n","    for key, value in module.__dict__.items():\n","        # If key is one of the ones we want to inspect its content:\n","        if key in ('ctx', 'cache', 'params_grad'):\n","            print(f\"{prefix}  {key}:\")\n","            print_dict(value, indent+4)\n","        elif isinstance(value, np.ndarray):\n","            print(f\"{prefix}  {key}: array of shape {value.shape}\")\n","        else:\n","            if isinstance(value, (int, float, str)):\n","                print(f\"{prefix}  {key}: {value}\")\n","            else:\n","                print(f\"{prefix}  {key}: {type(value).__name__}\")\n","\n","    # Recursively descend for each attribute.\n","    for key, value in module.__dict__.items():\n","        # If the attribute is a list or tuple, iterate through its items.\n","        if isinstance(value, (list, tuple)):\n","            for idx, item in enumerate(value):\n","                if isinstance(item, tuple):\n","                    for subitem in item:\n","                        if hasattr(subitem, 'forward') and callable(subitem.forward):\n","                            print_model_details(subitem, indent+4)\n","                elif hasattr(value, 'forward') and callable(value.forward):\n","                    print_model_details(value, indent+4)\n","                elif hasattr(item, 'forward') and callable(item.forward):\n","                    print_model_details(item, indent+4)\n","        # If the attribute is a module (has callable forward), recurse.\n","        elif hasattr(value, 'forward') and callable(value.forward) and value is not module:\n","            print_model_details(value, indent+4)\n","\n","print_model_details(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XkqOFnOMFDOS","executionInfo":{"status":"ok","timestamp":1744211327895,"user_tz":-60,"elapsed":16,"user":{"displayName":"Nam Tran","userId":"15778016803607933273"}},"outputId":"e70c99f8-5c87-42a0-c044-4d88c0caffaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MLPMixer details:\n","  num_tokens: 44\n","  input_dim: 1\n","  dim: 192\n","  depth: 8\n","  num_classes: 2\n","  dropout_p: 0.0\n","  proj: Linear\n","  mixer_layers: list\n","  layer_norm: LayerNorm\n","  reduce: Reduce_np\n","  classifier: Linear\n","  cache:\n","    Linear details:\n","      in_features: 1\n","      out_features: 192\n","      weight: array of shape (192, 1)\n","      bias: array of shape (192,)\n","      grad_weight: array of shape (192, 1)\n","      grad_bias: array of shape (192,)\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: TokenMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        TokenMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 44\n","              out_features: 176\n","              weight: array of shape (176, 44)\n","              bias: array of shape (176,)\n","              grad_weight: array of shape (176, 44)\n","              grad_bias: array of shape (176,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 176\n","              out_features: 44\n","              weight: array of shape (44, 176)\n","              bias: array of shape (44,)\n","              grad_weight: array of shape (44, 176)\n","              grad_bias: array of shape (44,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    PreNormResidual details:\n","      norm: LayerNorm\n","      fn: ChannelMixing\n","      cache:\n","        LayerNorm details:\n","          dim: 192\n","          weight: array of shape (192,)\n","          bias: array of shape (192,)\n","          grad_weight: array of shape (192,)\n","          grad_bias: array of shape (192,)\n","        ChannelMixing details:\n","          linear1: Linear\n","          gelu: GELU\n","          dropout1: Dropout\n","          linear2: Linear\n","          dropout2: Dropout\n","          cache:\n","          params_grad:\n","            Linear details:\n","              in_features: 192\n","              out_features: 96\n","              weight: array of shape (96, 192)\n","              bias: array of shape (96,)\n","              grad_weight: array of shape (96, 192)\n","              grad_bias: array of shape (96,)\n","            GELU details:\n","              ctx:\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","            Linear details:\n","              in_features: 96\n","              out_features: 192\n","              weight: array of shape (192, 96)\n","              bias: array of shape (192,)\n","              grad_weight: array of shape (192, 96)\n","              grad_bias: array of shape (192,)\n","            Dropout details:\n","              p: 0.0\n","              training: True\n","              mask: NoneType\n","    LayerNorm details:\n","      dim: 192\n","      weight: array of shape (192,)\n","      bias: array of shape (192,)\n","      grad_weight: array of shape (192,)\n","      grad_bias: array of shape (192,)\n","    Reduce_np details:\n","      axis: 1\n","      reduction: mean\n","      last_input: array of shape (64, 44, 192)\n","    Linear details:\n","      in_features: 192\n","      out_features: 2\n","      weight: array of shape (2, 192)\n","      bias: array of shape (2,)\n","      grad_weight: array of shape (2, 192)\n","      grad_bias: array of shape (2,)\n"]}]}]}